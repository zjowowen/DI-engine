

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PPO &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ACER" href="acer.html" />
    <link rel="prev" title="A2C" href="a2c.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>PPO</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/ppo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="ppo">
<h1>PPO<a class="headerlink" href="#ppo" title="Permalink to this headline">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>PPO (Proximal Policy Optimization) was proposed in <a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a>.
The key question to answer is that how can we utilize the existing data to take the most possible improvement step for the policy
without accidentally leading to performance collapse.
PPO follows the idea of TRPO (which restricts the step of policy update by explicit KL-divergence constraint),
but doesn’t have a KL-divergence term in the objective,
instead utilizing a specialized clipped objective to remove incentives for the new policy to get far from the old policy.
PPO avoids the calculation of the Hessian matrix in TRPO, thus is simpler to implement and empirically performs at least as well as TRPO.</p>
</section>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>PPO is a <strong>model-free</strong> and <strong>policy-gradient</strong> RL algorithm.</p></li>
<li><p>PPO supports both <strong>discrete</strong> and <strong>continuous action spaces</strong>.</p></li>
<li><p>PPO supports <strong>off-policy</strong> mode and <strong>on-policy</strong> mode.</p></li>
<li><p>PPO can be equipped with RNN.</p></li>
<li><p>PPO is a first-order gradient method that use a few tricks to keep new policies close to old.</p></li>
</ol>
</section>
<section id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>PPO use clipped probability ratios in the policy gradient to prevent the policy from too rapid changes, specifically the
optimizing objective is:</p>
<div class="math notranslate nohighlight">
\[L_{\theta_{k}}^{C L I P}(\theta) \doteq {\mathrm{E}}_{s, a \sim \theta_{k}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)} A^{\theta_{k}}(s, a), {clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)}, 1-\epsilon, 1+\epsilon\right) A^{\theta_{k}}(s, a)\right)\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)}\)</span> is denoted as the probability ratio <span class="math notranslate nohighlight">\(r_t(\theta)\)</span>,
<span class="math notranslate nohighlight">\(\theta\)</span> are the policy parameters to be optimized at the current time, <span class="math notranslate nohighlight">\(\theta_k\)</span> are the parameters of the policy at iteration k and <span class="math notranslate nohighlight">\(\gamma\)</span> is a small hyperparameter control that controls the maximum update step size of the policy parameters.</p>
<p>According to this <a class="reference external" href="https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view?usp=sharing">note</a>, the PPO-Clip objective can be simplified to:</p>
<div class="math notranslate nohighlight">
\[L_{\theta_{k}}^{C L I P}(\theta)={\mathrm{E}}_{s, a \sim \theta_{k}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)} A^{\theta_{k}}(s, a), g\left(\epsilon, A^{\theta_{k}}(s, a)\right)\right)\right]\]</div>
<p>where,</p>
<div class="math notranslate nohighlight">
\[\begin{split}g(\epsilon, A)= \begin{cases}(1+\epsilon) A &amp; A \geq 0 \\ (1-\epsilon) A &amp; \text { otherwise }\end{cases}\end{split}\]</div>
<p>Usually we don’t access to the true advantage value of the sampled state-action pair <span class="math notranslate nohighlight">\((s,a)\)</span>, but luckily we can calculate a approximate value <span class="math notranslate nohighlight">\(\hat{A}_t\)</span>.
The idea behind this clipping objective is: for <span class="math notranslate nohighlight">\((s,a)\)</span>, if <span class="math notranslate nohighlight">\(\hat{A}_t &lt; 0\)</span>, maximizing <span class="math notranslate nohighlight">\(L^{C L I P}(\theta)\)</span> means make <span class="math notranslate nohighlight">\(\pi_{\theta}(a_{t} \mid s_{t})\)</span> smaller, but no additional benefit to the objective function is gained
by making <span class="math notranslate nohighlight">\(\pi_{\theta}(a_{t} \mid s_{t})\)</span> smaller than <span class="math notranslate nohighlight">\((1-\epsilon)\pi_{\theta}(a_{t} \mid s_{t})\)</span>
. Analogously, if <span class="math notranslate nohighlight">\(\hat{A}_t &gt; 0\)</span>, maximizing <span class="math notranslate nohighlight">\(L^{C L I P}(\theta)\)</span> means make <span class="math notranslate nohighlight">\(\pi_{\theta}(a_{t} \mid s_{t})\)</span> larger, but no additional benefit is gained by making <span class="math notranslate nohighlight">\(\pi_{\theta}(a_{t} \mid s_{t})\)</span>
larger than <span class="math notranslate nohighlight">\((1+\epsilon)\pi_{\theta}(a_{t} \mid s_{t})\)</span>.
Empirically, by optimizing this objective function, the update step of the policy network can be controlled within a reasonable range.</p>
<p>For the value function, in order to balance the bias and variance in value learning, PPO adopts the <a class="reference external" href="https://arxiv.org/abs/1506.02438">Generalized Advantage Estimator</a> to compute the advantages,
which is a exponentially-weighted sum of Bellman residual terms.  that is analogous to TD(λ):</p>
<div class="math notranslate nohighlight">
\[\hat{A}_{t}=\delta_{t}+(\gamma \lambda) \delta_{t+1}+\cdots+\cdots+(\gamma \lambda)^{T-t+1} \delta_{T-1}\]</div>
<p>where V is an approximate value function, <span class="math notranslate nohighlight">\(\delta_{t}=r_{t}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\)</span> is the Bellman residual terms, or called TD-error at timestep t.</p>
<p>The value target is calculated as: <span class="math notranslate nohighlight">\(V_{t}^{target}=V_{t}+\hat{A}_{t}\)</span>,
and the value loss is defined as a squared-error: <span class="math notranslate nohighlight">\(\frac{1}{2}*\left(V_{\theta}\left(s_{t}\right)-V_{t}^{\mathrm{target}}\right)^{2}\)</span>,
To ensure adequate exploration, PPO further enhances the objective by adding a policy entropy bonus.</p>
<p>The total PPO loss is a weighted sum of policy loss, value loss and policy entropy regularization term:</p>
<div class="math notranslate nohighlight">
\[L_{t}^{total}=\hat{\mathbb{E}}_{t}[ L_{t}^{C L I P}(\theta)+c_{1} L_{t}^{V F}(\phi)-c_{2} H(a_t|s_{t}; \pi_{\theta})]\]</div>
<p>where c1 and c2 are coefficients that control the relative importance of different terms.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The standard implementation of PPO contains the many additional optimizations which are not described in the paper. Further details can be found in <a class="reference external" href="https://arxiv.org/abs/2005.12729">IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO</a>.</p>
</div>
</section>
<section id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/PPO_onpolicy.png"><img alt="../_images/PPO_onpolicy.png" class="align-center" src="../_images/PPO_onpolicy.png" style="width: 406.0px; height: 533.0px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is the on-policy version of PPO. In DI-engine, we also have the off-policy version of PPO, which is almost the same as on-policy PPO except that
we maintain a replay buffer that stored the recent experience,
and the data used to calculate the PPO loss is sampled from the replay buffer not the recently collected batch,
so off-policy PPO are able to reuse old data very efficiently, but potentially brittle and unstable.</p>
</div>
</section>
<section id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>PPO can be combined with:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://di-engine-docs.readthedocs.io/en/latest/best_practice/nstep_td.html">Multi-step learning</a></p></li>
<li><p><a class="reference external" href="https://di-engine-docs.readthedocs.io/en/latest/best_practice/rnn.html">RNN</a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<p>The policy loss and value loss of PPO is implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ppo_error</span><span class="p">(</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">namedtuple</span><span class="p">,</span>
        <span class="n">clip_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">use_value_clip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dual_clip</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">namedtuple</span><span class="p">,</span> <span class="n">namedtuple</span><span class="p">]:</span>

    <span class="k">assert</span> <span class="n">dual_clip</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dual_clip</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;dual_clip value must be greater than 1.0, but get value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">dual_clip</span>
    <span class="p">)</span>
    <span class="n">logit_new</span><span class="p">,</span> <span class="n">logit_old</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">value_new</span><span class="p">,</span> <span class="n">value_old</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">return_</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">policy_data</span> <span class="o">=</span> <span class="n">ppo_policy_data</span><span class="p">(</span><span class="n">logit_new</span><span class="p">,</span> <span class="n">logit_old</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">adv</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">policy_output</span><span class="p">,</span> <span class="n">policy_info</span> <span class="o">=</span> <span class="n">ppo_policy_error</span><span class="p">(</span><span class="n">policy_data</span><span class="p">,</span> <span class="n">clip_ratio</span><span class="p">,</span> <span class="n">dual_clip</span><span class="p">)</span>
    <span class="n">value_data</span> <span class="o">=</span> <span class="n">ppo_value_data</span><span class="p">(</span><span class="n">value_new</span><span class="p">,</span> <span class="n">value_old</span><span class="p">,</span> <span class="n">return_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">value_loss</span> <span class="o">=</span> <span class="n">ppo_value_error</span><span class="p">(</span><span class="n">value_data</span><span class="p">,</span> <span class="n">clip_ratio</span><span class="p">,</span> <span class="n">use_value_clip</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ppo_loss</span><span class="p">(</span><span class="n">policy_output</span><span class="o">.</span><span class="n">policy_loss</span><span class="p">,</span> <span class="n">value_loss</span><span class="p">,</span> <span class="n">policy_output</span><span class="o">.</span><span class="n">entropy_loss</span><span class="p">),</span> <span class="n">policy_info</span>
</pre></div>
</div>
<p>The interface of <code class="docutils literal notranslate"><span class="pre">ppo_policy_error</span></code> and <code class="docutils literal notranslate"><span class="pre">ppo_value_error</span></code> is defined as follows:</p>
</section>
<section id="implementation-tricks">
<h2>Implementation Tricks<a class="headerlink" href="#implementation-tricks" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-given docutils align-default" id="id7">
<caption><span class="caption-text">Some Implementation Tricks that Matter</span><a class="headerlink" href="#id7" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 63%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>trick</p></th>
<th class="head"><p>explanation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/e89d8fdc4b7340c708b48f987a8e9f312cd0f7a2/ding/rl_utils/gae.py#L26">Generalized Advantage Estimator</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Utilizing generalized advantage estimator to balance bias and variance in value learning.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/7630dbaa65e4ef33b07cc0f6c630fce280aa200c/ding/rl_utils/ppo.py#L193">Dual Clip</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">In the paper <a class="reference external" href="https://arxiv.org/abs/1912.09729">Mastering Complex Control in MOBA Games with Deep Reinforcement Learning</a>,</div>
<div class="line">the authors claim that when <span class="math notranslate nohighlight">\(\hat{A}_t &lt; 0\)</span>, a too large <span class="math notranslate nohighlight">\(r_t(\theta)\)</span> should also be clipped, which introduces dual clip:</div>
<div class="line"><span class="math notranslate nohighlight">\(\max \left(\min \left(r_{t}(\theta) \hat{A}_{t}, {clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right), c \hat{A}_{t}\right)\)</span></div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/7630dbaa65e4ef33b07cc0f6c630fce280aa200c/ding/policy/ppo.py#L171">Recompute Advantage</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">In on-policy PPO, each time we collect a batch data, we will train many epochs to improve data efficiency.</div>
<div class="line">And before the beginning of each training epoch, we recompute the advantage of historical transitions,</div>
<div class="line">to keep the advantage is an approximate evaluation of current policy.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/7630dbaa65e4ef33b07cc0f6c630fce280aa200c/ding/policy/ppo.py#L175">Value/Advantage Normalization</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">We standardize the targets of the value/advantage function using running estimates of the average</div>
<div class="line">and standard deviation of the value/advantage targets. For more implementation details about</div>
<div class="line">recompute advantage and normalization, users can refer to this <a class="reference external" href="https://github.com/opendilab/DI-engine/discussions/172#discussioncomment-1901038">discussion</a>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/e6cc06043b479b164b41189ac99c9315c0c938de/ding/rl_utils/ppo.py#L202">Value Clipping</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Value is clipped around the previous value estimates. We use the value clip_ratio same as that used to clip policy</div>
<div class="line">probability ratios in the PPO policy loss function.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/7630dbaa65e4ef33b07cc0f6c630fce280aa200c/ding/policy/ppo.py#L98">Orthogonal initialization</a></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Using an orthogonal initialization scheme for the policy and value networks.</div>
</div>
</td>
</tr>
</tbody>
</table>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<p>off-policy PPO Benchmark:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pong</p>
<p>(PongNoFrameskip-v4)</p>
</td>
<td><p>20</p></td>
<td><img alt="../_images/pong_offppo.png" src="../_images/pong_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/pong/pong_offppo_config.py">config_link_p</a></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Qbert</p>
<p>(QbertNoFrameskip-v4)</p>
</td>
<td><p>16400</p></td>
<td><img alt="../_images/qbert_offppo.png" src="../_images/qbert_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/qbert/qbert_offppo_config.py">config_link_q</a></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>SpaceInvaders</p>
<p>(SpaceInvadersNoFrame
skip-v4)</p>
</td>
<td><p>1200</p></td>
<td><img alt="../_images/spaceinvaders_offppo.png" src="../_images/spaceinvaders_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py">config_link_s</a></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Hopper</p>
<p>(Hopper-v3)</p>
</td>
<td><p>300</p></td>
<td><img alt="../_images/hopper_offppo.png" src="../_images/hopper_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/hopper_offppo_default_config.py">config_link_ho</a></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Walker2d</p>
<p>(Walker2d-v3)</p>
</td>
<td><p>500</p></td>
<td><img alt="../_images/walker2d_offppo.png" src="../_images/walker2d_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/walker2d_offppo_default_config.py">config_link_w</a></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Halfcheetah</p>
<p>(Halfcheetah-v3)</p>
</td>
<td><p>2000</p></td>
<td><img alt="../_images/halfcheetah_offppo.png" src="../_images/halfcheetah_offppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/halfcheetah_offppo_default_config.py">config_link_ha</a></p></td>
<td></td>
</tr>
</tbody>
</table>
<p>on-policy PPO Benchmark:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pong</p>
<p>(PongNoFrameskip-v4)</p>
</td>
<td><p>20</p></td>
<td><img alt="../_images/pong_onppo.png" src="../_images/pong_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/pong/pong_onppo_config.py">config_link_p</a></p></td>
<td><p>RLlib(20)</p></td>
</tr>
<tr class="row-odd"><td><p>Qbert</p>
<p>(QbertNoFrameskip-v4)</p>
</td>
<td><p>10000</p></td>
<td><img alt="../_images/qbert_onppo.png" src="../_images/qbert_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/qbert/qbert_onppo_config.py">config_link_q</a></p></td>
<td><p>RLlib(11085)</p></td>
</tr>
<tr class="row-even"><td><p>SpaceInvaders</p>
<p>(SpaceInvadersNoFrame
skip-v4)</p>
</td>
<td><p>800</p></td>
<td><img alt="../_images/spaceinvaders_onppo.png" src="../_images/spaceinvaders_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py">config_link_s</a></p></td>
<td><p>RLlib(671)</p></td>
</tr>
<tr class="row-odd"><td><p>Hopper</p>
<p>(Hopper-v3)</p>
</td>
<td><p>3000</p></td>
<td><img alt="../_images/hopper_onppo.png" src="../_images/hopper_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/hopper_onppo_default_config.py">config_link_ho</a></p></td>
<td><p>Tianshou(3127)</p>
<blockquote>
<div><p>Sb3(1567)</p>
</div></blockquote>
<p>spinningup(2500)</p>
</td>
</tr>
<tr class="row-even"><td><p>Walker2d</p>
<p>(Walker2d-v3)</p>
</td>
<td><p>3000</p></td>
<td><img alt="../_images/walker2d_onppo.png" src="../_images/walker2d_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/walker2d_onppo_default_config.py">config_link_w</a></p></td>
<td><p>Tianshou(4895)</p>
<blockquote>
<div><p>Sb3(1230)</p>
</div></blockquote>
<p>spinningup(2500)</p>
</td>
</tr>
<tr class="row-odd"><td><p>Halfcheetah</p>
<p>(Halfcheetah-v3)</p>
</td>
<td><p>3500</p></td>
<td><img alt="../_images/halfcheetah_onppo.png" src="../_images/halfcheetah_onppo.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/halfcheetah_onppo_default_config.py">config_link_ha</a></p></td>
<td><blockquote>
<div><p>Tianshou(7337)</p>
<blockquote>
<div><p>Sb3(1976)</p>
</div></blockquote>
</div></blockquote>
<p>spinningup(3000)</p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov: “Proximal Policy Optimization Algorithms”, 2017; [<a class="reference external" href="http://arxiv.org/abs/1707.06347">http://arxiv.org/abs/1707.06347</a> arXiv:1707.06347].</p></li>
<li><p>Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry: “Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO”, 2020; [<a class="reference external" href="http://arxiv.org/abs/2005.12729">http://arxiv.org/abs/2005.12729</a> arXiv:2005.12729].</p></li>
<li><p>Andrychowicz M, Raichuk A, Stańczyk P, et al. What matters in on-policy reinforcement learning? a large-scale empirical study[J]. arXiv preprint arXiv:2006.05990, 2020.</p></li>
<li><p>Ye D, Liu Z, Sun M, et al. Mastering complex control in moba games with deep reinforcement learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(04): 6672-6679.</p></li>
<li><p><a class="reference external" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">https://spinningup.openai.com/en/latest/algorithms/ppo.html</a></p></li>
</ul>
</section>
<section id="other-public-implementations">
<h2>Other Public Implementations<a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ppo/ppo.py">spinningup</a></p></li>
<li><p><a class="reference external" href="https://github.com/ray-project/ray/tree/master/python/ray/rllib/agents/ppo">RLlib (Ray)</a></p></li>
<li><p><a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py">SB3 (StableBaselines3)</a></p></li>
<li><p><a class="reference external" href="https://github.com/thu-ml/tianshou/blob/master/tianshou/policy/modelfree/ppo.py">Tianshou</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="acer.html" class="btn btn-neutral float-right" title="ACER" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="a2c.html" class="btn btn-neutral float-left" title="A2C" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
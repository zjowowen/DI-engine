

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>SQL &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="SQN" href="sqn.html" />
    <link rel="prev" title="IQN" href="iqn.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>SQL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/sql.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sql">
<h1>SQL<a class="headerlink" href="#sql" title="Permalink to this headline">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Soft Q Learning (SQL) is an off-policy maximum entropy Q learning algorithm that first proposed in <a class="reference external" href="https://arxiv.org/abs/1702.08165">Reinforcement Learning with Deep Energy-Based Policies</a>.
The energy based model is used in Soft Q-Learning, where the optimal policy expressed via a Boltzmann distribution is learned through maximizing the expectation of the cumulative reward added by an entropy term.
In this way, the resulting policies gain the advantages to try to learn all of the ways of performing the task, instead of only learning the best way to perform the task as the other traditional RL algorithms do.
The amortized Stein variational gradient descent (SVGD) has been utilized to learn
a stochastic sampling network that approximates
samples from this distribution. The features of
the algorithm include improved exploration via the maximum entropy formulation
and compositionality that allows transferring
skills between tasks.</p>
</section>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>SQL is a <strong>model-free</strong> and <strong>value-based</strong> RL algorithm.</p></li>
<li><p>SQL is an <strong>off-policy</strong> algorithm.</p></li>
<li><p>SQL supports both <strong>discrete</strong> and <strong>continuous</strong> action spaces.</p></li>
<li><p>SVGD has been adopted for sampling from the soft Q-function for environments with <strong>continuous</strong> action spaces.</p></li>
</ol>
</section>
<section id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>SQL considers a more general maximum entropy policy, such that the optimal policy aims to maximize its entropy at each visited state:</p>
<img alt="../_images/sql_policy.png" src="../_images/sql_policy.png" />
<p>where <span class="math notranslate nohighlight">\({\alpha}\)</span>   is an optional but convenient parameter that can be used to determine the relative importance of entropy and reward. In practice, <span class="math notranslate nohighlight">\({\alpha}\)</span>  is a hyperparameter that has to be tuned (not one to be learned during training).</p>
<p>By defining the soft Q function and soft V functions in equations 4 and 5 below respectively:</p>
<img alt="../_images/SQL_Q.png" src="../_images/SQL_Q.png" />
<img alt="../_images/SQL_V.png" src="../_images/SQL_V.png" />
<p>The optimal policy to the above maximum entropy formulation of the policy can be proved to be:</p>
<img alt="../_images/SQL_opt_policy.png" src="../_images/SQL_opt_policy.png" />
<p>The proof can be found in the appendix or from the paper <cite>Modeling purposeful adaptive behavior with the principle of maximum causal entropy &lt;https://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf&gt;</cite></p>
<p>The soft Q iteration for training expressive energy-based models is given by the following theorem (<cite>Theorem 3</cite> in the paper):</p>
<p>Theorem: Let <span class="math notranslate nohighlight">\(Q_{\text{soft}(\cdot, \cdot))\)</span> and <span class="math notranslate nohighlight">\(V_{\text{soft}(\cdot))\)</span> be bounded and assume that <span class="math notranslate nohighlight">\(\int_{\mathcal{A}} exp(\frac{1}{\alpha} Q_{\text{soft}}(\cdot, a^{'}) ) \,da^{'} &lt; \infty\)</span> and that <span class="math notranslate nohighlight">\(Q^{*}_{\text{soft}} &lt; \infty\)</span> exist. Then the fixed-point iteration</p>
<img alt="../_images/SQL_fi.png" src="../_images/SQL_fi.png" />
<p>converges to <span class="math notranslate nohighlight">\(Q^{*}_{\text{soft}} and V^{*}_{\text{soft}}\)</span> respectively.</p>
<p>However, there are several practicalities
that need to be considered in order to make use of the algorithm to solve real world problems.
First, the soft Bellman backup cannot be performed
exactly in continuous or large state and action spaces, and
second, sampling from the energy-based model in (6) is intractable
in general.</p>
<p>To convert the above theorem into a stochastic optimization
problem, we first express the soft value function in terms
of an expectation via importance sampling:</p>
<img alt="../_images/SQL_sto_V.png" src="../_images/SQL_sto_V.png" />
<p>where <span class="math notranslate nohighlight">\(q_{a'}\)</span> can be an arbitrary distribution over the action
space.</p>
<p>By noting the identity <span class="math notranslate nohighlight">\(g_{1}(x)=g_{2}(x) \forall x \in \mathbb{X} \Leftrightarrow  \mathbb{E}_{x\sim q}[((g_{1}(x)-g_{2}(x))^{2}]=0\)</span> , where q can be any strictly positive density function on <span class="math notranslate nohighlight">\(\mathbb{X}\)</span>, we can express the
soft Q-iteration in an equivalent form as minimizing</p>
<img alt="../_images/SQL_sto_Q.png" src="../_images/SQL_sto_Q.png" />
<p>where <span class="math notranslate nohighlight">\(q_{s_{t}}\)</span> and <span class="math notranslate nohighlight">\(q_{a_{t}}\)</span> are positive over <span class="math notranslate nohighlight">\(\mathrm{S}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{A}\)</span> respectively.</p>
<img alt="../_images/SQL_sto_Q_tar.png" src="../_images/SQL_sto_Q_tar.png" />
<p>is a target Q-value, with <span class="math notranslate nohighlight">\(V^{\bar{\theta}}_{\text{soft}}\)</span> given by the equation 10 and <span class="math notranslate nohighlight">\(\theta\)</span> being replaced by the target parameters, <span class="math notranslate nohighlight">\(\bar{\theta}\)</span>.</p>
<p>While the sampling distributions <span class="math notranslate nohighlight">\(q_{s_{t}}\)</span>, <span class="math notranslate nohighlight">\(q_{a_{t}}\)</span> and <span class="math notranslate nohighlight">\(q_{a'}\)</span> can be arbitrary, we typically use real samples from rollouts of the current policy <span class="math notranslate nohighlight">\(\pi(a_{t}|s_{t}) \propto exp(\frac{1}{\alpha} Q_{\text{soft}}^{\theta}(s_{t},a_{t}))\)</span>.</p>
<p>However, in continuous spaces, Since the form of the policy is so
general, sampling from it is intractable - We still need a tractable way to sample from the policy. Here is where SVGD comes in.</p>
<p>Formally, we want to learn a state-conditioned stochastic neural network <span class="math notranslate nohighlight">\(a_{t}=f^{\phi}(\xi,s_{t})\)</span> parametrized by <span class="math notranslate nohighlight">\(\phi\)</span>, that
maps noise samples <span class="math notranslate nohighlight">\(\xi\)</span> drawn from a normal Gaussian, or other arbitrary distribution, into unbiased action samples. We denote
the induced distribution of the actions as <span class="math notranslate nohighlight">\(\pi^{\phi}(a_{t}|s_{t})\)</span> and we want tp find parameters <span class="math notranslate nohighlight">\(\phi\)</span> so that the induced distribution approximates the energy-based distribution in terms of the
KL divergence.</p>
<img alt="../_images/SQL_sto_pi12.png" src="../_images/SQL_sto_pi12.png" />
<p>Practically, we optimise the policy by the following two equations:</p>
<img alt="../_images/SQL_sto_pi13.png" src="../_images/SQL_sto_pi13.png" />
<img alt="../_images/SQL_sto_pi14.png" src="../_images/SQL_sto_pi14.png" />
</section>
<section id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<p>The pseudo code is as follows:</p>
<img alt="../_images/SQL.png" src="../_images/SQL.png" />
<p>Where the equation 10, 11, 13, 14 can be referred from the above section.</p>
</section>
<section id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<p>SQL can be combined with:</p>
<blockquote>
<div><ul class="simple">
<li><p>Exploration techniques such as epsilon-greedy or OU Noise (implemented in the original paper; Please refer to <a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a> and <a class="reference external" href="https://link.aps.org/pdf/10.1103/PhysRev.36.823?casa_token=yFMSHBrxJoMAAAAA:5nFSMwUrqcdlUoobFDYOP0Y58r5jmNogkpHqFgMhzv0Md-4EcIkofMHHCkgsjEJFO10yqsmrhmNk_4dL">On the theory of the Brownian motion</a>) to enchance explorations.</p></li>
<li><p>Some analysts draw connection between Soft Q-learning and Policy
Gradient algorithms such as <a class="reference external" href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and
Soft Q-Learning</a>.</p></li>
<li><p>SQL can be combined with demonstration data to propose an imitation learning algorithm: SQIL proposed in <a class="reference external" href="https://arxiv.org/abs/1905.11108">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</a>.  Please also refer to <cite>SQIL code &lt;https://github.com/opendilab/DI-engine/blob/main/ding/policy/sql.py&gt;</cite> for a DI-engine implementation.</p></li>
</ul>
</div></blockquote>
</section>
<section id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The default config is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.sql.SQLPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.sql.</span></span><span class="sig-name descname"><span class="pre">SQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sql.html#SQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.sql.SQLPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of SQL algorithm.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<p>The table below shows a benchmark of the performance of DQN, SQL (in discrete action spaces), and SQIL in Lunarlander and Pong environments.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 26%" />
<col style="width: 28%" />
<col style="width: 29%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>env / method</p></th>
<th class="head"><p>DQN</p></th>
<th class="head"><p>SQL</p></th>
<th class="head"><p>SQIL</p></th>
<th class="head"><p>alpha</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LunarLander</p></td>
<td><p>153392 / 277 / 23900 (both off)
83016  / 155 / 12950 (both on)</p></td>
<td><p>693664 / 1017 / 32436 (both off)
1149592 / 1388/ 53805 (both on)</p></td>
<td><p>35856   / 238  / 1683   (both off)
31376   / 197  / 1479   (both on)</p></td>
<td><p>0.08</p></td>
</tr>
<tr class="row-odd"><td><p>Pong</p></td>
<td><p>765848 / 482 / 80000 (both on)</p></td>
<td><p>2682144 / 1750 / 278250 (both on)</p></td>
<td><p>2390608 / 1665 / 247700 (both on)</p></td>
<td><p>0.12</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The stopping values for Lunarlander and Pong are 200 and 20 respectively.</p></li>
<li><p>both on：cuda = True； base env manger = subprocess</p></li>
<li><p>both off：cuda = False； base env manager = base</p></li>
</ul>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Haarnoja, Tuomas, et al. “Reinforcement learning with deep energy-based policies.” International Conference on Machine Learning. PMLR, 2017.</p></li>
<li><p>Uhlenbeck, G. E. and Ornstein, L. S. On the theory of the brownian motion. Physical review, 36(5):823, 1930.</p></li>
<li><p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.</p></li>
<li><p>Schulman, John, Xi Chen, and Pieter Abbeel. “Equivalence between policy gradients and soft q-learning.” arXiv preprint arXiv:1704.06440 (2017).</p></li>
<li><p>Siddharth Reddy, Anca D. Dragan, Sergey Levine: “SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards”, 2019.</p></li>
<li><p>Ziebart, B. D. Modeling purposeful adaptive behavior with
the principle of maximum causal entropy. PhD thesis,
2010.</p></li>
</ul>
</section>
<section id="other-public-implementations">
<h2>Other Public Implementations<a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/rail-berkeley/softlearning">SQL release repo</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="sqn.html" class="btn btn-neutral float-right" title="SQN" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="iqn.html" class="btn btn-neutral float-left" title="IQN" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
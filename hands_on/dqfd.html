

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>DQfD &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="SQIL" href="sqil.html" />
    <link rel="prev" title="HER" href="her.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index.html">Middleware code specification</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">RL Algorithm Cheat Sheet</a> &raquo;</li>
        
      <li>DQfD</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/dqfd.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="dqfd">
<h1>DQfD<a class="headerlink" href="#dqfd" title="Permalink to this headline">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>DQfD was proposed in <a class="reference external" href="https://arxiv.org/abs/1704.03732">Deep Q-learning from Demonstrations</a> by DeepMind, which appeared at AAAI 2018.
It ﬁrst pretrains solely on demonstration data, using a combination of 1-step TD, n-step TD, supervised, and regularization losses so that it has a reasonable policy that is a good starting point for learning in the task. Once it starts interacting with the task, it continues learning by sampling from both its selfgenerated data as well as the demonstration data.
The ratio of both types of data in each mini-batch is automatically controlled by a prioritized-replay mechanism.</p>
<p>DQfD leverages small sets of demonstration data to massively accelerate the learning process and performs better than PDD DQN, RBS, HER and ADET on Atari games.</p>
</section>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>DQfD is an extension algorithm of DQN.</p></li>
<li><p>Store the demonstrations into an expert replay buffer.</p></li>
<li><p>Pre-train the network with expert demonstrations and accelerate the subsequent RL training process.</p></li>
<li><p>Agent gathers more transitions for new replay buffer (see <a class="reference internal" href="#detail-explanation">detail_explanation</a>). Trains network on mixture of new replay buffer and expert replay buffer.</p></li>
<li><p>Network is trained with special loss function made up of four parts: one-step loss, n-step loss, expert large margin classification loss and L2 regularization.</p></li>
</ol>
</section>
<section id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>The DQfD overall loss used to update the network is a combination of all four losses.</p>
<p>Overall Loss:  <span class="math notranslate nohighlight">\(J(Q) = J_{DQ}(Q) + \lambda_1 J_n(Q) + \lambda_2J_E(Q) + \lambda_3 J_{L2}(Q)\)</span></p>
<ul class="simple">
<li><p>one-step loss:  <span class="math notranslate nohighlight">\(J_{DQ}(Q) = (R(s,a) + \gamma Q(s_{t+1}, a_{t+1}^{max}; \theta ^') - Q(s,a;\theta))^2\)</span>, where <span class="math notranslate nohighlight">\(a_{t+1}^{max} = argmax_a Q(s_{t+1},a;\theta)\)</span>.</p></li>
<li><p>n-step loss:  <span class="math notranslate nohighlight">\(J_n(Q) = r_t + \gamma r_{t+1} + ... + \gamma^{n-1} r_{t+n-1} + max_a \gamma^n Q(s_{t+n},a)\)</span>.</p></li>
<li><p>large margin classiﬁcation loss: <span class="math notranslate nohighlight">\(J_E(Q) = max_{a \in A}[Q(s,a) + L(a_E,a)] - Q(s,a_E)\)</span>, <span class="math notranslate nohighlight">\(L(a_E,a)\)</span> is a margin function that is 0 when <span class="math notranslate nohighlight">\(a = a_E\)</span> and positive otherwise. This loss forces the values of the other actions to be at least a margin lower than the value of the demonstrator’s action.</p></li>
<li><p>L2 regularization loss: <span class="math notranslate nohighlight">\(J_{L2}(Q)\)</span> help prevent from over-ﬁtting.</p></li>
</ul>
</section>
<section id="pseudo-code">
<h2>Pseudo-code<a class="headerlink" href="#pseudo-code" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/DQfD.png"><img alt="../_images/DQfD.png" class="align-center" src="../_images/DQfD.png" style="width: 613.0px; height: 713.0px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>In Phase I, the agent just uses the demonstration data, and does not do any exploration. The goal of the pre-training phase is to learn to imitate the demonstrator with a value function that satisﬁes the Bellman equation. During this pre-training phase, the agent samples mini-batches from the demonstration data and updates the network by applying the total loss J(Q).</p></li>
</ul>
</div>
<blockquote id="detail-explanation">
<div><ul class="simple">
<li><p>In Phase II, the agent starts acting on the system, collecting self-generated data, and adding it to its replay buffer. Data is added to the replay buffer until it is full, and then the agent starts overwriting old data in that buffer. However, the agent never over-writes the demonstration data. All the losses are applied to the demonstration data in both phases, while the supervised loss is not applied to self-generated data.</p></li>
</ul>
</div></blockquote>
</section>
<section id="extensions">
<h2>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h2>
<p>DeepMind has extended DQfD in several ways. Upon a literature search, it seems like two relevant follow-up works are:</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1803.00933">Distributed Prioritized Experience Replay</a></p>
<blockquote>
<div><p>The main idea of this paper is to scale up the experience replay data by having many actors collect experience. Their framework is called <strong>Ape-X</strong>, and they claim that Ape-X DQN achieves a new state of the art performance on Atari games. This paper is not that particularly relevant to DQfD, but we include it here mainly because a follow-up paper (see below) used this technique with DQfD.</p>
</div></blockquote>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1805.11593">Observe and Look Further: Achieving Consistent Performance on Atari</a></p>
<blockquote>
<div><p>This paper proposes the <strong>Ape-X DQfD</strong> algorithm, which as one might expect combines DQfD with the distributed prioritized experience replay algorithm.</p>
</div></blockquote>
</li>
</ul>
</section>
<section id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The DI-engine implements <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py">DQfD</a>.</p>
<p>The default config of DQfD Policy is defined as follows:</p>
<p>The network interface DQfD used is defined as follows:</p>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Pong
(PongNoFrameskip-v4)</p></td>
<td><p>20</p></td>
<td><img alt="../_images/dqfd_pong.png" src="../_images/dqfd_pong.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/pong/pong_dqfd_config.py">config_link_p</a></p></td>
<td><dl class="simple">
<dt>Tianshou(20)</dt><dd><p>Sb3(20)</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>Qbert
(QbertNoFrameskip-v4)</p></td>
<td><p>2356</p></td>
<td><img alt="../_images/dqfd_qbert.png" src="../_images/dqfd_qbert.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/qbert/qbert_dqfd_config.py">config_link_q</a></p></td>
<td><dl class="simple">
<dt>Tianshou(7307)</dt><dd><p>Sb3(9496)</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td><p>SpaceInvaders
(SpaceInvadersNoFrame
skip-v4)</p></td>
<td><p>1371</p></td>
<td><img alt="../_images/dqfd_spaceinvaders.png" src="../_images/dqfd_spaceinvaders.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py">config_link_s</a></p></td>
<td><dl class="simple">
<dt>Tianshou(812.2)</dt><dd><p>Sb3(622)</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Hester T, Vecerik M, Pietquin O, et al. Deep q-learning from demonstrations[C]//Thirty-second AAAI conference on artificial intelligence. 2018.</p></li>
<li><p>Blog:  <a class="reference external" href="https://danieltakeshi.github.io/2019/04/30/il-and-rl/">Combining Imitation Learning and Reinforcement Learning Using DQfD</a></p></li>
</ul>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="sqil.html" class="btn btn-neutral float-right" title="SQIL" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="her.html" class="btn btn-neutral float-left" title="HER" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>R2D3 &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MBPO" href="mbpo.html" />
    <link rel="prev" title="Guided Cost Learning" href="guided_cost_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index_zh.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">使用者指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index_zh.html">安装说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index_zh.html">快速上手</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习算法攻略合集</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dqn_zh.html">DQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="rainbow.html">Rainbow</a></li>
<li class="toctree-l2"><a class="reference internal" href="c51.html">C51</a></li>
<li class="toctree-l2"><a class="reference internal" href="qrdqn.html">QRDQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="iqn.html">IQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="sql.html">SQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="sqn.html">SQN</a></li>
<li class="toctree-l2"><a class="reference internal" href="r2d2.html">R2D2</a></li>
<li class="toctree-l2"><a class="reference internal" href="a2c.html">A2C</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppo.html">PPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="acer.html">ACER</a></li>
<li class="toctree-l2"><a class="reference internal" href="impala.html">IMPALA</a></li>
<li class="toctree-l2"><a class="reference internal" href="ppg.html">PPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="ddpg.html">DDPG</a></li>
<li class="toctree-l2"><a class="reference internal" href="d4pg.html">D4PG</a></li>
<li class="toctree-l2"><a class="reference internal" href="td3.html">TD3</a></li>
<li class="toctree-l2"><a class="reference internal" href="sac.html">SAC</a></li>
<li class="toctree-l2"><a class="reference internal" href="cql.html">CQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="td3_bc.html">TD3BC</a></li>
<li class="toctree-l2"><a class="reference internal" href="qmix.html">QMIX</a></li>
<li class="toctree-l2"><a class="reference internal" href="coma.html">COMA</a></li>
<li class="toctree-l2"><a class="reference internal" href="wqmix.html">WQMIX</a></li>
<li class="toctree-l2"><a class="reference internal" href="collaq.html">CollaQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="atoc.html">ATOC</a></li>
<li class="toctree-l2"><a class="reference internal" href="rnd.html">RND</a></li>
<li class="toctree-l2"><a class="reference internal" href="her.html">HER</a></li>
<li class="toctree-l2"><a class="reference internal" href="dqfd.html">DQfD</a></li>
<li class="toctree-l2"><a class="reference internal" href="sqil.html">SQIL</a></li>
<li class="toctree-l2"><a class="reference internal" href="gail.html">GAIL</a></li>
<li class="toctree-l2"><a class="reference internal" href="trex.html">TREX</a></li>
<li class="toctree-l2"><a class="reference internal" href="icm_zh.html">ICM</a></li>
<li class="toctree-l2"><a class="reference internal" href="guided_cost_zh.html">Guided Cost Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">R2D3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">核心要点</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">关键方程或关键框图</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">伪代码</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">重要的实现细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">基准算法性能</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="mbpo.html">MBPO</a></li>
<li class="toctree-l2"><a class="reference internal" href="vpn.html">VPN</a></li>
<li class="toctree-l2"><a class="reference internal" href="mcts.html">MCTS</a></li>
<li class="toctree-l2"><a class="reference internal" href="alphago.html">AlphaGo</a></li>
<li class="toctree-l2"><a class="reference internal" href="alphago.html#alphagozero">AlphaGoZero</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index_zh.html">强化学习环境示例手册</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index_zh.html">分布式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_zh.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index_zh.html">特性介绍</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index_zh.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index_zh.html">中间件（middleware）编写规范</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index_zh.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index_zh.html">Docs</a> &raquo;</li>
        
          <li><a href="index_zh.html">强化学习算法攻略合集</a> &raquo;</li>
        
      <li>R2D3</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/hands_on/r2d3_zh.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="r2d3">
<h1>R2D3<a class="headerlink" href="#r2d3" title="Permalink to this headline">¶</a></h1>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>R2D3 (Recurrent Replay Distributed DQN from Demonstrations) 首次在论文
<a class="reference external" href="https://arxiv.org/abs/1909.01387">Making Efficient Use of Demonstrations to Solve Hard Exploration Problems</a> 中提出, 它可以有效地利用专家演示轨迹来解决具有以下3个属性的问题：初始条件高度可变、部分可观察、困难探索。
此外他们还介绍了一组结合这三个属性的八个任务，并表明R2D3可以解决像这些任务，值得注意的是，在类似这样的任务上，其他一些最先进的方法，无论有还是没有专家演示轨迹，在数百亿次的探索步骤之后甚至可能
无法看到一条成功的轨迹。R2D3本质上是有效结合了R2D2算法的分布式框架和循环神经网络结构，以及DQfD中为从专家轨迹中学习而特别设计的损失函数。</p>
</section>
<section id="id2">
<h2>核心要点<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>1.R2D3的基线强化学习算法是 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2.py">R2D2</a>, 可以参考我们的实现 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2.py">r2d2</a> ,
它本质上是一个基于分布式框架，采用了双Q网络(Double Q Networks), 决斗Q结构(Dueling Architecture)，多步时间差分损失函数(n-step TD loss)的DQN算法。</p>
<p>2.R2D3利用了DQfD的损失函数，包括：一步和n步的时序差分损失，神经网络参数的L2正则化损失(可选)，监督大间隔分类损失(supervised large margin classification loss)。
主要区别在于R2D3损失函数中的所有Q值都是序列样本通过循环神经Q网络后计算得到的，而原始DQfD中的Q值是通过一步的样本通过卷积网络和(或)前向全连接网络得到。</p>
<p>3.由于R2D3是在序列样本上进行运算的，所以其专家轨迹也应该以序列样本的方式给出。在具体实现中，我们往往是用另一个基线强化学习算法(如PPO或R2D2)收敛后得到的专家模型来
产生对应的专家演示轨迹，为此我们专门写了对应的策略函数来从这样的专家模型中产生专家演示,
参见 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo_offpolicy_collect_traj.py">ppo_offpolicy_collect_traj.py</a> 和 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/r2d2_collect_traj.py">r2d2_collect_traj.py</a> .</p>
<p>4.在训练Q网络时，采样的mini-batch中的每一条序列样本，有pho的概率是专家演示序列样本，有1-pho的概率是智能体与环境交互的经验序列样本。</p>
<p>5.R2D3的提出是为了解决初始条件高度可变、部分可观察环境中的困难探索问题，其他探索相关的论文，读者可以参考 <a class="reference external" href="https://arxiv.org/abs/2002.06038">NGU</a> ，它是融合了
<a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">ICM</a> 和 <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">RND</a> 等多种探索方法的一个综合体。</p>
<section id="id4">
<h3>关键方程或关键框图<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>R2D3算法的总体分布式训练流程如下：</p>
<a class="reference internal image-reference" href="../_images/r2d3_overview.png"><img alt="../_images/r2d3_overview.png" class="align-center" src="../_images/r2d3_overview.png" style="width: 440.8px; height: 284.0px;" /></a>
<p>learner中用于训练Q网络而采样的mini_batch包含了2部分: 1. 专家演示轨迹, 2. 智能体在训练过程中与环境交互产生的经验轨迹。
专家演示和智能体经验之间的比率是一个关键的超参数, 必须仔细调整以实现良好的性能。</p>
<p>R2D3算法的Q网络结构图如下：</p>
<a class="reference internal image-reference" href="../_images/r2d3_q_net.png"><img alt="../_images/r2d3_q_net.png" class="align-center" src="../_images/r2d3_q_net.png" style="width: 549.6px; height: 302.40000000000003px;" /></a>
<p>(a)R2D3智能体使用的recurrent head。 (b)DQfD智能体使用的feedforward head。(c)表示输入的是大小为96x72的图像帧，
接着通过一个ResNet，然后将前一时刻的动作，前一时刻的奖励和当前时刻的其他本体感受特征(proprioceptive features) <span class="math notranslate nohighlight">\(f_{t}\)</span> （包括加速度、avatar是否握住物体以及手与avatar的相对距离等辅助信息
连接(concat)为一个新的向量，传入a)和b)中的head，用于计算Q值。</p>
<p>下面描述r2d3的损失函数设置，和DQfD一样，不过这里所有的Q值都是通过上面所描述的循环神经网络计算得到。包括：
一步时序差分损失，n步时序差分损失，监督大间隔分类损失，神经网络参数的L2正则化损失(可选)。
时序差分损失确保网络满足贝尔曼方程，监督损失用于使得专家演示者的动作Q值至少比其他动作的Q值高一个间隔(一个常数值)，网络权重和偏差的L2正则化损失用于防止Q网络在相对数量较小的专家演示数据集上过拟合。</p>
<ul class="simple">
<li><p>除了通常的1-step turn, R2D3还增加n-step return，有助于将专家轨迹的Q值传播到所有的早期的状态，从而获得更好的学习效果。
n步return为：</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/r2d3_nstep_return.png"><img alt="../_images/r2d3_nstep_return.png" class="align-center" src="../_images/r2d3_nstep_return.png" style="width: 363.20000000000005px; height: 34.4px;" /></a>
<ul>
<li><p>监督损失对于训练的效果至关重要。由于存在以下的情况：
1.专家演示数据可能只覆盖完整状态空间的一小部分,
2.数据中并不包含,(某一特定状态，所有可能执行的动作)的状态动作对,
因此许多 <em>状态动作对</em> 从未在专家样本中出现过。如果我们仅使用Q-learning的loss朝着下一个状态的最大Q值方向来更新Q网络，网络将倾向于朝着那些不准确的Q值中的最高方向更新,
并且网络将在整个学习过程中通过Q函数传播这些误差，造成误差累计造成过估计问题。这里采用了 <a class="reference external" href="https://arxiv.org/pdf/1606.01128.pdf">监督大边际分类损失(supervised large margin classification loss)</a>  来缓解这个问题，
其计算公式为：</p>
<a class="reference internal image-reference" href="../_images/r2d3_slmcl.png"><img alt="../_images/r2d3_slmcl.png" class="align-center" src="../_images/r2d3_slmcl.png" style="width: 341.6px; height: 38.400000000000006px;" /></a>
<p>其中 <span class="math notranslate nohighlight">\(a_{E}\)</span> 表示专家执行的动作。 <span class="math notranslate nohighlight">\(l(a_{E}, a)\)</span>  是一个边际函数，当:math:<cite>a = a_{E}</cite> 时为 0，否则为一个正的常数。
最小化这个监督损失，可以迫使除专家演示者执行的动作以外的 <strong>其他动作的Q值至少比专家演示者的动作Q值低一个间隔</strong>。
通过加上这个损失，将专家数据集合中没有遇到过的动作的Q值变成合理范围内的值，并使学得的值函数导出的贪婪策略模仿专家演示者的策略。</p>
<p>我们在DI-engine中的具体实现如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">margin_function</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
<span class="n">JE</span> <span class="o">=</span> <span class="n">is_expert</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span> <span class="o">+</span> <span class="n">l</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_s_a</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>最终用于更新Q网络的整体损失是以上所有四种损失的线性组合：</p>
<a class="reference internal image-reference" href="../_images/r2d3_loss.png"><img alt="../_images/r2d3_loss.png" class="align-center" src="../_images/r2d3_loss.png" style="width: 365.6px; height: 32.800000000000004px;" /></a>
</section>
<section id="id5">
<h3>伪代码<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>下面是R2D3智能体learner和actor的伪代码。单个学习器进程(learner process)从专家演示缓冲区和智能体经验缓冲区中采样数据样本用于计算损失函数，更新其Q网络参数。
A个并行的行动者进程(actor process)与不同的独立的A个环境实例交互以快速获得多样化的数据，然后将数据放入智能体经验缓冲区。
A个actor会定期获取learner上最新的参数。</p>
<a class="reference internal image-reference" href="../_images/r2d3_pseudo_code_actor.png"><img alt="../_images/r2d3_pseudo_code_actor.png" class="align-center" src="../_images/r2d3_pseudo_code_actor.png" style="width: 691.2px; height: 126.4px;" /></a>
<a class="reference internal image-reference" href="../_images/r2d3_pseudo_code_learner.png"><img alt="../_images/r2d3_pseudo_code_learner.png" class="align-center" src="../_images/r2d3_pseudo_code_learner.png" style="width: 700.8000000000001px; height: 269.6px;" /></a>
</section>
<section id="id6">
<h3>重要的实现细节<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>1. 用于计算损失函数的mini-batch是从专家演示缓冲区和智能体经验缓冲区采样得到的，mini-batch包含&lt;batch_size&gt;个序列样本，以pho的概率从专家演示缓冲区中采样，以1-pho的概率从智能体经验缓冲区中采样。
其具体实现方式如下，通过从&lt;batch_size&gt;大小个的[0，1]均匀分布中采样，如果采样值大于pho则选择一个专家演示轨迹
这&lt;batch_size&gt;个采样值中大于pho的采样值的个数即为本次mini-batch中专家演示所占的个数。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The hyperparameter pho, the demo ratio, control the propotion of data coming</span>
<span class="c1"># from expert demonstrations versus from the agent&#39;s own experience.</span>
<span class="n">expert_batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">pho</span>
               <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">agent_batch_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">))</span> <span class="o">-</span> <span class="n">expert_batch_size</span>
<span class="n">train_data_agent</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">agent_batch_size</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
<span class="n">train_data_expert</span> <span class="o">=</span> <span class="n">expert_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">expert_batch_size</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
</pre></div>
</div>
<p>2.由于基线算法R2D2采用有优先级的采样，对于一个序列样本，每一时刻的TD error是 使用1步TD error和n步TD error的和的绝对值，TD error在这个序列经历的所有时刻上的平均值和最大值的加权和
作为整个序列样本的优先级。由于专家数据和经验数据对应的loss函数不一样， 在R2D2中我们设置了独立的2个replay_buffer, 分别为专家演示的 <code class="docutils literal notranslate"><span class="pre">expert_buffer</span></code> ，和智能体经验的 <code class="docutils literal notranslate"><span class="pre">replay_buffer</span></code> ，
并且分开进行优先级采样和buffer中相关参数的更新。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># using the mixture of max and mean absolute n-step TD-errors as the priority of the sequence</span>
<span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">td_error</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.9</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">td_error</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">td_error</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>
<span class="c1"># td_error shape list(&lt;self._unroll_len_add_burnin_step-self._burnin_step-self._nstep&gt;, B), for example, (75,64)</span>
<span class="c1"># torch.sum(torch.stack(td_error), dim=0) can also be replaced with sum(td_error)</span>
<span class="o">...</span>
<span class="k">if</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;priority&#39;</span><span class="p">):</span>
    <span class="c1"># When collector, set replay_buffer_idx and replay_unique_id for each data item, priority = 1.\</span>
    <span class="c1"># When learner, assign priority for each data item according their loss</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">][</span>
        <span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">][</span>
        <span class="mi">0</span><span class="p">:</span><span class="n">agent_batch_size</span><span class="p">]</span>

    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;priority&#39;</span><span class="p">][</span><span class="n">agent_batch_size</span><span class="p">:]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_buffer_idx&#39;</span><span class="p">][</span>
        <span class="n">agent_batch_size</span><span class="p">:]</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">[</span><span class="s1">&#39;replay_unique_id&#39;</span><span class="p">][</span>
        <span class="n">agent_batch_size</span><span class="p">:]</span>

    <span class="c1"># Expert data and demo data update their priority separately.</span>
    <span class="n">replay_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info_agent</span><span class="p">)</span>
    <span class="n">expert_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info_expert</span><span class="p">)</span>
</pre></div>
</div>
<p>3.对于专家演示样本和智能体经验样本，我们分别对原数据增加一个键 <code class="docutils literal notranslate"><span class="pre">is_expert</span></code> 加以区分, 如果是专家演示样本，此键值为1，
如果是智能体经验样本，此键值为0，</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 如果是专家演示样本，此键值为1，</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">expert_data</span><span class="p">)):</span>
    <span class="c1"># for rnn/sequence-based alg.</span>
    <span class="n">expert_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;is_expert&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">expert_cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">unroll_len</span>
<span class="o">...</span>
<span class="c1"># 如果是智能体经验样本，此键值为0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_data</span><span class="p">)):</span>
    <span class="n">new_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;is_expert&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">expert_cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">collect</span><span class="o">.</span><span class="n">unroll_len</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>预训练。在智能体与环境交互之前，我们可以先利用专家演示样本预训练Q网络，期望能得到一个好的初始化参数，加速后续的训练进程。</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">per_train_iter_k</span><span class="p">):</span>  <span class="c1"># pretrain</span>
    <span class="k">if</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">should_eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">):</span>
        <span class="n">stop</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">,</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="c1"># Learn policy from collected demo data</span>
    <span class="c1"># Expert_learner will train ``update_per_collect == 1`` times in one iteration.</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">expert_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">),</span> <span class="n">learner</span><span class="o">.</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="n">learner</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">collector</span><span class="o">.</span><span class="n">envstep</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">learner</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_attribute</span><span class="p">(</span><span class="s1">&#39;priority&#39;</span><span class="p">):</span>
        <span class="n">expert_buffer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="n">priority_info</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id7">
<h3>实现<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>r2d3的策略 <code class="docutils literal notranslate"><span class="pre">R2D3Policy</span></code> 的接口定义如下：</p>
<p>dqfd的损失函数 <code class="docutils literal notranslate"><span class="pre">nstep_td_error_with_rescale</span></code> 的接口定义如下：</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>我们目前的r2d3策略实现中网络的输入只是时刻t的状态观测，不包含时刻t-1的动作和奖励, 也不包括额外的信息向量 <span class="math notranslate nohighlight">\(f_{t}\)</span> .</p>
</div>
</section>
<section id="id8">
<h3>基准算法性能<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>我们在PongNoFrameskip-v4环境上，做了一系列对比实验，以验证：1.用于训练的一个mini-batch中专家样本的占比pho, 2.专家演示所占比例, 3.是否利用预训练与l2正则化等不同参数设置对r2d3算法最终性能的影响。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>我们的专家数据通过 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo_offpolicy_collect_traj.py">ppo_offpolicy_collect_traj.py</a> 产生,
其专家模型来自于r2d2算法在该环境上训练到收敛后得到的专家模型。以下所有实验seed=0。</p>
<p>r2d2基线算法设置记为r2d2_n5_bs2_ul40_upc8_tut0.001_ed1e5_rbs1e5_bs64, 其中：</p>
<ul class="simple">
<li><p>n表示nstep,</p></li>
<li><p>bs表示burnin_step,</p></li>
<li><p>ul表示unroll_len,</p></li>
<li><p>upc表示update_per_collect,</p></li>
<li><p>tut表示target_update_theta,</p></li>
<li><p>ed表示eps_decay,</p></li>
<li><p>rbs表示replay_buffer_size,</p></li>
<li><p>bs表示batch_size,</p></li>
</ul>
<p>具体参见 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/pong/pong_r2d2_config.py">r2d2 pong config</a> .</p>
</div>
<ul>
<li><dl class="simple">
<dt>测试在用于训练的一个mini-batch中专家样本的占比的影响。观测1: pho需要适中，取1/4</dt><dd><ul class="simple">
<li><p>蓝线 pong_r2d2_rbs1e4</p></li>
<li><p>橙线 pong_r2d3_r2d2expert_k0_pho1-4_rbs1e4_1td_l2_ds5e3</p></li>
<li><p>灰线 pong_r2d3_r2d2expert_k0_pho1-16_rbs1e4_1td_l2_ds5e3</p></li>
<li><p>红线 pong_r2d3_r2d2expert_k0_pho1-2_rbs1e4_1td_l2_ds5e3</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_pho.png"><img alt="../_images/r2d3_pong_pho.png" class="align-center" src="../_images/r2d3_pong_pho.png" style="width: 684.0px; height: 161.0px;" /></a>
</li>
<li><dl class="simple">
<dt>测试总的专家样本库的大小的影响。观测2：demo size需要适中，取5e3</dt><dd><ul class="simple">
<li><p>橙线 pong_r2d2_rbs2e4</p></li>
<li><p>天蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds5e3</p></li>
<li><p>蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds1e3</p></li>
<li><p>绿线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2_ds1e4</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_demosize.png"><img alt="../_images/r2d3_pong_demosize.png" class="align-center" src="../_images/r2d3_pong_demosize.png" style="width: 818.0px; height: 195.0px;" /></a>
</li>
<li><dl class="simple">
<dt>测试是否预训练以及L2正则化的影响。观测3：预训练和L2正则化影响不大</dt><dd><ul class="simple">
<li><p>橙线 r2d2_rbs2e4_rbs2e4</p></li>
<li><p>蓝线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_l2</p></li>
<li><p>粉红线 pong_r2d3_r2d2expert_k0_pho1-4_rbs2e4_1td_nol2</p></li>
<li><p>深红线 pong_r2d3_r2d2expert_k100_pho1-4_rbs2e4_1td_l2</p></li>
<li><p>绿线 pong_r2d3_r2d2expert_k100_pho1-4_rbs2e4_1td_nol2</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/r2d3_pong_l2_pretrain.png"><img alt="../_images/r2d3_pong_l2_pretrain.png" class="align-center" src="../_images/r2d3_pong_l2_pretrain.png" style="width: 683.0px; height: 160.0px;" /></a>
</li>
</ul>
</section>
<section id="id10">
<h3>参考资料<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Paine T L, Gulcehre C, Shahriari B, et al. Making efficient use of demonstrations to solve hard exploration problems[J]. arXiv preprint arXiv:1909.01387, 2019.</p></li>
<li><p>Kapturowski S, Ostrovski G, Quan J, et al. Recurrent experience replay in distributed reinforcement learning[C]//International conference on learning representations. 2018.</p></li>
<li><p>Badia A P, Sprechmann P, Vitvitskyi A, et al. Never give up: Learning directed exploration strategies[J]. arXiv preprint arXiv:2002.06038, 2020.</p></li>
<li><p>Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">https://arxiv.org/abs/1810.12894v1</a>. arXiv:1810.12894, 2018.</p></li>
<li><p>Pathak D, Agrawal P, Efros A A, et al. Curiosity-driven exploration by self-supervised prediction[C]//International conference on machine learning. PMLR, 2017: 2778-2787.</p></li>
<li><p>Piot, B.; Geist, M.; and Pietquin, O. 2014a. Boosted bellman residual minimization handling expert demonstrations. In European Conference on Machine Learning (ECML).</p></li>
</ul>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mbpo.html" class="btn btn-neutral float-right" title="MBPO" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="guided_cost_zh.html" class="btn btn-neutral float-left" title="Guided Cost Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
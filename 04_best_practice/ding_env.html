


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>How to migrate your own environment to DI-engine &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="How to customize the neural network model" href="custom_model.html" />
  <link rel="prev" title="Best Practice" href="index.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">Best Practice</a> &gt;</li>
        
      <li>How to migrate your own environment to DI-engine</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/04_best_practice/ding_env.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="how-to-migrate-your-own-environment-to-di-engine">
<h1>How to migrate your own environment to DI-engine<a class="headerlink" href="#how-to-migrate-your-own-environment-to-di-engine" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">DI-zoo</span></code> provides users with a large number of commonly used environments for reinforcement learning（ <a class="reference external" href="https://github.com/opendilab/DI-engine#environment-versatility">supported environments</a> ），but in many research and engineering scenarios, users still need to implement an environment by themselves, and expect to quickly migrate it to <code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> to meet the relevant specifications of <code class="docutils literal notranslate"><span class="pre">DI-engine</span></code>. Therefore, in this section, we will introduce how to perform the above migration step by step to meet the specification of the environment base class  <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code>  of  <code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> , so that it can be easily applied in the training pipeline.</p>
<p>The following introduction will start with <strong>Basic</strong> and <strong>Advanced</strong> . <strong>Basic</strong> describes the functions that must be implemented, and the details that you should pay attention to ; <strong>Advanced</strong> describes some extended functions.</p>
<p>Then <code class="docutils literal notranslate"><span class="pre">DingEnvWrapper</span></code> will be introduced , it is a “tool” that can quickly convert simple environments such as ClassicControl, Box2d, Atari, Mujoco, GymHybrid, etc. into environments that conform to <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code>. And there is a Q &amp; A at the end.</p>
<section id="basic">
<h2>Basic<a class="headerlink" href="#basic" title="Permalink to this headline">¶</a></h2>
<p>This section describes the specification constraints that users <strong>MUST</strong> meet, and the features that must be implemented when migrating environments.</p>
<p>If you want to use the environment in the DI-engine, you need to implement a subclass environment that inherits from  <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code>, such as  <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code>. The relationship between  <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code>  and your own environment is a <a class="reference external" href="https://en.wikipedia.org/wiki/Object_composition">composition</a> relationship, that is, within a  <code class="docutils literal notranslate"><span class="pre">YourEnv</span></code> instance, there will be an instance of an environment that is native to the user (eg, a gym-type environment).</p>
<p>Reinforcement learning environments have some common major interfaces that are implemented by most environments, such as <code class="docutils literal notranslate"><span class="pre">reset()</span></code>, <code class="docutils literal notranslate"><span class="pre">step()</span></code>, <code class="docutils literal notranslate"><span class="pre">seed()</span></code>, etc. In DI-engine, <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> will further encapsulate these interfaces. In most cases, Atari will be used as an example to illustrate. For specific code, please refer to <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/envs/atari_env.py">Atari Env</a>  and  <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/envs/atari_wrappers.py">Atari Env Wrapper</a></p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__()</span></code></p>
<p>In general, the environment may be instantiated in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, <strong>but</strong> in DI-engine, in order to facilitate the support of “environment vectorization” modules like <code class="docutils literal notranslate"><span class="pre">EnvManager</span></code> , the environment instances generally use the <strong>Lazy Init</strong> mechanism, that is, the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method does not initialize the real original environment instance, but only sets the relevant <strong>parameter configuration</strong>. When the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method is called for the first time , the actual environment initialization will take place.</p>
<p>Take Atari for example. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> does not instantiate the environment, it just sets the parameter configuration value <code class="docutils literal notranslate"><span class="pre">self._cfg</span></code>, and initializes the variable <code class="docutils literal notranslate"><span class="pre">self._init_flag</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> (indicating that the environment has not been instantiated).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">seed()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">seed</span></code> is used to set the random seed in the environment. There are two types of the random seed in the environment that need to be set, one is the random seed of the <strong>original environment</strong>, the other is the library seeds (e.g. <code class="docutils literal notranslate"><span class="pre">random</span></code> , <code class="docutils literal notranslate"><span class="pre">np.random</span></code>, etc.) in various <strong>environment transformations</strong>.</p>
<p>For the second type, the setting of the seed of the random library is relatively simple, and it is set directly in the <code class="docutils literal notranslate"><span class="pre">seed</span></code> method of the environment.</p>
<p>For the first type, the seed of the original environment is only assigned in the <code class="docutils literal notranslate"><span class="pre">seed</span></code> method, but not really set; the real setting is inside the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method of the calling environment, the specific original environment <code class="docutils literal notranslate"><span class="pre">reset</span></code> before setting.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dynamic_seed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">seed</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_seed</span> <span class="o">=</span> <span class="n">dynamic_seed</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span><span class="p">)</span>
</pre></div>
</div>
<p>For the seeds of the original environment, DI-engine has the concepts of <strong>static seeds</strong> and <strong>dynamic seeds</strong>.</p>
<p><strong>Static seed</strong> is used in the test environment (evaluator_env) to ensure that the random seed of all episodes are the same, that is, only the fixed static seed value of <code class="docutils literal notranslate"><span class="pre">self._seed</span></code> is used when <code class="docutils literal notranslate"><span class="pre">reset</span></code>. Need to manually pass the <code class="docutils literal notranslate"><span class="pre">dynamic_seed</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> in the <code class="docutils literal notranslate"><span class="pre">seed</span></code> method.</p>
<p><strong>Dynamic seed</strong> is used for the training environment (collector_env), try to make the random seed of each episode different, that is, when <code class="docutils literal notranslate"><span class="pre">reset</span></code>, a random number generator will be used <code class="docutils literal notranslate"><span class="pre">100</span> <span class="pre">*</span> <span class="pre">np.random.randint(1,</span> <span class="pre">1000)</span></code> (but the seed of this random number generator is fixed by the environment’s <code class="docutils literal notranslate"><span class="pre">seed</span></code> method, so the reproducibility of the experiment is guaranteed). You need to manually pass in the <code class="docutils literal notranslate"><span class="pre">dynamic_seed</span></code> parameter as <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">seed</span></code> (or you can not pass it, because the default parameter is <code class="docutils literal notranslate"><span class="pre">True</span></code>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">reset()</span></code></p>
<p>The <strong>Lazy Init</strong> initialization method of DI-engine has been introduced in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, that is, the actual environment initialization is performed when <strong>the first call</strong> <code class="docutils literal notranslate"><span class="pre">reset</span></code> method is performed.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">reset</span></code> method will judge whether the actual environment needs to be instantiated according to <code class="docutils literal notranslate"><span class="pre">self._init_flag</span></code> (if it is <code class="docutils literal notranslate"><span class="pre">False</span></code>, it will be instantiated; otherwise, it has already been instantiated and can be used directly), and Set the random seed, then call the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method of the original environment to get the observation value <code class="docutils literal notranslate"><span class="pre">obs</span></code> in the initial state, and convert it to the <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> data format (will be explained in detail in 4) , and initialize the value of <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> (will be explained in detail in 5), in Atari <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> refers to the cumulative sum of the real rewards obtained by a whole episode, used to evaluate the agent Performance on this environment, not used for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">False</span>

   <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_env</span><span class="p">(</span><span class="n">only_info</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_init_flag</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_seed&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_dynamic_seed&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic_seed</span><span class="p">:</span>
         <span class="n">np_seed</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">+</span> <span class="n">np_seed</span><span class="p">)</span>
      <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_seed&#39;</span><span class="p">):</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span><span class="p">)</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="k">return</span> <span class="n">obs</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">step()</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">step</span></code> method is responsible for receiving the <code class="docutils literal notranslate"><span class="pre">action</span></code> of the current timestep, and then giving the <code class="docutils literal notranslate"><span class="pre">reward</span></code> of the current timestep and the <code class="docutils literal notranslate"><span class="pre">obs</span></code> of the next timestep. In DI-engine, you also need to give: The flag <code class="docutils literal notranslate"><span class="pre">done</span></code> of whether the current episode ends (here requires <code class="docutils literal notranslate"><span class="pre">done</span></code> must be of type <code class="docutils literal notranslate"><span class="pre">bool</span></code>, not <code class="docutils literal notranslate"><span class="pre">np.bool</span></code>), other information in the form of a dictionary <code class="docutils literal notranslate"><span class="pre">info</span></code> (which includes at least the key <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code>).</p>
<p>After getting <code class="docutils literal notranslate"><span class="pre">reward</span></code> , <code class="docutils literal notranslate"><span class="pre">obs</span></code> , <code class="docutils literal notranslate"><span class="pre">done</span></code> , <code class="docutils literal notranslate"><span class="pre">info</span></code> and other data, it needs to be processed and converted into <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format to conform to the DI-engine specification. <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> will accumulate the actual reward obtained at the current step at each time step, and return the accumulated value at the end of an episode ( <code class="docutils literal notranslate"><span class="pre">done</span> <span class="pre">==</span> <span class="pre">True</span></code>).</p>
<p>Finally, put the above four data into <code class="docutils literal notranslate"><span class="pre">BaseEnvTimestep</span></code> defined as <code class="docutils literal notranslate"><span class="pre">namedtuple</span></code> and return (defined as: <code class="docutils literal notranslate"><span class="pre">BaseEnvTimestep</span> <span class="pre">=</span> <span class="pre">namedtuple('BaseEnvTimestep',</span> <span class="pre">['obs',</span> <span class="pre">'reward',</span> <span class="pre">'done</span> <span class="pre">',</span> <span class="pre">'info'])</span></code> )</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ding.envs</span> <span class="kn">import</span> <span class="n">BaseEnvTimestep</span>

<span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseEnvTimestep</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
      <span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span> <span class="o">+=</span> <span class="n">rew</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
      <span class="n">rew</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">([</span><span class="n">rew</span><span class="p">])</span>  <span class="c1"># Transformed to an array with shape (1, )</span>
      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
         <span class="n">info</span><span class="p">[</span><span class="s1">&#39;final_eval_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_eval_reward</span>
      <span class="k">return</span> <span class="n">BaseEnvTimestep</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code></p>
<p>In the Atari environment, <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> refers to the cumulative sum of all rewards of an episode, and the data type of <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> must be a python native type, not <code class="docutils literal notranslate"><span class="pre">np.array</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p>In the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method, set the current <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code> to 0;</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">step</span></code> method, add the actual reward obtained at each time step to <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code>.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">step</span></code> method, if the current episode has ended ( <code class="docutils literal notranslate"><span class="pre">done</span> <span class="pre">==</span> <span class="pre">True</span></code> ), then add to the <code class="docutils literal notranslate"><span class="pre">info</span></code> dictionary and return: <code class="docutils literal notranslate"><span class="pre">info['final_eval_reward']</span> <span class="pre">=</span> <span class="pre">self._final_eval_reward</span></code></p></li>
</ul>
</div></blockquote>
<p>However, other environments may not require the sum of <code class="docutils literal notranslate"><span class="pre">self._final_eval_reward</span></code>. For example, in smac, the winning percentage of the current episode is required, so it is necessary to modify the simple accumulation in the second step <code class="docutils literal notranslate"><span class="pre">step</span></code> method. Instead, we should record the game situation and finally return the calculated winning percentage at the end of the episode.</p>
</li>
<li><p>Data Specifications</p>
<p>DI-engine requires that the input and output data of each method in the environment must be in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format, and the data dtype must be <code class="docutils literal notranslate"><span class="pre">np.int64</span></code> (integer), <code class="docutils literal notranslate"><span class="pre">np.float32</span></code> ( float) or <code class="docutils literal notranslate"><span class="pre">np.uint8</span></code> (image). include:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">obs</span></code> returned by the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">action</span></code> received by the <code class="docutils literal notranslate"><span class="pre">step</span></code> method</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obs</span></code> returned by the <code class="docutils literal notranslate"><span class="pre">step</span></code> method</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reward</span></code> returned by the <code class="docutils literal notranslate"><span class="pre">step</span></code> method, here also requires that <code class="docutils literal notranslate"><span class="pre">reward</span></code> must be <strong>one-dimensional</strong>, not zero-dimensional, for example, Atari will expand zero-dimensional to one-dimensional <code class="docutils literal notranslate"><span class="pre">rew</span> <span class="pre">=</span> <span class="pre">to_ndarray([rew])</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">done</span></code> returned by the <code class="docutils literal notranslate"><span class="pre">step</span></code> method must be of type <code class="docutils literal notranslate"><span class="pre">bool</span></code>, not <code class="docutils literal notranslate"><span class="pre">np.bool</span></code></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</section>
<section id="advanced">
<h2>Advanced<a class="headerlink" href="#advanced" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>Environment preprocessing wrapper</p>
<p>If many environments are to be used in reinforcement learning training, some preprocessing is required to achieve the purpose of increasing randomness, data normalization, and ease of training. These preprocessing are implemented in the form of wrappers (for the introduction of wrappers, please refer to <a class="reference external" href="./env_wrapper_zh.html">here</a> ）.</p>
<p>Each wrapper for environment preprocessing is a subclass of <code class="docutils literal notranslate"><span class="pre">gym.Wrapper</span></code>. For example, <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code> is to perform a random number of No-Operation actions at the beginning of the episode. It is a means of increasing randomness. It is used as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;Pong-v4&#39;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">NoopResetEnv</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
<p>Since the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method is implemented in <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code>, the corresponding logic in <code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code> will be executed when <code class="docutils literal notranslate"><span class="pre">env.reset()</span></code>.</p>
<p>The following env wrapper has been implemented in DI-engine:( in <code class="docutils literal notranslate"><span class="pre">ding/envs/env_wrappers/env_wrappers.py</span></code>)</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NoopResetEnv</span></code>: perform a random number of No-Operation actions at the beginning of the episode</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MaxAndSkipEnv</span></code>: Returns the maximum value in several frames, which can be considered as a kind of max pooling on time steps</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WarpFrame</span></code>: Convert the original image to the color code using <code class="docutils literal notranslate"><span class="pre">cvtColor</span></code> of the <code class="docutils literal notranslate"><span class="pre">cv2</span></code> library, and resize it into an image of a certain length and width (usually 84x84)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ScaledFloatFrame</span></code>: normalize the observation to the interval [0, 1] (keep the dtype as <code class="docutils literal notranslate"><span class="pre">np.float32</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ClipRewardEnv</span></code>: Pass the reward through a symbolic function to <code class="docutils literal notranslate"><span class="pre">{+1,</span> <span class="pre">0,</span> <span class="pre">-1}</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FrameStack</span></code>: stacks a certain number (usually 4) of frames together as a new observation, which can be used to deal with POMDP situations, for example, the speed direction of the movement cannot be known by a single frame of information</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ObsTransposeWrapper</span></code>: Transpose observation to put channel to first dim</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ObsNormEnv</span></code>: use <code class="docutils literal notranslate"><span class="pre">RunningMeanStd</span></code> to normalize the observation for sliding windows</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RewardNormEnv</span></code>: use <code class="docutils literal notranslate"><span class="pre">RunningMeanStd</span></code> to normalize the reward with sliding window</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RamWrapper</span></code>: Wrap ram env into image-like env</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EpisodicLifeEnv</span></code>: treat environments with multiple lives built in (eg Qbert), and treat each life as an episode</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FireResetEnv</span></code>: execute action 1 (fire) immediately after environment reset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GymHybridDictActionWrapper</span></code>: Transform Gym-Hybrid’s original <code class="docutils literal notranslate"><span class="pre">gym.spaces.Tuple</span></code> action space to <code class="docutils literal notranslate"><span class="pre">gym.spaces.Dict</span></code></p></li>
</ul>
</div></blockquote>
<p>If the above wrappers cannot meet your needs, you can also customize the wrappers yourself.</p>
<p>It is worth mentioning that each wrapper must not only complete the change of the corresponding observation/action/reward value, but also modify its space accordingly (if and only when shpae, dtype, etc. are modified), this method will be described in the next described in detail in the section.</p>
</li>
<li><p>Three space attributes <code class="docutils literal notranslate"><span class="pre">observation/action/reward</span> <span class="pre">space</span></code></p>
<p>If you want to automatically create a neural network based on the dimensions of the environment, or use the <code class="docutils literal notranslate"><span class="pre">shared_memory</span></code> technique in the <code class="docutils literal notranslate"><span class="pre">EnvManager</span></code> to speed up the transmission of large tensor data returned by the environment, you need to let the environment support provide the attribute  <code class="docutils literal notranslate"><span class="pre">observation_space</span></code> <code class="docutils literal notranslate"><span class="pre">action_space</span></code> <code class="docutils literal notranslate"><span class="pre">reward_space</span></code>  .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the sake of code extensibility, we <strong>strongly recommend implementing these three space attributes</strong>.</p>
</div>
<p>The spaces here are all instances of subclasses of <code class="docutils literal notranslate"><span class="pre">gym.spaces.Space</span></code>, the most commonly used <code class="docutils literal notranslate"><span class="pre">gym.spaces.Space</span></code> include <code class="docutils literal notranslate"><span class="pre">Discrete</span></code> <code class="docutils literal notranslate"><span class="pre">Box</span></code> <code class="docutils literal notranslate"><span class="pre">Tuple</span></code> <code class="docutils literal notranslate"><span class="pre">Dict</span></code>  etc. <strong>shape</strong> and <strong>dtype</strong> need to be given in space. In the original gym environment, most of them will support <code class="docutils literal notranslate"><span class="pre">observation_space</span></code>, <code class="docutils literal notranslate"><span class="pre">action_space</span></code> and <code class="docutils literal notranslate"><span class="pre">reward_range</span></code>. In DI-engine, <code class="docutils literal notranslate"><span class="pre">reward_range</span></code> is also expanded into <code class="docutils literal notranslate"><span class="pre">reward_space</span></code>, so that this All three remain the same.</p>
<p>For example, here are the three attributes of cartpole:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CartpoleEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{})</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span>
            <span class="n">low</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.8</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="o">-</span><span class="mf">0.42</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)]),</span>
            <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.8</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="mf">0.42</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)]),</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
      <span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_reward_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">observation_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_space</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">action_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action_space</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">reward_space</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_space</span>
</pre></div>
</div>
<p>Since the cartpole does not use any wrapper, its three spaces are fixed. However, if an environment like Atari has been decorated with multiple wrappers, it is necessary to modify the corresponding space after each wrapper wraps the original environment. For example, Atari will use <code class="docutils literal notranslate"><span class="pre">ScaledFloatFrameWrapper</span></code> to normalize the observation to the interval [0, 1], then it will modify its <code class="docutils literal notranslate"><span class="pre">observation_space</span></code> accordingly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ScaledFloatFrameWrapper</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">ObservationWrapper</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
      <span class="c1"># ...</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">enable_save_replay()</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">DI-engine</span></code> does not require the implementation of the <code class="docutils literal notranslate"><span class="pre">render</span></code> method. If you want to complete the visualization, we recommend implementing the <code class="docutils literal notranslate"><span class="pre">enable_save_replay</span></code> method to save the game video.</p>
<p>This method is called before the <code class="docutils literal notranslate"><span class="pre">reset</span></code> method and after the <code class="docutils literal notranslate"><span class="pre">seed</span></code> method, in which the path to the recording storage is specified. It should be noted that this method <strong>does not directly store the video</strong>, but only sets a flag for whether to save the video. The code and logic for actually storing the video needs to be implemented by yourself. (Because multiple environments may be opened, and each environment runs multiple episodes, it needs to be distinguished in the file name)</p>
<p>Here, an example in DI-engine is given. The <code class="docutils literal notranslate"><span class="pre">reset</span></code> method uses the decorator provided by <code class="docutils literal notranslate"><span class="pre">gym</span></code> to encapsulate the environment, giving it the function of storing game videos, as shown in the code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">enable_save_replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replay_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">replay_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
         <span class="n">replay_path</span> <span class="o">=</span> <span class="s1">&#39;./video&#39;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_replay_path</span> <span class="o">=</span> <span class="n">replay_path</span>

   <span class="k">def</span> <span class="nf">reset</span><span class="p">():</span>
      <span class="c1"># ...</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">wrappers</span><span class="o">.</span><span class="n">RecordVideo</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env</span><span class="p">,</span>
            <span class="n">video_folder</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_replay_path</span><span class="p">,</span>
            <span class="n">episode_trigger</span><span class="o">=</span><span class="k">lambda</span> <span class="n">episode_id</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">name_prefix</span><span class="o">=</span><span class="s1">&#39;rl-video-</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
         <span class="p">)</span>
      <span class="c1"># ...</span>
</pre></div>
</div>
<p>In actual use, the order of calling these methods should be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">atari_env</span> <span class="o">=</span> <span class="n">AtariEnv</span><span class="p">(</span><span class="n">easydict_cfg</span><span class="p">)</span>
<span class="n">atari_env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">413</span><span class="p">)</span>
<span class="n">atari_env</span><span class="o">.</span><span class="n">enable_save_replay</span><span class="p">(</span><span class="s1">&#39;./replay_video&#39;</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">atari_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># ...</span>
</pre></div>
</div>
</li>
<li><p>Use different config for training environment and test environment</p>
<p>The environment used for training (collector_env) and the environment used for testing (evaluator_env) may use different configuration items. You can implement a static method in the environment to implement custom configuration for different environment configuration items. Take Atari as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AtariEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="nd">@staticmethod</span>
   <span class="k">def</span> <span class="nf">create_collector_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
      <span class="n">collector_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collector_env_num&#39;</span><span class="p">)</span>
      <span class="n">cfg</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
      <span class="n">cfg</span><span class="o">.</span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">cfg</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">collector_env_num</span><span class="p">)]</span>

   <span class="nd">@staticmethod</span>
   <span class="k">def</span> <span class="nf">create_evaluator_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
      <span class="n">evaluator_env_num</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;evaluator_env_num&#39;</span><span class="p">)</span>
      <span class="n">cfg</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
      <span class="n">cfg</span><span class="o">.</span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">return</span> <span class="p">[</span><span class="n">cfg</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluator_env_num</span><span class="p">)]</span>
</pre></div>
</div>
<p>In actual use, the original configuration item <code class="docutils literal notranslate"><span class="pre">cfg</span></code> can be converted to obtain two versions of configuration items for training and testing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># env_fn is an env class</span>
<span class="n">collector_env_cfg</span> <span class="o">=</span> <span class="n">env_fn</span><span class="o">.</span><span class="n">create_collector_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">evaluator_env_cfg</span> <span class="o">=</span> <span class="n">env_fn</span><span class="o">.</span><span class="n">create_evaluator_env_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
<p>Setting the <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span></code> item will use different decorations in the wrapper accordingly. For example, if <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span> <span class="pre">==</span> <span class="pre">True</span></code>, a symbolic function of reward will be used to map to <code class="docutils literal notranslate"><span class="pre">{+1,</span> <span class="pre">0,</span> <span class="pre">-1}</span></code> to facilitate training, if <code class="docutils literal notranslate"><span class="pre">cfg.is_train</span> <span class="pre">==</span> <span class="pre">False</span></code> Then the original reward value will remain unchanged, which is convenient for evaluating the performance of the agent during testing.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_action()</span></code></p>
<p>Some off-policy algorithms hope to use a random strategy to collect some data to fill the buffer before training starts, and complete the initialization of the buffer. For such a need, DI-engine encourages the implementation of the <code class="docutils literal notranslate"><span class="pre">random_action</span></code> method.</p>
<p>Since the environment already implements <code class="docutils literal notranslate"><span class="pre">action_space</span></code>, you can directly call the <code class="docutils literal notranslate"><span class="pre">Space.sample()</span></code> method provided in the gym to randomly select actions. But it should be noted that since DI-engine requires all returned actions to be in <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> format, some necessary dtype conversions may be required. The <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">dict</span></code> types are converted to the <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> type using the <code class="docutils literal notranslate"><span class="pre">to_ndarray</span></code> function, as shown in the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_action</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
   <span class="n">random_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
   <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">random_action</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
         <span class="k">pass</span>
   <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">random_action</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
         <span class="n">random_action</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">([</span><span class="n">random_action</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
   <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">random_action</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
         <span class="n">random_action</span> <span class="o">=</span> <span class="n">to_ndarray</span><span class="p">(</span><span class="n">random_action</span><span class="p">)</span>
   <span class="k">else</span><span class="p">:</span>
         <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s1">&#39;`random_action` should be either int/np.ndarray or dict of int/np.ndarray, but get </span><span class="si">{}</span><span class="s1">: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
               <span class="nb">type</span><span class="p">(</span><span class="n">random_action</span><span class="p">),</span> <span class="n">random_action</span>
            <span class="p">)</span>
         <span class="p">)</span>
   <span class="k">return</span> <span class="n">random_action</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">default_config()</span></code></p>
<p>If an environment has some default or commonly used configuration items, you can consider setting the class variable <code class="docutils literal notranslate"><span class="pre">config</span></code> as <strong>default config</strong> (for the convenience of external access, you can also implement the class method <code class="docutils literal notranslate"><span class="pre">default_config</span></code>, which returns config). As shown in the following code:</p>
<p>When running an experiment, a <strong>user config</strong> file for this experiment is configured, such as <code class="docutils literal notranslate"><span class="pre">dizoo/mujoco/config/ant_ddpg_config.py</span></code>. In the user config file, you can omit this part of the key-value pair, and merge <strong>default config</strong> with <strong>user config</strong> through <code class="docutils literal notranslate"><span class="pre">deep_merge_dicts</span></code> (remember to use the default config as the first parameter here, the user config is used as the second parameter to ensure that the user config has a higher priority). As shown in the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MujocoEnv</span><span class="p">(</span><span class="n">BaseEnv</span><span class="p">):</span>

   <span class="nd">@classmethod</span>
   <span class="k">def</span> <span class="nf">default_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="nb">type</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EasyDict</span><span class="p">:</span>
      <span class="n">cfg</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">config</span><span class="p">))</span>
      <span class="n">cfg</span><span class="o">.</span><span class="n">cfg_type</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;Dict&#39;</span>
      <span class="k">return</span> <span class="n">cfg</span>

   <span class="n">config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
      <span class="n">use_act_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">delay_reward_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
   <span class="p">)</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">deep_merge_dicts</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Environment implementation correctness check</p>
<p>We provide a set of inspection tools for user-implemented environments to check:</p>
<ul class="simple">
<li><p>data type of observation/action/reward</p></li>
<li><p>reset/step method</p></li>
<li><p>Whether there are unreasonable identical references in the observation of two adjacent time steps (that is, deepcopy should be used to avoid identical references)</p></li>
</ul>
<p>The implementation of the check tool is in <code class="docutils literal notranslate"><span class="pre">ding/envs/env/env_implementation_check.py</span></code> .
For the usage of the check tool, please refer to <code class="docutils literal notranslate"><span class="pre">ding/envs/env/tests/test_env_implementation_check.py</span></code> ‘s <code class="docutils literal notranslate"><span class="pre">test_an_implemented_env</span></code>。</p>
</li>
</ol>
</section>
<section id="dingenvwrapper">
<h2>DingEnvWrapper<a class="headerlink" href="#dingenvwrapper" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">DingEnvWrapper</span></code> can quickly convert simple environments such as ClassicControl, Box2d, Atari, Mujoco, GymHybrid, etc., to <code class="docutils literal notranslate"><span class="pre">BaseEnv</span></code> compliant environments.</p>
<p>Note: The specific implementation of <code class="docutils literal notranslate"><span class="pre">DingEnvWrapper</span></code> can be found in <code class="docutils literal notranslate"><span class="pre">ding/envs/env/ding_env_wrapper.py</span></code>, in addition, you can see <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/envs/env/tests/test_ding_env_wrapper.py">Example</a> for more info.</p>
</section>
<section id="q-a">
<h2>Q &amp; A<a class="headerlink" href="#q-a" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>How should the MARL environment be migrated?</p>
<p>You can refer to <a class="reference external" href="../env_tutorial/competitive_rl_zh.html">Competitive RL</a></p>
<ul class="simple">
<li><p>If the environment supports both single-agent, double-agent or even multi-agent, consider different mode classifications</p></li>
<li><p>In a multi-agent environment, the number of action and observation matches the number of agents, but the reward and done are not necessarily the same. It is necessary to clarify the definition of reward</p></li>
<li><p>Note how the original environment requires actions and observations to be combined (tuples, lists, dictionaries, stacked arrays and so on)</p></li>
</ul>
</li>
<li><p>How should the environment of the hybrid action space be migrated?</p>
<p>You can refer to  <a class="reference external" href="../env_tutorial/gym_hybrid_zh.html">Gym-Hybrid</a></p>
<ul class="simple">
<li><p>Some discrete actions (Accelerate, Turn) in Gym-Hybrid need to give corresponding 1-dimensional continuous parameters to represent acceleration and rotation angle, so similar environments need to focus on the definition of their action space</p></li>
</ul>
</li>
</ol>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="custom_model.html" class="btn btn-neutral float-right" title="How to customize the neural network model" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="index.html" class="btn btn-neutral" title="Best Practice" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">How to migrate your own environment to DI-engine</a><ul>
<li><a class="reference internal" href="#basic">Basic</a></li>
<li><a class="reference internal" href="#advanced">Advanced</a></li>
<li><a class="reference internal" href="#dingenvwrapper">DingEnvWrapper</a></li>
<li><a class="reference internal" href="#q-a">Q &amp; A</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>SMAC &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="D4RL (Mujoco)" href="d4rl_zh.html" />
    <link rel="prev" title="Procgen" href="procgen_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index_zh.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">使用者指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index_zh.html">安装说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index_zh.html">快速上手</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro_rl/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index_zh.html">强化学习算法攻略合集</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习环境示例手册</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="atari_zh.html">Atari</a></li>
<li class="toctree-l2"><a class="reference internal" href="mujoco_zh.html">Mujoco</a></li>
<li class="toctree-l2"><a class="reference internal" href="cartpole_zh.html">Cartpole</a></li>
<li class="toctree-l2"><a class="reference internal" href="pendulum_zh.html">Pendulum</a></li>
<li class="toctree-l2"><a class="reference internal" href="lunarlander_zh.html">LunarLander</a></li>
<li class="toctree-l2"><a class="reference internal" href="bipedalwalker_zh.html">BipedalWalker</a></li>
<li class="toctree-l2"><a class="reference internal" href="bitflip_zh.html">Bit-flip</a></li>
<li class="toctree-l2"><a class="reference internal" href="minigrid_zh.html">MiniGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="slime_volleyball_zh.html">Slime Volleyball</a></li>
<li class="toctree-l2"><a class="reference internal" href="competitive_rl_zh.html">Competitive RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="procgen_zh.html">Procgen</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">SMAC</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">安装</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">安装方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">验证安装</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">镜像</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id6">变换前的空间（原始环境）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">观察空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">动作空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">奖励空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id16">关键事实</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rl">变换后的空间（RL环境）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id18">观察空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">动作空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id22">奖励空间</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id24">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id26">其他</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id28">惰性初始化</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id29">随机种子</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id30">训练和测试环境的区别</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id31">存储录像</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#di-zoo">DI-zoo可运行代码示例</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id32">基准算法性能</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="d4rl_zh.html">D4RL (Mujoco)</a></li>
<li class="toctree-l2"><a class="reference internal" href="gfootball_zh.html">Google Research Football (Gfootball)</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiagent_particle_zh.html">Multi-Agent Particle</a></li>
<li class="toctree-l2"><a class="reference internal" href="pybullet_zh.html">PyBullet</a></li>
<li class="toctree-l2"><a class="reference internal" href="pettingzoo_zh.html">PettingZoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gym_hybrid_zh.html">Gym-Hybrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="gym_soccer_zh.html">Gym-Soccer (HFO)</a></li>
<li class="toctree-l2"><a class="reference internal" href="pybullet_zh.html">PyBullet</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_cls_zh.html">Image Classification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index_zh.html">分布式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_zh.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index_zh.html">特性介绍</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index_zh.html">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../specification/index_zh.html">中间件（middleware）编写规范</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index_zh.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index_zh.html">Docs</a> &raquo;</li>
        
          <li><a href="index_zh.html">强化学习环境示例手册</a> &raquo;</li>
        
      <li>SMAC</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/env_tutorial/smac_zh.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="smac">
<h1>SMAC<a class="headerlink" href="#smac" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>SMAC是一个用于在暴雪星际争霸2上进行多智能体协同强化学习（MARL）的环境。SMAC用了暴雪星际争霸2的机器学习API和DeepMind的PySC2为智能体与星际争霸2的交互提供了友好的接口，方便开发者观察和执行行动。
与PySC2相比，SMAC专注于分散的微观操作方案，其中游戏的每个智能体均由单独的RL agent控制。</p>
<img alt="../_images/smac.gif" class="align-center" src="../_images/smac.gif" />
</div>
<div class="section" id="id2">
<h2>安装<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>安装方法<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>需要安装星际争霸2游戏和PySC2库，安装方法可以参考<a class="reference external" href="https://github.com/opendilab/DI-star">DI-star安装</a></p>
<p>安装主要包括两部分：</p>
<p>1.下载星际争霸2游戏
对于Linux系统使用者，安装路径为<a class="reference external" href="https://github.com/Blizzard/s2client-proto#downloads">https://github.com/Blizzard/s2client-proto#downloads</a>，之后使用export SC2PATH=&lt;sc2/installation/path&gt;命令将安装路径添加到环境变量中
对于Windows系统使用者，安装请参考<a class="reference external" href="https://starcraft2.com">https://starcraft2.com</a></p>
<p>2.安装与DI-engine适配的PySC2</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/opendilab/DI-star.git
<span class="nb">cd</span> DI-star
pip install -e .
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h3>验证安装<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>安装完成后，可以通过安装成功后 <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$SC2PATH</span></code> 确认环境变量设置成功</p>
</div>
<div class="section" id="id5">
<h3>镜像<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>DI-engine的镜像配备有框架本身和Smac环境，可通过<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">pull</span> <span class="pre">opendilab/ding:nightly-smac</span></code>获取，或访问<a class="reference external" href="https://hub.docker.com/repository/docker/opendilab/ding">docker
hub</a>获取更多镜像</p>
</div>
</div>
<div class="section" id="id6">
<span id="id7"></span><h2>变换前的空间（原始环境）<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id8">
<span id="id9"></span><h3>观察空间<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>可以获取各个智能体是否存活，各个智能体剩余血量，各个智能体视野范围内的盟友或敌人等零碎的信息。</p></li>
</ul>
</div>
<div class="section" id="id10">
<span id="id11"></span><h3>动作空间<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>游戏操作按键空间，一般是大小为N的离散动作空间（N随具体子环境变化），数据类型为<code class="docutils literal notranslate"><span class="pre">int</span></code>，需要传入python数值（或是0维np数组，例如动作3为<code class="docutils literal notranslate"><span class="pre">np.array(3)</span></code>）</p></li>
<li><p>对于各个地图，动作空间N一般等于6+敌人数，如3s5z地图中为14，2c_vs_64zg地图中为70。具体的含义是：</p>
<ul>
<li><p>0：NOOP</p></li>
<li><p>1：STOP</p></li>
<li><p>2：MOVE_NORTH</p></li>
<li><p>3：MOVE_SOUTH</p></li>
<li><p>4：MOVE_EAST</p></li>
<li><p>5：MOVE_WEST</p></li>
<li><p>6 - N: ATTACK ENEMY，所攻击的敌人的ID为N-6</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id12">
<span id="id13"></span><h3>奖励空间<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>游戏胜负，胜利为1，失败为0，一般是一个<code class="docutils literal notranslate"><span class="pre">int</span></code>数值。</p></li>
</ul>
</div>
<div class="section" id="id14">
<span id="id15"></span><h3>其他<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>游戏结束即为当前环境episode结束</p></li>
</ul>
</div>
</div>
<div class="section" id="id16">
<h2>关键事实<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>输入为将离散信息综合后的信息</p></li>
<li><p>离散动作空间</p></li>
<li><p>奖励为稀疏奖励，我们设置fake_reward，使得训练时所用的奖励为稠密奖励。</p></li>
</ol>
</div>
<div class="section" id="rl">
<span id="id17"></span><h2>变换后的空间（RL环境）<a class="headerlink" href="#rl" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id18">
<span id="id19"></span><h3>观察空间<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>变换内容：拼接各个agent看到的各类离散信息，将拼接后的信息作为各个agent看到的agent_state和全局的global_state</p></li>
<li><p>变换结果：一个dict型数据，其中包含agent_state，global_state和action_mask，均为一个一维Tensor型数组</p></li>
</ul>
</div>
<div class="section" id="id20">
<span id="id21"></span><h3>动作空间<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>基本无变换，依然是大小为N的离散动作空间</p></li>
</ul>
</div>
<div class="section" id="id22">
<span id="id23"></span><h3>奖励空间<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>变换内容：设置fake_reward，使得智能体在作出一些动作后就可以获得奖励，我们设置每一步的fake_reward为’打掉的敌人血量-损失的己方血量‘，且消灭一个敌人奖励20分，获取全局的胜利获得200分</p></li>
<li><p>变换结果：一个一维且只包含一个float32类型数据的Tensor</p></li>
</ul>
</div>
<div class="section" id="id24">
<span id="id25"></span><h3>其他<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>开启<code class="docutils literal notranslate"><span class="pre">special_global_state</span></code>返回的global_state则为各个全局信息+各个agent特殊信息拼接成的信息，若不开启，则仅返回全局信息</p></li>
<li><p>开启<code class="docutils literal notranslate"><span class="pre">special_global_state</span></code>且开启<code class="docutils literal notranslate"><span class="pre">death_mask</span></code>，则若一个agent阵亡，则其返回的global_state仅包含其自身的ID信息，其余信息全部被屏蔽</p></li>
<li><p>环境<code class="docutils literal notranslate"><span class="pre">step</span></code>方法返回的<code class="docutils literal notranslate"><span class="pre">info</span></code>必须包含<code class="docutils literal notranslate"><span class="pre">final_eval_reward</span></code>键值对，表示整个episode的评测指标，在SMAC中为整个episode的fake_reward累加和</p></li>
<li><p>环境<code class="docutils literal notranslate"><span class="pre">step</span></code>方法最终返回的<code class="docutils literal notranslate"><span class="pre">reward</span></code>为胜利与否</p></li>
</ul>
</div>
</div>
<div class="section" id="id26">
<span id="id27"></span><h2>其他<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id28">
<h3>惰性初始化<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<p>为了便于支持环境向量化等并行操作，环境实例一般实现惰性初始化，即<code class="docutils literal notranslate"><span class="pre">__init__</span></code>方法不初始化真正的原始环境实例，只是设置相关参数和配置值，在第一次调用<code class="docutils literal notranslate"><span class="pre">reset</span></code>方法时初始化具体的原始环境实例。</p>
</div>
<div class="section" id="id29">
<h3>随机种子<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>环境中有两部分随机种子需要设置，一是原始环境的随机种子，二是各种环境变换使用到的随机库的随机种子（例如<code class="docutils literal notranslate"><span class="pre">random</span></code>，<code class="docutils literal notranslate"><span class="pre">np.random</span></code>）</p></li>
<li><p>对于环境调用者，只需通过环境的<code class="docutils literal notranslate"><span class="pre">seed</span></code>方法进行设置这两个种子，无需关心具体实现细节</p></li>
<li><p>环境内部的具体实现：对于原始环境的种子，在调用环境的<code class="docutils literal notranslate"><span class="pre">reset</span></code>方法内部，具体的原始环境<code class="docutils literal notranslate"><span class="pre">reset</span></code>之前设置</p></li>
<li><p>环境内部的具体实现：对于随机库种子，则在环境的<code class="docutils literal notranslate"><span class="pre">seed</span></code>方法中直接设置该值</p></li>
</ul>
</div>
<div class="section" id="id30">
<h3>训练和测试环境的区别<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>训练环境使用动态随机种子，即每个episode的随机种子都不同，都是由一个随机数发生器产生，但这个随机数发生器的种子是通过环境的<code class="docutils literal notranslate"><span class="pre">seed</span></code>方法固定的；测试环境使用静态随机种子，即每个episode的随机种子相同，通过<code class="docutils literal notranslate"><span class="pre">seed</span></code>方法指定。</p></li>
</ul>
</div>
<div class="section" id="id31">
<h3>存储录像<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
<p>调用<a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/smac/utils/eval.py">https://github.com/opendilab/DI-engine/blob/main/dizoo/smac/utils/eval.py</a> 所提供的方法存储视频，并在星际争霸2游戏中播放存储的视频。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">from</span> <span class="nn">ding.config</span> <span class="kn">import</span> <span class="n">compile_config</span><span class="p">,</span> <span class="n">read_config</span>
<span class="kn">from</span> <span class="nn">ding.envs</span> <span class="kn">import</span> <span class="n">get_vec_env_setting</span>
<span class="kn">from</span> <span class="nn">ding.policy</span> <span class="kn">import</span> <span class="n">create_policy</span>
<span class="kn">from</span> <span class="nn">ding.utils</span> <span class="kn">import</span> <span class="n">set_pkg_seed</span>


<span class="k">def</span> <span class="nf">eval</span><span class="p">(</span>
        <span class="n">input_cfg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]],</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">env_setting</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_cfg</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">cfg</span><span class="p">,</span> <span class="n">create_cfg</span> <span class="o">=</span> <span class="n">read_config</span><span class="p">(</span><span class="n">input_cfg</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cfg</span><span class="p">,</span> <span class="n">create_cfg</span> <span class="o">=</span> <span class="n">input_cfg</span>
    <span class="n">create_cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">type</span> <span class="o">+=</span> <span class="s1">&#39;_command&#39;</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">compile_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">create_cfg</span><span class="o">=</span><span class="n">create_cfg</span><span class="p">)</span>

    <span class="n">env_fn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">evaluator_env_cfg</span> <span class="o">=</span> <span class="n">get_vec_env_setting</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">env</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">env_fn</span><span class="p">(</span><span class="n">evaluator_env_cfg</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">dynamic_seed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">set_pkg_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">cuda</span><span class="p">)</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">create_policy</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">enable_field</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;eval&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">eval_mode</span>
    <span class="k">if</span> <span class="n">state_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">learner</span><span class="o">.</span><span class="n">load_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">eval_reward</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">policy_output</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">forward</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">obs</span><span class="p">})</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy_output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;action&#39;</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">timestep</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">eval_reward</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">obs</span>
        <span class="k">if</span> <span class="n">timestep</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>
            <span class="k">break</span>

    <span class="n">env</span><span class="o">.</span><span class="n">save_replay</span><span class="p">(</span><span class="n">replay_dir</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">_map_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Eval is over! The performance of your RL policy is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eval_reward</span><span class="p">))</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="c1">#model path</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="n">config</span> <span class="n">path</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
    <span class="nb">eval</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">state_dict</span><span class="o">=</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="di-zoo">
<h2>DI-zoo可运行代码示例<a class="headerlink" href="#di-zoo" title="Permalink to this headline">¶</a></h2>
<p>完整的训练配置文件在 <a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/smac/config">github
link</a>
内，对于具体的配置文件，例如<code class="docutils literal notranslate"><span class="pre">smac_3s5z_mappo_config.py</span></code>，使用如下的demo即可运行：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">ding.entry</span> <span class="kn">import</span> <span class="n">serial_pipeline_onpolicy</span>
<span class="kn">from</span> <span class="nn">easydict</span> <span class="kn">import</span> <span class="n">EasyDict</span>

<span class="n">agent_num</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">collector_env_num</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">evaluator_env_num</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">special_global_state</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">main_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">exp_name</span><span class="o">=</span><span class="s1">&#39;smac_3s5z_mappo&#39;</span><span class="p">,</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">map_name</span><span class="o">=</span><span class="s1">&#39;3s5z&#39;</span><span class="p">,</span>
        <span class="n">difficulty</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">reward_only_positive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">mirror_opponent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">agent_num</span><span class="o">=</span><span class="n">agent_num</span><span class="p">,</span>
        <span class="n">collector_env_num</span><span class="o">=</span><span class="n">collector_env_num</span><span class="p">,</span>
        <span class="n">evaluator_env_num</span><span class="o">=</span><span class="n">evaluator_env_num</span><span class="p">,</span>
        <span class="n">n_evaluator_episode</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">stop_value</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
        <span class="n">death_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">special_global_state</span><span class="o">=</span><span class="n">special_global_state</span><span class="p">,</span>
        <span class="c1"># save_replay_episodes = 1,</span>
        <span class="n">manager</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="n">shared_memory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">reset_timeout</span><span class="o">=</span><span class="mi">6000</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">cuda</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">multi_agent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">continuous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="c1"># (int) agent_num: The number of the agent.</span>
            <span class="c1"># For SMAC 3s5z, agent_num=8; for 2c_vs_64zg, agent_num=2.</span>
            <span class="n">agent_num</span><span class="o">=</span><span class="n">agent_num</span><span class="p">,</span>
            <span class="c1"># (int) obs_shape: The shapeension of observation of each agent.</span>
            <span class="c1"># For 3s5z, obs_shape=150; for 2c_vs_64zg, agent_num=404.</span>
            <span class="c1"># (int) global_obs_shape: The shapeension of global observation.</span>
            <span class="c1"># For 3s5z, obs_shape=216; for 2c_vs_64zg, agent_num=342.</span>
            <span class="n">agent_obs_shape</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
            <span class="c1">#global_obs_shape=216,</span>
            <span class="n">global_obs_shape</span><span class="o">=</span><span class="mi">295</span><span class="p">,</span>
            <span class="c1"># (int) action_shape: The number of action which each agent can take.</span>
            <span class="c1"># action_shape= the number of common action (6) + the number of enemies.</span>
            <span class="c1"># For 3s5z, obs_shape=14 (6+8); for 2c_vs_64zg, agent_num=70 (6+64).</span>
            <span class="n">action_shape</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
            <span class="c1"># (List[int]) The size of hidden layer</span>
            <span class="c1"># hidden_size_list=[64],</span>
        <span class="p">),</span>
        <span class="c1"># used in state_num of hidden_state</span>
        <span class="n">learn</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
            <span class="c1"># (bool) Whether to use multi gpu</span>
            <span class="n">multi_gpu</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">epoch_per_collect</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">3200</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
            <span class="c1"># ==============================================================</span>
            <span class="c1"># The following configs is algorithm-specific</span>
            <span class="c1"># ==============================================================</span>
            <span class="c1"># (float) The loss weight of value network, policy network weight is set to 1</span>
            <span class="n">value_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="c1"># (float) The loss weight of entropy regularization, policy network weight is set to 1</span>
            <span class="n">entropy_weight</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
            <span class="c1"># (float) PPO clip ratio, defaults to 0.2</span>
            <span class="n">clip_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
            <span class="c1"># (bool) Whether to use advantage norm in a whole training batch</span>
            <span class="n">adv_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">value_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">ppo_param_init</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">grad_clip_type</span><span class="o">=</span><span class="s1">&#39;clip_norm&#39;</span><span class="p">,</span>
            <span class="n">grad_clip_value</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">ignore_done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">on_policy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">collect</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">env_num</span><span class="o">=</span><span class="n">collector_env_num</span><span class="p">,</span> <span class="n">n_sample</span><span class="o">=</span><span class="mi">3200</span><span class="p">),</span>
        <span class="nb">eval</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">env_num</span><span class="o">=</span><span class="n">evaluator_env_num</span><span class="p">,</span> <span class="n">evaluator</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">eval_freq</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="p">)),</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">main_config</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">main_config</span><span class="p">)</span>
<span class="n">create_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;smac&#39;</span><span class="p">,</span>
        <span class="n">import_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;dizoo.smac.envs.smac_env&#39;</span><span class="p">],</span>
    <span class="p">),</span>
    <span class="n">env_manager</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;base&#39;</span><span class="p">),</span>
    <span class="n">policy</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;ppo&#39;</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">create_config</span> <span class="o">=</span> <span class="n">EasyDict</span><span class="p">(</span><span class="n">create_config</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">serial_pipeline_onpolicy</span><span class="p">([</span><span class="n">main_config</span><span class="p">,</span> <span class="n">create_config</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>注：对于On policy算法，使用serial_pipeline_onpolicy进入，对于Off policy算法，使用serial_pipeline进入</p>
</div>
<div class="section" id="id32">
<h2>基准算法性能<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>MMM（2M env step下胜率为1视为较好性能）</p>
<ul class="simple">
<li><p>MMM + MAPPO</p></li>
</ul>
<img alt="../_images/MMM_mappo.png" class="align-center" src="../_images/MMM_mappo.png" />
</li>
<li><p>3s5z（3M env step下胜率为1视为较好性能）</p>
<ul class="simple">
<li><p>3s5z + MAPPO</p></li>
</ul>
<img alt="../_images/3s5z_mappo1.png" class="align-center" src="../_images/3s5z_mappo1.png" />
</li>
<li><p>5m_vs_6m（5M env step下胜率为0.75视为较好性能）</p>
<ul class="simple">
<li><p>5m_vs_6m + MAPPO</p></li>
</ul>
<img alt="../_images/5m6m_mappo.png" class="align-center" src="../_images/5m6m_mappo.png" />
</li>
<li><p>MMM2（5M env step下胜率为1视为较好性能）</p>
<ul class="simple">
<li><p>MMM2 + MAPPO</p></li>
</ul>
<img alt="../_images/MMM2_mappo.png" class="align-center" src="../_images/MMM2_mappo.png" />
</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="d4rl_zh.html" class="btn btn-neutral float-right" title="D4RL (Mujoco)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="procgen_zh.html" class="btn btn-neutral float-left" title="Procgen" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
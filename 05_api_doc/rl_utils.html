


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ding.rl_utils &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Policy" href="policy/index.html" />
  <link rel="prev" title="ding.model" href="model.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">API Doc</a> &gt;</li>
        
      <li>ding.rl_utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/05_api_doc/rl_utils.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="ding-rl-utils">
<h1>ding.rl_utils<a class="headerlink" href="#ding-rl-utils" title="Permalink to this headline">¶</a></h1>
<section id="a2c">
<h2>a2c<a class="headerlink" href="#a2c" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/a2c</span></code> for more details.</p>
<section id="a2c-error">
<h3>a2c_error<a class="headerlink" href="#a2c-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.a2c_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">a2c_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">collections.namedtuple</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/a2c.html#a2c_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.a2c_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of A2C(Advantage Actor-Critic) (arXiv:1602.01783) for discrete action space</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): a2c input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">a2c_data</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>a2c_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the a2c loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">a2c_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">a2c_error</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="a2c-error-continuous">
<h3>a2c_error_continuous<a class="headerlink" href="#a2c-error-continuous" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.a2c_error_continuous">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">a2c_error_continuous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">collections.namedtuple</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/a2c.html#a2c_error_continuous"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.a2c_error_continuous" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of A2C(Advantage Actor-Critic) (arXiv:1602.01783) for continuous action space</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): a2c input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">a2c_data</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>a2c_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the a2c loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">a2c_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)},</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">a2c_error_continuous</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="acer">
<h2>acer<a class="headerlink" href="#acer" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/acer</span></code> for more details.</p>
<section id="acer-policy-error">
<h3>acer_policy_error<a class="headerlink" href="#acer-policy-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.acer_policy_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">acer_policy_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_retraces</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_logit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">10.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/acer.html#acer_policy_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.acer_policy_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get ACER policy loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>q_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Q values</p></li>
<li><p>q_retraces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Q values (be calculated by retrace method)</p></li>
<li><p>v_pred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): V values</p></li>
<li><p>target_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The new policy’s probability</p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The actions in replay buffer</p></li>
<li><p>ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): ratio of new polcy with behavior policy</p></li>
<li><p>c_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): clip value for ratio</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>actor_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): policy loss from q_retrace</p></li>
<li><p>bc_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): correct policy loss</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>q_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>q_retraces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
<li><p>v_pred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
<li><p>target_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>actor_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
<li><p>bc_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">q_values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q_retraces</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_pred</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_pi</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ratio</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">acer_policy_error</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">q_retraces</span><span class="p">,</span> <span class="n">v_pred</span><span class="p">,</span> <span class="n">target_pi</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="acer-value-error">
<h3>acer_value_error<a class="headerlink" href="#acer-value-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.acer_value_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">acer_value_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_retraces</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/acer.html#acer_value_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.acer_value_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get ACER critic loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>q_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Q values</p></li>
<li><p>q_retraces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Q values (be calculated by retrace method)</p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The actions in replay buffer</p></li>
<li><p>ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): ratio of new polcy with behavior policy</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>critic_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): critic loss</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>q_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>q_retraces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>critic_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, 1)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">q_values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q_retraces</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">acer_value_error</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">q_retraces</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="acer-trust-region-update">
<h3>acer_trust_region_update<a class="headerlink" href="#acer-trust-region-update" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.acer_trust_region_update">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">acer_trust_region_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor_gradients</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_logit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_logit</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trust_region_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/acer.html#acer_trust_region_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.acer_trust_region_update" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>calcuate gradient with trust region constrain</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>actor_gradients (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list(torch.Tensor)</span></code>): gradients value’s for different part</p></li>
<li><p>target_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The new policy’s probability</p></li>
<li><p>avg_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The average policy’s probability</p></li>
<li><p>trust_region_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the range of trust region</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>update_gradients (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list(torch.Tensor)</span></code>): gradients with trust region constraint</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>target_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>avg_pi (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>update_gradients (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list(torch.FloatTensor)</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">actor_gradients</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_pi</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">avg_pi</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">acer_trust_region_update</span><span class="p">(</span><span class="n">actor_gradients</span><span class="p">,</span> <span class="n">target_pi</span><span class="p">,</span> <span class="n">avg_pi</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="adder">
<h2>adder<a class="headerlink" href="#adder" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/adder</span></code> for more details.</p>
<section id="id1">
<h3>Adder<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.adder.</span></span><span class="sig-name descname"><span class="pre">Adder</span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Adder is a component that handles different transformations and calculations for transitions
in Collector Module(data generation and processing), such as GAE, n-step return, transition sampling etc.</p>
</dd>
<dt>Interface:</dt><dd><p>__init__, get_gae, get_gae_with_default_last_value, get_nstep_return_data, get_train_sample</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder._get_null_transition">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">_get_null_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">template</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">null_transition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder._get_null_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder._get_null_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get null transition for padding. If <code class="docutils literal notranslate"><span class="pre">cls._null_transition</span></code> is None, return input <code class="docutils literal notranslate"><span class="pre">template</span></code> instead.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>template (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): The template for null transition.</p></li>
<li><p>null_transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[dict]</span></code>): Dict type null transition, used in <code class="docutils literal notranslate"><span class="pre">null_padding</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>null_transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): The deepcopied null transition.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder.get_gae">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">get_gae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cuda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder.get_gae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder.get_gae" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get GAE advantage for stacked transitions(T timestep, 1 batch). Call <code class="docutils literal notranslate"><span class="pre">gae</span></code> for calculation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): Transitions list, each element is a transition dict with at least [‘value’, ‘reward’]</p></li>
<li><p>last_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The last value(i.e.: the T+1 timestep)</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The future discount factor, should be in [0, 1], defaults to 0.99.</p></li>
<li><p>gae_lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): GAE lambda parameter, should be in [0, 1], defaults to 0.97,             when lambda -&gt; 0, it induces bias, but when lambda -&gt; 1, it has high variance due to the sum of terms.</p></li>
<li><p>cuda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether use cuda in GAE computation</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): transitions list like input one, but each element owns extra advantage key ‘adv’</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># batch_size, timestep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">last_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gae_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_gae</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">last_value</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder.get_gae_with_default_last_value">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">get_gae_with_default_last_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.deque</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cuda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder.get_gae_with_default_last_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder.get_gae_with_default_last_value" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Like <code class="docutils literal notranslate"><span class="pre">get_gae</span></code> above to get GAE advantage for stacked transitions. However, this function is designed in
case <code class="docutils literal notranslate"><span class="pre">last_value</span></code> is not passed. If transition is not done yet, it wouold assign last value in <code class="docutils literal notranslate"><span class="pre">data</span></code>
as <code class="docutils literal notranslate"><span class="pre">last_value</span></code>, discard the last element in <code class="docutils literal notranslate"><span class="pre">data``(i.e.</span> <span class="pre">len(data)</span> <span class="pre">would</span> <span class="pre">decrease</span> <span class="pre">by</span> <span class="pre">1),</span> <span class="pre">and</span> <span class="pre">then</span> <span class="pre">call</span>
<span class="pre">``get_gae</span></code>. Otherwise it would make <code class="docutils literal notranslate"><span class="pre">last_value</span></code> equal to 0.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list, each element is a transition dict with                 at least[‘value’, ‘reward’]</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether the transition reaches the end of an episode(i.e. whether the env is done)</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The future discount factor, should be in [0, 1], defaults to 0.99.</p></li>
<li><p>gae_lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): GAE lambda parameter, should be in [0, 1], defaults to 0.97,             when lambda -&gt; 0, it induces bias, but when lambda -&gt; 1, it has high variance due to the sum of terms.</p></li>
<li><p>cuda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether use cuda in GAE computation</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): transitions list like input one, but each element owns                 extra advantage key ‘adv’</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># batch_size, timestep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gae_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_gae_with_default_last_value</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder.get_nstep_return_data">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">get_nstep_return_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.deque</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cum_reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_terminate_gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">collections.deque</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder.get_nstep_return_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder.get_nstep_return_data" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Process raw traj data by updating keys [‘next_obs’, ‘reward’, ‘done’] in data’s dict element.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list, each element is a transition dict</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of steps. If equals to 1, return <code class="docutils literal notranslate"><span class="pre">data</span></code> directly;                 Otherwise update with nstep value.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list like input one, but each element updated with nstep value.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_nstep_return_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.adder.Adder.get_train_sample">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unroll_len</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_fn_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'last'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">null_transition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/adder.html#Adder.get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.adder.Adder.get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process raw traj data by updating keys [‘next_obs’, ‘reward’, ‘done’] in data’s dict element.
If <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code> equals to 1, which means no process is needed, can directly return <code class="docutils literal notranslate"><span class="pre">data</span></code>.
Otherwise, <code class="docutils literal notranslate"><span class="pre">data</span></code> will be splitted according to <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code>, process residual part according to
<code class="docutils literal notranslate"><span class="pre">last_fn_type</span></code> and call <code class="docutils literal notranslate"><span class="pre">lists_to_dicts</span></code> to form sampled training data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): Transitions list, each element is a transition dict</p></li>
<li><p>unroll_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Learn training unroll length</p></li>
<li><p>last_fn_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The method type name for dealing with last residual data in a traj                 after splitting, should be in [‘last’, ‘drop’, ‘null_padding’]</p></li>
<li><p>null_transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[dict]</span></code>): Dict type null transition, used in <code class="docutils literal notranslate"><span class="pre">null_padding</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): Transitions list processed after unrolling</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="get-gae">
<h3>get_gae<a class="headerlink" href="#get-gae" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.adder.get_gae">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.adder.</span></span><span class="sig-name descname"><span class="pre">get_gae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cuda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.rl_utils.adder.get_gae" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get GAE advantage for stacked transitions(T timestep, 1 batch). Call <code class="docutils literal notranslate"><span class="pre">gae</span></code> for calculation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): Transitions list, each element is a transition dict with at least [‘value’, ‘reward’]</p></li>
<li><p>last_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The last value(i.e.: the T+1 timestep)</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The future discount factor, should be in [0, 1], defaults to 0.99.</p></li>
<li><p>gae_lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): GAE lambda parameter, should be in [0, 1], defaults to 0.97,             when lambda -&gt; 0, it induces bias, but when lambda -&gt; 1, it has high variance due to the sum of terms.</p></li>
<li><p>cuda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether use cuda in GAE computation</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>): transitions list like input one, but each element owns extra advantage key ‘adv’</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># batch_size, timestep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">last_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gae_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_gae</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">last_value</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-gae-with-default-last-value">
<h3>get_gae_with_default_last_value<a class="headerlink" href="#get-gae-with-default-last-value" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.adder.get_gae_with_default_last_value">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.adder.</span></span><span class="sig-name descname"><span class="pre">get_gae_with_default_last_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.deque</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cuda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.rl_utils.adder.get_gae_with_default_last_value" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Like <code class="docutils literal notranslate"><span class="pre">get_gae</span></code> above to get GAE advantage for stacked transitions. However, this function is designed in
case <code class="docutils literal notranslate"><span class="pre">last_value</span></code> is not passed. If transition is not done yet, it wouold assign last value in <code class="docutils literal notranslate"><span class="pre">data</span></code>
as <code class="docutils literal notranslate"><span class="pre">last_value</span></code>, discard the last element in <code class="docutils literal notranslate"><span class="pre">data``(i.e.</span> <span class="pre">len(data)</span> <span class="pre">would</span> <span class="pre">decrease</span> <span class="pre">by</span> <span class="pre">1),</span> <span class="pre">and</span> <span class="pre">then</span> <span class="pre">call</span>
<span class="pre">``get_gae</span></code>. Otherwise it would make <code class="docutils literal notranslate"><span class="pre">last_value</span></code> equal to 0.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list, each element is a transition dict with                 at least[‘value’, ‘reward’]</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether the transition reaches the end of an episode(i.e. whether the env is done)</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The future discount factor, should be in [0, 1], defaults to 0.99.</p></li>
<li><p>gae_lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): GAE lambda parameter, should be in [0, 1], defaults to 0.97,             when lambda -&gt; 0, it induces bias, but when lambda -&gt; 1, it has high variance due to the sum of terms.</p></li>
<li><p>cuda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether use cuda in GAE computation</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): transitions list like input one, but each element owns                 extra advantage key ‘adv’</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="c1"># batch_size, timestep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gae_lambda</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_gae_with_default_last_value</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gae_lambda</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-nstep-return-data">
<h3>get_nstep_return_data<a class="headerlink" href="#get-nstep-return-data" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.adder.get_nstep_return_data">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.adder.</span></span><span class="sig-name descname"><span class="pre">get_nstep_return_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.deque</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cum_reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">correct_terminate_gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">collections.deque</span></span></span><a class="headerlink" href="#ding.rl_utils.adder.get_nstep_return_data" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Process raw traj data by updating keys [‘next_obs’, ‘reward’, ‘done’] in data’s dict element.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list, each element is a transition dict</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Number of steps. If equals to 1, return <code class="docutils literal notranslate"><span class="pre">data</span></code> directly;                 Otherwise update with nstep value.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">deque</span></code>): Transitions list like input one, but each element updated with nstep value.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_obs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Adder</span><span class="o">.</span><span class="n">get_nstep_return_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="get-train-sample">
<h3>get_train_sample<a class="headerlink" href="#get-train-sample" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.adder.get_train_sample">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.adder.</span></span><span class="sig-name descname"><span class="pre">get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unroll_len</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_fn_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'last'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">null_transition</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.rl_utils.adder.get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process raw traj data by updating keys [‘next_obs’, ‘reward’, ‘done’] in data’s dict element.
If <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code> equals to 1, which means no process is needed, can directly return <code class="docutils literal notranslate"><span class="pre">data</span></code>.
Otherwise, <code class="docutils literal notranslate"><span class="pre">data</span></code> will be splitted according to <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code>, process residual part according to
<code class="docutils literal notranslate"><span class="pre">last_fn_type</span></code> and call <code class="docutils literal notranslate"><span class="pre">lists_to_dicts</span></code> to form sampled training data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): Transitions list, each element is a transition dict</p></li>
<li><p>unroll_len (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Learn training unroll length</p></li>
<li><p>last_fn_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The method type name for dealing with last residual data in a traj                 after splitting, should be in [‘last’, ‘drop’, ‘null_padding’]</p></li>
<li><p>null_transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[dict]</span></code>): Dict type null transition, used in <code class="docutils literal notranslate"><span class="pre">null_padding</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): Transitions list processed after unrolling</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="beta-function">
<h2>beta_function<a class="headerlink" href="#beta-function" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/beta_function</span></code> for more details.</p>
<section id="cpw">
<h3>cpw<a class="headerlink" href="#cpw" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.beta_function.cpw">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.beta_function.</span></span><span class="sig-name descname"><span class="pre">cpw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.71</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/beta_function.html#cpw"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.beta_function.cpw" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="cvar">
<h3>CVaR<a class="headerlink" href="#cvar" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.beta_function.CVaR">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.beta_function.</span></span><span class="sig-name descname"><span class="pre">CVaR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.71</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/beta_function.html#CVaR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.beta_function.CVaR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="beta-function-map">
<h3>beta_function_map<a class="headerlink" href="#beta-function-map" title="Permalink to this headline">¶</a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.rl_utils.beta_function_map">
<span class="sig-prename descclassname"><span class="pre">rl_utils.</span></span><span class="sig-name descname"><span class="pre">beta_function_map</span></span><em class="property"> <span class="pre">=</span> <span class="pre">{'CPW':</span> <span class="pre">&lt;function</span> <span class="pre">cpw&gt;,</span> <span class="pre">'CVaR':</span> <span class="pre">&lt;function</span> <span class="pre">CVaR&gt;,</span> <span class="pre">'Pow':</span> <span class="pre">&lt;function</span> <span class="pre">Pow&gt;,</span> <span class="pre">'uniform':</span> <span class="pre">&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;}</span></em><a class="headerlink" href="#ding.rl_utils.beta_function_map" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</section>
<section id="coma">
<h2>coma<a class="headerlink" href="#coma" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/coma</span></code> for more details.</p>
<section id="coma-error">
<h3>coma_error<a class="headerlink" href="#coma-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.coma_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.</span></span><span class="sig-name descname"><span class="pre">coma_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">collections.namedtuple</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/coma.html#coma_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.coma_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of COMA</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): coma input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">coma_data</span></code></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>coma_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the coma loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, A, N)\)</span>, where B is batch size A is the agent num, and N is             action dim</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, A)\)</span></p></li>
<li><p>q_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, A, N)\)</span></p></li>
<li><p>target_q_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, A, N)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((T ,B, A)\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">agent_num</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">coma_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">agent_num</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">agent_num</span><span class="p">)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">q_value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">agent_num</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">target_q_value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">agent_num</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">agent_num</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">coma_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="exploration">
<h2>exploration<a class="headerlink" href="#exploration" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/exploration</span></code> for more details.</p>
<section id="get-epsilon-greedy-fn">
<h3>get_epsilon_greedy_fn<a class="headerlink" href="#get-epsilon-greedy-fn" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.get_epsilon_greedy_fn">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.exploration.</span></span><span class="sig-name descname"><span class="pre">get_epsilon_greedy_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'exp'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#get_epsilon_greedy_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.get_epsilon_greedy_fn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generate an epsilon_greedy function with decay, which inputs current timestep and outputs current epsilon.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>start (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Epsilon start value. For ‘linear’, it should be 1.0.</p></li>
<li><p>end (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Epsilon end value.</p></li>
<li><p>decay (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Controls the speed that epsilon decreases from <code class="docutils literal notranslate"><span class="pre">start</span></code> to <code class="docutils literal notranslate"><span class="pre">end</span></code>.             We recommend epsilon decays according to env step rather than iteration.</p></li>
<li><p>type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): How epsilon decays, now supports [‘linear’, ‘exp’(exponential)]</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>eps_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>): The epsilon greedy function with decay</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="basenoise">
<h3>BaseNoise<a class="headerlink" href="#basenoise" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.BaseNoise">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.exploration.</span></span><span class="sig-name descname"><span class="pre">BaseNoise</span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#BaseNoise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.BaseNoise" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Base class for action noise</p>
</dd>
<dt>Interface:</dt><dd><p>__init__, __call__</p>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">noise_generator</span> <span class="o">=</span> <span class="n">OUNoise</span><span class="p">()</span>  <span class="c1"># init one type of noise</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">noise</span> <span class="o">=</span> <span class="n">noise_generator</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">action</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># generate noise</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.BaseNoise.__call__">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">tuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#BaseNoise.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.BaseNoise.__call__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generate noise according to action tensor’s shape, device</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): size of the action tensor, output noise’s size should be the same</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): device of the action tensor, output noise’s device should be the same as it</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>noise (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): generated action noise,                 have the same shape and device with the input action tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.BaseNoise.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#BaseNoise.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.BaseNoise.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialization method</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="gaussiannoise">
<h3>GaussianNoise<a class="headerlink" href="#gaussiannoise" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.GaussianNoise">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.exploration.</span></span><span class="sig-name descname"><span class="pre">GaussianNoise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#GaussianNoise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.GaussianNoise" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Derived class for generating gaussian noise, which satisfies <span class="math notranslate nohighlight">\(X \sim N(\mu, \sigma^2)\)</span></p>
</dd>
<dt>Interface:</dt><dd><p>__init__, __call__</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.GaussianNoise.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">tuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#GaussianNoise.__call__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.GaussianNoise.__call__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Generate gaussian noise according to action tensor’s shape, device</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>shape (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): size of the action tensor, output noise’s size should be the same</p></li>
<li><p>device (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): device of the action tensor, output noise’s device should be the same as it</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>noise (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): generated action noise,                 have the same shape and device with the input action tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.GaussianNoise.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#GaussianNoise.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.GaussianNoise.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> in Gaussian Distribution</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>mu (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):  <span class="math notranslate nohighlight">\(\mu\)</span> , mean value</p></li>
<li><p>sigma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): <span class="math notranslate nohighlight">\(\sigma\)</span> , standard deviation, should be positive</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="ounoise">
<h3>OUNoise<a class="headerlink" href="#ounoise" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.OUNoise">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.exploration.</span></span><span class="sig-name descname"><span class="pre">OUNoise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">theta</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dt</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x0</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#OUNoise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.OUNoise" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Derived class for generating Ornstein-Uhlenbeck process noise.
Satisfies <span class="math notranslate nohighlight">\(dx_t=\theta(\mu-x_t)dt + \sigma dW_t\)</span>,
where <span class="math notranslate nohighlight">\(W_t\)</span> denotes Weiner Process, acting as a random perturbation term.</p>
</dd>
<dt>Interface:</dt><dd><p>__init__, reset, __call__</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.OUNoise.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#OUNoise.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.OUNoise.reset" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset <code class="docutils literal notranslate"><span class="pre">_x</span></code> to the initial state <code class="docutils literal notranslate"><span class="pre">_x0</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.OUNoise.x0">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">x0</span></span><em class="property"><span class="pre">:</span> <span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#ding.rl_utils.exploration.OUNoise.x0" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get <code class="docutils literal notranslate"><span class="pre">self._x0</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="noise-mapping">
<h3>noise_mapping<a class="headerlink" href="#noise-mapping" title="Permalink to this headline">¶</a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.noise_mapping">
<span class="sig-prename descclassname"><span class="pre">exploration.</span></span><span class="sig-name descname"><span class="pre">noise_mapping</span></span><em class="property"> <span class="pre">=</span> <span class="pre">{'gauss':</span> <span class="pre">&lt;class</span> <span class="pre">'ding.rl_utils.exploration.GaussianNoise'&gt;,</span> <span class="pre">'ou':</span> <span class="pre">&lt;class</span> <span class="pre">'ding.rl_utils.exploration.OUNoise'&gt;}</span></em><a class="headerlink" href="#ding.rl_utils.exploration.noise_mapping" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="create-noise-generator">
<h3>create_noise_generator<a class="headerlink" href="#create-noise-generator" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.exploration.create_noise_generator">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.exploration.</span></span><span class="sig-name descname"><span class="pre">create_noise_generator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">noise_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_kwargs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#ding.rl_utils.exploration.BaseNoise" title="ding.rl_utils.exploration.BaseNoise"><span class="pre">ding.rl_utils.exploration.BaseNoise</span></a></span></span><a class="reference internal" href="../_modules/ding/rl_utils/exploration.html#create_noise_generator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.exploration.create_noise_generator" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Given the key (noise_type), create a new noise generator instance if in noise_mapping’s values,
or raise an KeyError. In other words, a derived noise generator must first register,
then call <code class="docutils literal notranslate"><span class="pre">create_noise</span> <span class="pre">generator</span></code> to get the instance object.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>noise_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): the type of noise generator to be created</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>noise (<a class="reference internal" href="#ding.rl_utils.exploration.BaseNoise" title="ding.rl_utils.exploration.BaseNoise"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BaseNoise</span></code></a>): the created new noise generator, should be an instance of one of             noise_mapping’s values</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="gae">
<h2>gae<a class="headerlink" href="#gae" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/gae</span></code> for more details.</p>
<section id="gae-data">
<h3>gae_data<a class="headerlink" href="#gae-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.gae.gae_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.gae.</span></span><span class="sig-name descname"><span class="pre">gae_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">traj_flag</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.gae.gae_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="shape-fn-gae">
<h3>shape_fn_gae<a class="headerlink" href="#shape-fn-gae" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.gae.shape_fn_gae">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.gae.</span></span><span class="sig-name descname"><span class="pre">shape_fn_gae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/gae.html#shape_fn_gae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.gae.shape_fn_gae" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return shape of gae for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [T, B]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="id2">
<h3>gae<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.gae.gae">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.gae.</span></span><span class="sig-name descname"><span class="pre">gae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.97</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.FloatTensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/gae.html#gae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.gae.gae" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of Generalized Advantage Estimator (arXiv:1506.02438)</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): gae input data with fields [‘value’, ‘reward’], which contains some episodes or             trajectories data.</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the future discount factor, should be in [0, 1], defaults to 0.99.</p></li>
<li><p>lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the gae parameter lambda, should be in [0, 1], defaults to 0.97, when lambda -&gt; 0,             it induces bias, but when lambda -&gt; 1, it has high variance due to the sum of terms.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): the calculated advantage</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is trajectory length and B is batch size</p></li>
<li><p>next_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">gae_data</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">next_value</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adv</span> <span class="o">=</span> <span class="n">gae</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="isw">
<h2>isw<a class="headerlink" href="#isw" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/isw</span></code> for more details.</p>
<section id="compute-importance-weights">
<h3>compute_importance_weights<a class="headerlink" href="#compute-importance-weights" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.isw.compute_importance_weights">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.isw.</span></span><span class="sig-name descname"><span class="pre">compute_importance_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">behaviour_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_space_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">'discrete'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/isw.html#compute_importance_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.isw.compute_importance_weights" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Computing importance sampling weight with given output and action</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,dict]</span></code>): the output taking the action             by the current policy network,             usually this output is network output logit if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,dict]</span></code>): the output taking the action             by the behaviour policy network,            usually this output is network output logit,  if action space is discrete,             or is a dict containing parameters of action distribution if action space is continuous.</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the chosen action(index for the discrete action space) in trajectory,            i.e.: behaviour_action</p></li>
<li><p>action_space_type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): action space types in [‘discrete’, ‘continuous’]</p></li>
<li><p>requires_grad (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether requires grad computation</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>rhos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Importance sampling weight</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.FloatTensor,dict]</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span>,             where T is timestep, B is batch size and N is action dim</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.FloatTensor,dict]</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>rhos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">target_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">behaviour_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rhos</span> <span class="o">=</span> <span class="n">compute_importance_weights</span><span class="p">(</span><span class="n">target_output</span><span class="p">,</span> <span class="n">behaviour_output</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="ppg">
<h2>ppg<a class="headerlink" href="#ppg" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/ppg</span></code> for more details.</p>
<section id="ppg-data">
<h3>ppg_data<a class="headerlink" href="#ppg-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppg.ppg_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppg.</span></span><span class="sig-name descname"><span class="pre">ppg_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logit_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppg.ppg_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppg-joint-loss">
<h3>ppg_joint_loss<a class="headerlink" href="#ppg-joint-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppg.ppg_joint_loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppg.</span></span><span class="sig-name descname"><span class="pre">ppg_joint_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auxiliary_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">behavioral_cloning_loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppg.ppg_joint_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppg-joint-error">
<h3>ppg_joint_error<a class="headerlink" href="#ppg-joint-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.ppg.ppg_joint_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppg.</span></span><span class="sig-name descname"><span class="pre">ppg_joint_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_value_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppg.html#ppg_joint_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.ppg.ppg_joint_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get PPG joint loss</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): ppg input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppg_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): clip value for ratio</p></li>
<li><p>use_value_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether use value clip</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ppg_joint_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppg loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>logit_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B,)\)</span></p></li>
<li><p>value_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, 1)\)</span></p></li>
<li><p>value_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, 1)\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, 1)\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B,)\)</span></p></li>
<li><p>auxiliary_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>behavioral_cloning_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppg_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">ppg_joint_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="ppo">
<h2>ppo<a class="headerlink" href="#ppo" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/ppo</span></code> for more details.</p>
<section id="ppo-data">
<h3>ppo_data<a class="headerlink" href="#ppo-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logit_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppo.ppo_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppo-policy-data">
<h3>ppo_policy_data<a class="headerlink" href="#ppo-policy-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_policy_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_policy_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logit_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppo.ppo_policy_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppo-value-data">
<h3>ppo_value_data<a class="headerlink" href="#ppo-value-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_value_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value_new</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_old</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

</section>
<section id="ppo-loss">
<h3>ppo_loss<a class="headerlink" href="#ppo-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppo.ppo_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppo-policy-loss">
<h3>ppo_policy_loss<a class="headerlink" href="#ppo-policy-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_policy_loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_policy_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppo.ppo_policy_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="ppo-info">
<h3>ppo_info<a class="headerlink" href="#ppo-info" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_info">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_info</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">approx_kl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clipfrac</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.ppo.ppo_info" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="shape-fn-ppo">
<h3>shape_fn_ppo<a class="headerlink" href="#shape-fn-ppo" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.shape_fn_ppo">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">shape_fn_ppo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#shape_fn_ppo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.ppo.shape_fn_ppo" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return shape of ppo for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [B, N]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="ppo-error">
<h3>ppo_error<a class="headerlink" href="#ppo-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_value_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#ppo_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.ppo.ppo_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of Proximal Policy Optimization (arXiv:1707.06347) with value_clip and dual_clip</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppo_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the ppo clip ratio for the constraint of policy update, defaults to 0.2</p></li>
<li><p>use_value_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use clip in value loss with the same ratio as policy</p></li>
<li><p>dual_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): a parameter c mentioned in arXiv:1912.09729 Equ. 5, shoule be in [1, inf),        defaults to 5.0, if you don’t want to use it, set this parameter to None</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ppo_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo loss item, all of them are the differentiable 0-dim tensor</p></li>
<li><p>ppo_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo optim information for monitoring, all of them are Python scalar</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>logit_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppo_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">ppo_error</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>adv is already normalized value (adv - adv.mean()) / (adv.std() + 1e-8), and there are many
ways to calculate this mean and std, like among data buffer or train batch, so we don’t couple
this part into ppo_error, you can refer to our examples for different ways.</p>
</div>
</dd></dl>

</section>
<section id="ppo-policy-error">
<h3>ppo_policy_error<a class="headerlink" href="#ppo-policy-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_policy_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#ppo_policy_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get PPO policy loss</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): ppo input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppo_policy_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): clip value for ratio</p></li>
<li><p>dual_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): a parameter c mentioned in arXiv:1912.09729 Equ. 5, shoule be in [1, inf),        defaults to 5.0, if you don’t want to use it, set this parameter to None</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ppo_policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo policy loss item, all of them are the differentiable 0-dim tensor</p></li>
<li><p>ppo_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo optim information for monitoring, all of them are Python scalar</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>logit_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span>, where B is batch size and N is action dim</p></li>
<li><p>logit_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppo_policy_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">logit_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">ppo_policy_error</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="ppo-value-error">
<h3>ppo_value_error<a class="headerlink" href="#ppo-value-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_value_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_value_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#ppo_value_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Get PPO value loss</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): ppo input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppo_value_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): clip value for ratio</p></li>
<li><p>use_value_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether use value clip</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): the ppo value loss item,             all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>value_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span>, where B is batch size</p></li>
<li><p>value_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppo_value_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">ppo_value_error</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="ppo-error-continuous">
<h3>ppo_error_continuous<a class="headerlink" href="#ppo-error-continuous" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_error_continuous">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_error_continuous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_value_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#ppo_error_continuous"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.ppo.ppo_error_continuous" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of Proximal Policy Optimization (arXiv:1707.06347) with value_clip and dual_clip</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppo_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the ppo clip ratio for the constraint of policy update, defaults to 0.2</p></li>
<li><p>use_value_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): whether to use clip in value loss with the same ratio as policy</p></li>
<li><p>dual_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): a parameter c mentioned in arXiv:1912.09729 Equ. 5, shoule be in [1, inf),        defaults to 5.0, if you don’t want to use it, set this parameter to None</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ppo_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo loss item, all of them are the differentiable 0-dim tensor</p></li>
<li><p>ppo_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo optim information for monitoring, all of them are Python scalar</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>mu_sigma_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): <span class="math notranslate nohighlight">\(((B, N), (B, N))\)</span>, where B is batch size and N is action dim</p></li>
<li><p>mu_sigma_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): <span class="math notranslate nohighlight">\(((B, N), (B, N))\)</span>, where B is batch size and N is action dim</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>value_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>value_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppo_data_continuous</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mu_sigma_new</span><span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mu_sigma_old</span><span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_new</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">value_old</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">return_</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">ppo_error</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>adv is already normalized value (adv - adv.mean()) / (adv.std() + 1e-8), and there are many
ways to calculate this mean and std, like among data buffer or train batch, so we don’t couple
this part into ppo_error, you can refer to our examples for different ways.</p>
</div>
</dd></dl>

</section>
<section id="ppo-policy-error-continuous">
<h3>ppo_policy_error_continuous<a class="headerlink" href="#ppo-policy-error-continuous" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.ppo.ppo_policy_error_continuous">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.ppo.</span></span><span class="sig-name descname"><span class="pre">ppo_policy_error_continuous</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">,</span> </span><span class="pre">collections.namedtuple</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/ppo.html#ppo_policy_error_continuous"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.ppo.ppo_policy_error_continuous" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of Proximal Policy Optimization (arXiv:1707.06347) with dual_clip</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo input data with fieids shown in <code class="docutils literal notranslate"><span class="pre">ppo_data</span></code></p></li>
<li><p>clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the ppo clip ratio for the constraint of policy update, defaults to 0.2</p></li>
<li><p>dual_clip (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): a parameter c mentioned in arXiv:1912.09729 Equ. 5, shoule be in [1, inf),        defaults to 5.0, if you don’t want to use it, set this parameter to None</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>ppo_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo loss item, all of them are the differentiable 0-dim tensor</p></li>
<li><p>ppo_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the ppo optim information for monitoring, all of them are Python scalar</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>mu_sigma_new (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): <span class="math notranslate nohighlight">\(((B, N), (B, N))\)</span>, where B is batch size and N is action dim</p></li>
<li><p>mu_sigma_old (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code>): <span class="math notranslate nohighlight">\(((B, N), (B, N))\)</span>, where B is batch size and N is action dim</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>adv (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>policy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
<li><p>entropy_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">ppo_policy_data_continuous</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mu_sigma_new</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mu_sigma_old</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">action</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">adv</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">ppo_policy_error_continuous</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="retrace">
<h2>retrace<a class="headerlink" href="#retrace" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/retrace</span></code> for more details.</p>
<section id="compute-q-retraces">
<h3>compute_q_retraces<a class="headerlink" href="#compute-q-retraces" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.retrace.compute_q_retraces">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.retrace.</span></span><span class="sig-name descname"><span class="pre">compute_q_retraces</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewards</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/retrace.html#compute_q_retraces"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.retrace.compute_q_retraces" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>q_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T + 1, B, N)\)</span>, where T is unroll_len, B is batch size, N is discrete             action dim.</p></li>
<li><p>v_pred (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T + 1, B, 1)\)</span></p></li>
<li><p>rewards (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>weights (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>q_retraces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((T + 1, B, 1)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="o">=</span><span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="o">=</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span><span class="o">=</span><span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q_values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_pred</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rewards</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actions</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ratio</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q_retraces</span> <span class="o">=</span> <span class="n">compute_q_retraces</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">v_pred</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>q_retrace operation doesn’t need to compute gradient, just executes forward computation.</p>
</div>
</dd></dl>

</section>
</section>
<section id="sampler">
<h2>sampler<a class="headerlink" href="#sampler" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/sampler</span></code> for more details.</p>
<section id="argmaxsampler">
<h3>ArgmaxSampler<a class="headerlink" href="#argmaxsampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.ArgmaxSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">ArgmaxSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#ArgmaxSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.ArgmaxSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Argmax sampler, return the index of the maximum value</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="multinomialsampler">
<h3>MultinomialSampler<a class="headerlink" href="#multinomialsampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.MultinomialSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">MultinomialSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#MultinomialSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.MultinomialSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Multinomial sampler, return the index of the sampled value</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="musampler">
<h3>MuSampler<a class="headerlink" href="#musampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.MuSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">MuSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#MuSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.MuSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Mu sampler, return the mu of the input tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="reparameterizationsampler">
<h3>ReparameterizationSampler<a class="headerlink" href="#reparameterizationsampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.ReparameterizationSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">ReparameterizationSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#ReparameterizationSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.ReparameterizationSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reparameterization sampler, return the reparameterized value of the input tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="hybridstochasticsampler">
<h3>HybridStochasticSampler<a class="headerlink" href="#hybridstochasticsampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.HybridStochasticSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">HybridStochasticSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#HybridStochasticSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.HybridStochasticSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Hybrid stochastic sampler, return the sampled action type and the reparameterized action args</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="hybriddeterminsticsampler">
<h3>HybridDeterminsticSampler<a class="headerlink" href="#hybriddeterminsticsampler" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.sampler.HybridDeterminsticSampler">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.sampler.</span></span><span class="sig-name descname"><span class="pre">HybridDeterminsticSampler</span></span><a class="reference internal" href="../_modules/ding/rl_utils/sampler.html#HybridDeterminsticSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.sampler.HybridDeterminsticSampler" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Hybrid deterministic sampler, return the argmax action type and the mu action args</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="td">
<h2>td<a class="headerlink" href="#td" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/td</span></code> for more details.</p>
<section id="q-1step-td-data">
<h3>q_1step_td_data<a class="headerlink" href="#q-1step-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_1step_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_1step_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.q_1step_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="q-1step-td-error">
<h3>q_1step_td_error<a class="headerlink" href="#q-1step-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_1step_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_1step_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#q_1step_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.q_1step_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>1 step td_error, support single agent case and multi agent case.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_1step_td_data" title="ding.rl_utils.td.q_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_1step_td_data</span></code></a>): The input data, q_1step_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): 1step td error</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_1step_td_data" title="ding.rl_utils.td.q_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_1step_td_data</span></code></a>): the q_1step_td_data containing             [‘q’, ‘next_q’, ‘act’, ‘next_act’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(( , B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_1step_td_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">q_1step_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="m-q-1step-td-data">
<h3>m_q_1step_td_data<a class="headerlink" href="#m-q-1step-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.m_q_1step_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">m_q_1step_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.m_q_1step_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="m-q-1step-td-error">
<h3>m_q_1step_td_error<a class="headerlink" href="#m-q-1step-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.m_q_1step_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">m_q_1step_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">tau:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">alpha:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#m_q_1step_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.m_q_1step_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Munchausen td_error for DQN algorithm, support 1 step td error.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.m_q_1step_td_data" title="ding.rl_utils.td.m_q_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">m_q_1step_td_data</span></code></a>): The input data, m_q_1step_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>tau (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Entropy factor for Munchausen DQN</p></li>
<li><p>alpha (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor for Munchausen term</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): 1step td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.m_q_1step_td_data" title="ding.rl_utils.td.m_q_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">m_q_1step_td_data</span></code></a>): the m_q_1step_td_data containing             [‘q’, ‘target_q’, ‘next_q’, ‘act’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>target_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(( , B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">m_q_1step_td_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">target_q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">m_q_1step_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="q-v-1step-td-data">
<h3>q_v_1step_td_data<a class="headerlink" href="#q-v-1step-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_v_1step_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_v_1step_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.q_v_1step_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="q-v-1step-td-error">
<h3>q_v_1step_td_error<a class="headerlink" href="#q-v-1step-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_v_1step_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_v_1step_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#q_v_1step_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.q_v_1step_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>td_error between q and v value for SAC algorithm, support 1 step td error.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_v_1step_td_data" title="ding.rl_utils.td.q_v_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_v_1step_td_data</span></code></a>): The input data, q_v_1step_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): 1step td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_v_1step_td_data" title="ding.rl_utils.td.q_v_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_v_1step_td_data</span></code></a>): the q_v_1step_td_data containing             [‘q’, ‘v’, ‘act’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(( , B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_dim</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_v_1step_td_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">q</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">v</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">act</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">q_v_1step_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="nstep-return-data">
<h3>nstep_return_data<a class="headerlink" href="#nstep-return-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.nstep_return_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">nstep_return_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.nstep_return_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="nstep-return">
<h3>nstep_return<a class="headerlink" href="#nstep-return" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.nstep_return">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">nstep_return</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">list</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#nstep_return"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.nstep_return" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Calculate nstep return for DQN algorithm, support single agent case and multi agent case.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.nstep_return_data" title="ding.rl_utils.td.nstep_return_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nstep_return_data</span></code></a>): The input data, nstep_return_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Discount factor for value</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep return</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.nstep_return_data" title="ding.rl_utils.td.nstep_return_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nstep_return_data</span></code></a>): the nstep_return_data containing             [‘reward’, ‘next_value’, ‘done’]</p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>next_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((, B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">nstep_return_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">reward</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_value</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">done</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nstep_return</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="dist-1step-td-data">
<h3>dist_1step_td_data<a class="headerlink" href="#dist-1step-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.dist_1step_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dist_1step_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.dist_1step_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="dist-1step-td-error">
<h3>dist_1step_td_error<a class="headerlink" href="#dist-1step-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.dist_1step_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dist_1step_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_min</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_max</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_atom</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#dist_1step_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.dist_1step_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>1 step td_error for distributed q-learning based algorithm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dist_1step_td_data" title="ding.rl_utils.td.dist_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_1step_td_data</span></code></a>): The input data, dist_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>v_min (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The min value of support</p></li>
<li><p>v_max (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The max value of support</p></li>
<li><p>n_atom (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): The num of atom</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dist_1step_td_data" title="ding.rl_utils.td.dist_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_1step_td_data</span></code></a>): the dist_1step_td_data containing            [‘dist’, ‘next_n_dist’, ‘act’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N, n_atom)\)</span> i.e. [batch_size, action_dim, n_atom]</p></li>
<li><p>next_dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N, n_atom)\)</span></p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((, B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">dist_1step_td_data</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">next_dist</span><span class="p">,</span> <span class="n">act</span><span class="p">,</span> <span class="n">next_act</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">dist_1step_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="dist-nstep-td-data">
<h3>dist_nstep_td_data<a class="headerlink" href="#dist-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py attribute">
<dt class="sig sig-object py" id="ding.rl_utils.td.dist_nstep_td_data">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dist_nstep_td_data</span></span><a class="headerlink" href="#ding.rl_utils.td.dist_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#ding.rl_utils.td.dist_1step_td_data" title="ding.rl_utils.td.dist_1step_td_data"><code class="xref py py-class docutils literal notranslate"><span class="pre">ding.rl_utils.td.dist_1step_td_data</span></code></a></p>
</dd></dl>

</section>
<section id="shape-fn-dntd">
<h3>shape_fn_dntd<a class="headerlink" href="#shape-fn-dntd" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.shape_fn_dntd">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">shape_fn_dntd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#shape_fn_dntd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.shape_fn_dntd" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return dntd shape for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [T, B, N, n_atom]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="dist-nstep-td-error">
<h3>dist_nstep_td_error<a class="headerlink" href="#dist-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.dist_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dist_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_min</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_max</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_atom</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#dist_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.dist_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single            agent case and multi agent case.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dist_nstep_td_data" title="ding.rl_utils.td.dist_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_nstep_td_data</span></code></a>): The input data, dist_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dist_nstep_td_data" title="ding.rl_utils.td.dist_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_nstep_td_data</span></code></a>): the dist_nstep_td_data containing            [‘dist’, ‘next_n_dist’, ‘act’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N, n_atom)\)</span> i.e. [batch_size, action_dim, n_atom]</p></li>
<li><p>next_n_dist (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N, n_atom)\)</span></p></li>
<li><p>act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_act (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_n_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">dist_nstep_td_data</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">next_n_dist</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dist_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="v-1step-td-data">
<h3>v_1step_td_data<a class="headerlink" href="#v-1step-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.v_1step_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">v_1step_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.v_1step_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="v-1step-td-error">
<h3>v_1step_td_error<a class="headerlink" href="#v-1step-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.v_1step_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">v_1step_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#v_1step_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.v_1step_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>1 step td_error for distributed value based algorithm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.v_1step_td_data" title="ding.rl_utils.td.v_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_1step_td_data</span></code></a>): The input data, v_1step_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): 1step td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.v_1step_td_data" title="ding.rl_utils.td.v_1step_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_1step_td_data</span></code></a>): the v_1step_td_data containing            [‘v’, ‘next_v’, ‘reward’, ‘done’, ‘weight’]</p></li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span> i.e. [batch_size, ]</p></li>
<li><p>next_v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((, B)\)</span></p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">v_1step_td_data</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">next_v</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">v_1step_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="v-nstep-td-data">
<h3>v_nstep_td_data<a class="headerlink" href="#v-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.v_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">v_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_gamma</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.v_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="v-nstep-td-error">
<h3>v_nstep_td_error<a class="headerlink" href="#v-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.v_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">v_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#v_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.v_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (n step) td_error for distributed value based algorithm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dist_nstep_td_data" title="ding.rl_utils.td.dist_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_nstep_td_data</span></code></a>): The input data, v_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>data (<a class="reference internal" href="#ding.rl_utils.td.dist_nstep_td_data" title="ding.rl_utils.td.dist_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dist_nstep_td_data</span></code></a>): The v_nstep_td_data containing</dt><dd><p>[‘v’, ‘next_n_v’, ‘reward’, ‘done’, ‘weight’, ‘value_gamma’]</p>
</dd>
</dl>
</li>
<li><p>v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span> i.e. [batch_size, ]</p></li>
<li><p>next_v (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
<li><dl class="simple">
<dt>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): If the remaining data in the buffer is less than n_step</dt><dd><p>we use value_gamma as the gamma discount value for next_v rather than gamma**n_step</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">v_nstep_td_data</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">next_v</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">v_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="q-nstep-td-data">
<h3>q_nstep_td_data<a class="headerlink" href="#q-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.q_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="dqfd-nstep-td-data">
<h3>dqfd_nstep_td_data<a class="headerlink" href="#dqfd-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.dqfd_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dqfd_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done_one_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">new_n_q_one_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action_one_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_expert</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.dqfd_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="shape-fn-qntd">
<h3>shape_fn_qntd<a class="headerlink" href="#shape-fn-qntd" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.shape_fn_qntd">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">shape_fn_qntd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#shape_fn_qntd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.shape_fn_qntd" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return qntd shape for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [T, B, N]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="q-nstep-td-error">
<h3>q_nstep_td_error<a class="headerlink" href="#q-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple,</span> <span class="pre">gamma:</span> <span class="pre">Union[float,</span> <span class="pre">list],</span> <span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#q_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.q_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error for q-learning based algorithm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The input data, q_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Gamma discount value for target q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing            [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">q_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="bdq-nstep-td-error">
<h3>bdq_nstep_td_error<a class="headerlink" href="#bdq-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.bdq_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">bdq_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple,</span> <span class="pre">gamma:</span> <span class="pre">Union[float,</span> <span class="pre">list],</span> <span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#bdq_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.bdq_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error for BDQ algorithm, referenced paper “Action Branching Architectures         for Deep Reinforcement Learning”, link: <a class="reference external" href="https://arxiv.org/pdf/1711.08946">https://arxiv.org/pdf/1711.08946</a>.
In fact, the original paper only provides the 1-step TD-error calculation method, and here we extend the         calculation method of n-step TD-error.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The input data, q_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Gamma discount value for target q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing             [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, D, N)\)</span> i.e. [batch_size, branch_num, action_bins_per_branch]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, D, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, D)\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, D)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">action_per_branch</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">action_per_branch</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_per_branch</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">action_per_branch</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">action_per_branch</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">bdq_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="shape-fn-qntd-rescale">
<h3>shape_fn_qntd_rescale<a class="headerlink" href="#shape-fn-qntd-rescale" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.shape_fn_qntd_rescale">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">shape_fn_qntd_rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#shape_fn_qntd_rescale"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.shape_fn_qntd_rescale" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return qntd_rescale shape for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [T, B, N]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="q-nstep-td-error-with-rescale">
<h3>q_nstep_td_error_with_rescale<a class="headerlink" href="#q-nstep-td-error-with-rescale" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_nstep_td_error_with_rescale">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_nstep_td_error_with_rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple,</span> <span class="pre">gamma:</span> <span class="pre">Union[float,</span> <span class="pre">list],</span> <span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None,</span> <span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss(),</span> <span class="pre">trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_transform&gt;,</span> <span class="pre">inv_trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_inv_transform&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#q_nstep_td_error_with_rescale"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.q_nstep_td_error_with_rescale" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error with value rescaling</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The input data, q_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>trans_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Value transfrom function, default to value_transform            (refer to rl_utils/value_rescale.py)</p></li>
<li><p>inv_trans_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): Value inverse transfrom function, default to value_inv_transform            (refer to rl_utils/value_rescale.py)</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing        [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span><span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q_nstep_td_error_with_rescale</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="dqfd-nstep-td-error">
<h3>dqfd_nstep_td_error<a class="headerlink" href="#dqfd-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.dqfd_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dqfd_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_n_step_td:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_supervised_loss:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">margin_function:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_one_step_td:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></em>, <em class="sig-param"><span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em>, <em class="sig-param"><span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em>, <em class="sig-param"><span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#dqfd_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.dqfd_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dqfd_nstep_td_data" title="ding.rl_utils.td.dqfd_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dqfd_nstep_td_data</span></code></a>): The input data, dqfd_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): discount factor</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Gamma discount value for target q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 10</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): the q_nstep_td_data containing            [‘q’, ‘next_n_q’, ‘action’, ‘next_n_action’, ‘reward’, ‘done’, ‘weight’                , ‘new_n_q_one_step’, ‘next_n_action_one_step’, ‘is_expert’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>new_n_q_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>next_n_action_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>is_expert (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) : 0 or 1</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_q_one_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action_one_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_expert</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">dqfd_nstep_td_data</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">done_1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">next_q_one_step</span><span class="p">,</span> <span class="n">next_action_one_step</span><span class="p">,</span> <span class="n">is_expert</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span><span class="p">,</span> <span class="n">loss_statistics</span> <span class="o">=</span> <span class="n">dqfd_nstep_td_error</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">lambda_n_step_td</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lambda_supervised_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">margin_function</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="dqfd-nstep-td-error-with-rescale">
<h3>dqfd_nstep_td_error_with_rescale<a class="headerlink" href="#dqfd-nstep-td-error-with-rescale" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.dqfd_nstep_td_error_with_rescale">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">dqfd_nstep_td_error_with_rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_n_step_td:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_supervised_loss:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">lambda_one_step_td:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">margin_function:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em>, <em class="sig-param"><span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em>, <em class="sig-param"><span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em>, <em class="sig-param"><span class="pre">trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_transform&gt;</span></em>, <em class="sig-param"><span class="pre">inv_trans_fn:</span> <span class="pre">Callable</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">value_inv_transform&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#dqfd_nstep_td_error_with_rescale"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.dqfd_nstep_td_error_with_rescale" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.dqfd_nstep_td_data" title="ding.rl_utils.td.dqfd_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dqfd_nstep_td_data</span></code></a>): The input data, dqfd_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Gamma discount value for target q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 10</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Multistep n step td_error + 1 step td_error            + supervised margin loss, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing            [‘q’, ‘next_n_q’, ‘action’, ‘next_n_action’, ‘reward’, ‘done’, ‘weight’                , ‘new_n_q_one_step’, ‘next_n_action_one_step’, ‘is_expert’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>new_n_q_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>next_n_action_one_step (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>is_expert (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) : 0 or 1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="qrdqn-nstep-td-data">
<h3>qrdqn_nstep_td_data<a class="headerlink" href="#qrdqn-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.qrdqn_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">qrdqn_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.qrdqn_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="qrdqn-nstep-td-error">
<h3>qrdqn_nstep_td_error<a class="headerlink" href="#qrdqn-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.qrdqn_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">qrdqn_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#qrdqn_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.qrdqn_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error with in QRDQN</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.iqn_nstep_td_data" title="ding.rl_utils.td.iqn_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">iqn_nstep_td_data</span></code></a>): The input data, iqn_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing        [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((tau, B, N)\)</span> i.e. [tau x batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((tau', B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">qrdqn_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">qrdqn_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="q-nstep-sql-td-error">
<h3>q_nstep_sql_td_error<a class="headerlink" href="#q-nstep-sql-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.q_nstep_sql_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">q_nstep_sql_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">data:</span> <span class="pre">collections.namedtuple</span></em>, <em class="sig-param"><span class="pre">gamma:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">alpha:</span> <span class="pre">float</span></em>, <em class="sig-param"><span class="pre">nstep:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></em>, <em class="sig-param"><span class="pre">cum_reward:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em>, <em class="sig-param"><span class="pre">value_gamma:</span> <span class="pre">Optional[torch.Tensor]</span> <span class="pre">=</span> <span class="pre">None</span></em>, <em class="sig-param"><span class="pre">criterion:</span> <span class="pre">&lt;module</span> <span class="pre">'torch.nn.modules'</span> <span class="pre">from</span> <span class="pre">'/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/torch/nn/modules/__init__.py'&gt;</span> <span class="pre">=</span> <span class="pre">MSELoss()</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#q_nstep_sql_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.q_nstep_sql_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error for q-learning based algorithm</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The input data, q_nstep_sql_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>Alpha (:obj:｀float`): A parameter to weight entropy term in a policy equation</p></li>
<li><p>cum_reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cumulative nstep reward, which is figured out when collecting data</p></li>
<li><p>value_gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Gamma discount value for target soft_q_value</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 1-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing            [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span> i.e. [batch_size, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>td_error_per_sample (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">q_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span><span class="p">,</span> <span class="n">record_target_v</span> <span class="o">=</span> <span class="n">q_nstep_sql_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="iqn-nstep-td-data">
<h3>iqn_nstep_td_data<a class="headerlink" href="#iqn-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.iqn_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">iqn_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replay_quantiles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.iqn_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>iqn_nstep_td_error
~~~~~~~~~~~~~~~~~~~``
.. autofunction:: ding.rl_utils.td.iqn_nstep_td_error</p>
</section>
<section id="fqf-nstep-td-data">
<h3>fqf_nstep_td_data<a class="headerlink" href="#fqf-nstep-td-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.fqf_nstep_td_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">fqf_nstep_td_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">next_n_action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantiles_hats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.fqf_nstep_td_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="fqf-nstep-td-error">
<h3>fqf_nstep_td_error<a class="headerlink" href="#fqf-nstep-td-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.fqf_nstep_td_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">fqf_nstep_td_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kappa</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#fqf_nstep_td_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.fqf_nstep_td_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Multistep (1 step or n step) td_error with in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             &lt;<a class="reference external" href="https://arxiv.org/pdf/1911.02140.pdf">https://arxiv.org/pdf/1911.02140.pdf</a>&gt;</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.fqf_nstep_td_data" title="ding.rl_utils.td.fqf_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fqf_nstep_td_data</span></code></a>): The input data, fqf_nstep_td_data to calculate loss</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Discount factor</p></li>
<li><p>nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): nstep num, default set to 1</p></li>
<li><p>criterion (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.modules</span></code>): Loss function criterion</p></li>
<li><p>beta_function (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The risk function</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): nstep td error, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>data (<a class="reference internal" href="#ding.rl_utils.td.q_nstep_td_data" title="ding.rl_utils.td.q_nstep_td_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">q_nstep_td_data</span></code></a>): The q_nstep_td_data containing        [‘q’, ‘next_n_q’, ‘action’, ‘reward’, ‘done’]</p></li>
<li><p>q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, tau, N)\)</span> i.e. [batch_size, tau, action_dim]</p></li>
<li><p>next_n_q (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, tau', N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>next_n_action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep(nstep)</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.BoolTensor</span></code>) <span class="math notranslate nohighlight">\((B, )\)</span>, whether done in last timestep</p></li>
<li><p>quantiles_hats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((B, tau)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">next_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nstep</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantiles_hats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">nstep</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">fqf_nstep_td_data</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">next_q</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">quantiles_hats</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">fqf_nstep_td_error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">nstep</span><span class="o">=</span><span class="n">nstep</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="evaluate-quantile-at-action">
<h3>evaluate_quantile_at_action<a class="headerlink" href="#evaluate-quantile-at-action" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.evaluate_quantile_at_action">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">evaluate_quantile_at_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#evaluate_quantile_at_action"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.evaluate_quantile_at_action" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="fqf-calculate-fraction-loss">
<h3>fqf_calculate_fraction_loss<a class="headerlink" href="#fqf-calculate-fraction-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.fqf_calculate_fraction_loss">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">fqf_calculate_fraction_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_tau_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantiles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actions</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#fqf_calculate_fraction_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.fqf_calculate_fraction_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Calculate the fraction loss in FQF,             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning             &lt;<a class="reference external" href="https://arxiv.org/pdf/1911.02140.pdf">https://arxiv.org/pdf/1911.02140.pdf</a>&gt;</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>q_tau_i (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((batch_size, num_quantiles-1, action_dim)\)</span></p></li>
<li><p>q_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((batch_size, num_quantiles, action_dim)\)</span></p></li>
<li><p>quantiles (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((batch_size, num_quantiles+1)\)</span></p></li>
<li><p>actions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((batch_size, )\)</span></p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>fraction_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): fraction loss, 0-dim tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="td-lambda-data">
<h3>td_lambda_data<a class="headerlink" href="#td-lambda-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.td.td_lambda_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">td_lambda_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.td.td_lambda_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="shape-fn-td-lambda">
<h3>shape_fn_td_lambda<a class="headerlink" href="#shape-fn-td-lambda" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.shape_fn_td_lambda">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">shape_fn_td_lambda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#shape_fn_td_lambda"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.shape_fn_td_lambda" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return td_lambda shape for hpc</p>
</dd>
<dt>Returns:</dt><dd><p>shape: [T, B]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="td-lambda-error">
<h3>td_lambda_error<a class="headerlink" href="#td-lambda-error" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.td_lambda_error">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">td_lambda_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.8</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#td_lambda_error"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.td_lambda_error" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Computing TD(lambda) loss given constant gamma and lambda.
There is no special handling for terminal state value,
if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal
(<em>including the terminal state</em>, values[terminal] should also be 0)</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): td_lambda input data with fields [‘value’, ‘reward’, ‘weight’]</p></li>
<li><p>gamma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Constant discount factor gamma, should be in [0, 1], defaults to 0.9</p></li>
<li><p>lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Constant lambda, should be in [0, 1], defaults to 0.8</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Computed MSE loss, averaged over the batch</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T+1, B)\)</span>, where T is trajectory length and B is batch,            which is the estimation of the state value at step 0 to T</p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, the returns from time step 0 to T-1</p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> or None): <span class="math notranslate nohighlight">\((B, )\)</span>, the training sample weight</p></li>
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\(()\)</span>, 0-dim tensor</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">td_lambda_error</span><span class="p">(</span><span class="n">td_lambda_data</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="generalized-lambda-returns">
<h3>generalized_lambda_returns<a class="headerlink" href="#generalized-lambda-returns" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.generalized_lambda_returns">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">generalized_lambda_returns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewards</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gammas</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#generalized_lambda_returns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.generalized_lambda_returns" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Functional equivalent to trfl.value_ops.generalized_lambda_returns
<a class="reference external" href="https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74">https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74</a>
Passing in a number instead of tensor to make the value constant for all samples in batch</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):
estimation of the value at step 0 to <em>T</em>, of size [T_traj+1, batchsize]</p></li>
<li><p>rewards (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The returns from 0 to T-1, of size [T_traj, batchsize]</p></li>
<li><p>gammas (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):
Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]</p></li>
<li><p>lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): Determining the mix of bootstrapping
vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]</p></li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):
Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Computed lambda return value
for each state from 0 to T-1, of size [T_traj, batchsize]</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="multistep-forward-view">
<h3>multistep_forward_view<a class="headerlink" href="#multistep-forward-view" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.td.multistep_forward_view">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.td.</span></span><span class="sig-name descname"><span class="pre">multistep_forward_view</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewards</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gammas</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">done</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/td.html#multistep_forward_view"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.td.multistep_forward_view" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Same as trfl.sequence_ops.multistep_forward_view
Implementing (12.18) in Sutton &amp; Barto</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">result[T-1]</span> <span class="pre">=</span> <span class="pre">rewards[T-1]</span> <span class="pre">+</span> <span class="pre">gammas[T-1]</span> <span class="pre">*</span> <span class="pre">bootstrap_values[T]</span>
<span class="pre">for</span> <span class="pre">t</span> <span class="pre">in</span> <span class="pre">0...T-2</span> <span class="pre">:</span>
<span class="pre">result[t]</span> <span class="pre">=</span> <span class="pre">rewards[t]</span> <span class="pre">+</span> <span class="pre">gammas[t]*(lambdas[t]*result[t+1]</span> <span class="pre">+</span> <span class="pre">(1-lambdas[t])*bootstrap_values[t+1])</span>
<span class="pre">`</span></code></p>
<p>Assuming the first dim of input tensors correspond to the index in batch</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Estimation of the value at <em>step 1 to T</em>, of size [T_traj, batchsize]</p></li>
<li><p>rewards (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The returns from 0 to T-1, of size [T_traj, batchsize]</p></li>
<li><p>gammas (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]</p></li>
<li><dl class="simple">
<dt>lambda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Determining the mix of bootstrapping vs further accumulation of </dt><dd><p>multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored and effectively set to 0, as there is no information about future rewards.</p>
</dd>
</dl>
</li>
<li><p>done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):
Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>ret (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Computed lambda return value </dt><dd><p>for each state from 0 to T-1, of size [T_traj, batchsize]</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="upgo">
<h2>upgo<a class="headerlink" href="#upgo" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/upgo</span></code> for more details.</p>
<section id="upgo-returns">
<h3>upgo_returns<a class="headerlink" href="#upgo-returns" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.upgo.upgo_returns">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.upgo.</span></span><span class="sig-name descname"><span class="pre">upgo_returns</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rewards</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/upgo.html#upgo_returns"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.upgo.upgo_returns" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Computing UPGO return targets. Also notice there is no special handling for the terminal state.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>rewards (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the returns from time step 0 to T-1, </dt><dd><p>of size [T_traj, batchsize]</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): estimation of the state value at step 0 to T, </dt><dd><p>of size [T_traj+1, batchsize]</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>ret (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Computed lambda return value for each state from 0 to T-1, </dt><dd><p>of size [T_traj, batchsize]</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">N2</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bootstrap_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">returns</span> <span class="o">=</span> <span class="n">upgo_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">bootstrap_values</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="upgo-loss">
<h3>upgo_loss<a class="headerlink" href="#upgo-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.upgo.upgo_loss">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.upgo.</span></span><span class="sig-name descname"><span class="pre">upgo_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rhos</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rewards</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/upgo.html#upgo_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.upgo.upgo_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Computing UPGO loss given constant gamma and lambda. There is no special handling for terminal state value,
if the last state in trajectory is the terminal, just pass a 0 as bootstrap_terminal_value.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the output computed by the target policy network, </dt><dd><p>of size [T_traj, batchsize, n_output]</p>
</dd>
</dl>
</li>
<li><p>rhos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the importance sampling ratio, of size [T_traj, batchsize]</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the action taken, of size [T_traj, batchsize]</p></li>
<li><p>rewards (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the returns from time step 0 to T-1, of size [T_traj, batchsize]</p></li>
<li><dl class="simple">
<dt>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): estimation of the state value at step 0 to T, </dt><dd><p>of size [T_traj+1, batchsize]</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Computed importance sampled UPGO loss, averaged over the samples, of size []</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">N2</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rhos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">upgo_loss</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">rhos</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">bootstrap_values</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="value-rescale">
<h2>value_rescale<a class="headerlink" href="#value-rescale" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/value_rescale</span></code> for more details.</p>
<section id="value-transform">
<h3>value_transform<a class="headerlink" href="#value-transform" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.value_rescale.value_transform">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.value_rescale.</span></span><span class="sig-name descname"><span class="pre">value_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/value_rescale.html#value_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.value_rescale.value_transform" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A function to reduce the scale of the action-value function.
:math: <cite>h(x) = sign(x)(sqrt{(abs(x)+1)} - 1) + eps * x</cite> .</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) The input tensor to be normalized.</p></li>
<li><dl class="simple">
<dt>eps: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) The coefficient of the additive regularization term </dt><dd><p>to ensure h^{-1} is Lipschitz continuous</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) Normalized tensor.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>Observe and Look Further: Achieving Consistent Performance on Atari</dt><dd><p>(<a class="reference external" href="https://arxiv.org/abs/1805.11593">https://arxiv.org/abs/1805.11593</a>)</p>
</dd>
</dl>
</div>
</dd></dl>

</section>
<section id="value-inv-transform">
<h3>value_inv_transform<a class="headerlink" href="#value-inv-transform" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.value_rescale.value_inv_transform">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.value_rescale.</span></span><span class="sig-name descname"><span class="pre">value_inv_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/value_rescale.html#value_inv_transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.value_rescale.value_inv_transform" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The inverse form of value rescale.
:math: <cite>h^{-1}(x) = sign(x)({(frac{sqrt{1+4eps(|x|+1+eps)}-1}{2eps})}^2-1)</cite> .</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) The input tensor to be unnormalized.</p></li>
<li><dl class="simple">
<dt>eps: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>) The coefficient of the additive regularization term </dt><dd><p>to ensure h^{-1} is Lipschitz continuous</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) Unnormalized tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="symlog">
<h3>symlog<a class="headerlink" href="#symlog" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.value_rescale.symlog">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.value_rescale.</span></span><span class="sig-name descname"><span class="pre">symlog</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/value_rescale.html#symlog"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.value_rescale.symlog" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>A function to normalize the targets.
:math: <cite>symlog(x) = sign(x)(ln{|x|+1})</cite> .</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) The input tensor to be normalized.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) Normalized tensor.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>Mastering Diverse Domains through World Models</dt><dd><p>(<a class="reference external" href="https://arxiv.org/abs/2301.04104">https://arxiv.org/abs/2301.04104</a>)</p>
</dd>
</dl>
</div>
</dd></dl>

</section>
<section id="inv-symlog">
<h3>inv_symlog<a class="headerlink" href="#inv-symlog" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.value_rescale.inv_symlog">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.value_rescale.</span></span><span class="sig-name descname"><span class="pre">inv_symlog</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../_modules/ding/rl_utils/value_rescale.html#inv_symlog"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.value_rescale.inv_symlog" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The inverse form of symlog.
:math: <cite>symexp(x) = sign(x)(exp{|x|}-1)</cite> .</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>x: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) The input tensor to be unnormalized.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>) Unnormalized tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="vtrace">
<h2>vtrace<a class="headerlink" href="#vtrace" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/rl_utils/vtrace</span></code> for more details.</p>
<section id="vtrace-nstep-return">
<h3>vtrace_nstep_return<a class="headerlink" href="#vtrace-nstep-return" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_nstep_return">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_nstep_return</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">clipped_rhos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clipped_cs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/vtrace.html#vtrace_nstep_return"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_nstep_return" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Computation of vtrace return.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>vtrace_return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): the vtrace loss item, all of them are differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>clipped_rhos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep, B is batch size</p></li>
<li><p>clipped_cs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T+1, B)\)</span></p></li>
<li><p>vtrace_return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>):  <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="vtrace-advantage">
<h3>vtrace_advantage<a class="headerlink" href="#vtrace-advantage" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_advantage">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_advantage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">clipped_pg_rhos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_values</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/vtrace.html#vtrace_advantage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_advantage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Computation of vtrace advantage.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>vtrace_advantage (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the vtrace loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>clipped_pg_rhos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span>, where T is timestep, B is batch size</p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>return (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>bootstrap_values (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>vtrace_advantage (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="vtrace-data">
<h3>vtrace_data<a class="headerlink" href="#vtrace-data" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_data">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">behaviour_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_data" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="vtrace-loss">
<h3>vtrace_loss<a class="headerlink" href="#vtrace-loss" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_loss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">entropy_loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="vtrace-error-discrete-action">
<h3>vtrace_error_discrete_action<a class="headerlink" href="#vtrace-error-discrete-action" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_error_discrete_action">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_error_discrete_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho_pg_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/vtrace.html#vtrace_error_discrete_action"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_error_discrete_action" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): input data with fields shown in <code class="docutils literal notranslate"><span class="pre">vtrace_data</span></code></dt><dd><ul>
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the output taking the action by the current policy network,                usually this output is network output logit</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the output taking the action by the behaviour policy network,                usually this output is network output logit, which is used to produce the trajectory(collector)</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the chosen action(index for the discrete action space) in trajectory,                i.e.: behaviour_action</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>gamma: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the future discount factor, defaults to 0.95</p></li>
<li><p>lambda: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0</p></li>
<li><p>rho_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)</p></li>
<li><p>c_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)</p></li>
<li><p>rho_pg_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>trace_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the vtrace loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span>, where T is timestep, B is batch size and            N is action dim</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T+1, B)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">behaviour_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">vtrace_data</span><span class="p">(</span><span class="n">target_output</span><span class="p">,</span> <span class="n">behaviour_output</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">vtrace_error_discrete_action</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">rho_clip_ratio</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="vtrace-error-continuous-action">
<h3>vtrace_error_continuous_action<a class="headerlink" href="#vtrace-error-continuous-action" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.rl_utils.vtrace.vtrace_error_continuous_action">
<span class="sig-prename descclassname"><span class="pre">ding.rl_utils.vtrace.</span></span><span class="sig-name descname"><span class="pre">vtrace_error_continuous_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho_pg_clip_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/rl_utils/vtrace.html#vtrace_error_continuous_action"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.rl_utils.vtrace.vtrace_error_continuous_action" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Implementation of vtrace(IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner        Architectures), (arXiv:1802.01561)</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): input data with fields shown in <code class="docutils literal notranslate"><span class="pre">vtrace_data</span></code></dt><dd><ul>
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict{key:torch.Tensor}</span></code>): the output taking the action                 by the current policy network, usually this output is network output,                 which represents the distribution by reparameterization trick.</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict{key:torch.Tensor}</span></code>): the output taking the action                 by the behaviour policy network, usually this output is network output logit,                 which represents the distribution by reparameterization trick.</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): the chosen action(index for the discrete action space) in trajectory,                 i.e.: behaviour_action</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>gamma: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the future discount factor, defaults to 0.95</p></li>
<li><p>lambda: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): mix factor between 1-step (lambda_=0) and n-step, defaults to 1.0</p></li>
<li><p>rho_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (rho) when calculating            the baseline targets (vs)</p></li>
<li><p>c_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (c) when calculating            the baseline targets (vs)</p></li>
<li><p>rho_pg_clip_ratio (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): the clipping threshold for importance weights (rho) when calculating            the policy gradient advantage</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>trace_loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): the vtrace loss item, all of them are the differentiable 0-dim tensor</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>target_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict{key:torch.FloatTensor}</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span>,             where T is timestep, B is batch size and             N is action dim. The keys are usually parameters of reparameterization trick.</p></li>
<li><p>behaviour_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict{key:torch.FloatTensor}</span></code>): <span class="math notranslate nohighlight">\((T, B, N)\)</span></p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>): <span class="math notranslate nohighlight">\((T+1, B)\)</span></p></li>
<li><p>reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
<li><p>weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>): <span class="math notranslate nohighlight">\((T, B)\)</span></p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_output</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">behaviour_output</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">)),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">vtrace_data</span><span class="p">(</span><span class="n">target_output</span><span class="p">,</span> <span class="n">behaviour_output</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">vtrace_error_continuous_action</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">rho_clip_ratio</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="policy/index.html" class="btn btn-neutral float-right" title="Policy" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="model.html" class="btn btn-neutral" title="ding.model" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">ding.rl_utils</a><ul>
<li><a class="reference internal" href="#a2c">a2c</a><ul>
<li><a class="reference internal" href="#a2c-error">a2c_error</a></li>
<li><a class="reference internal" href="#a2c-error-continuous">a2c_error_continuous</a></li>
</ul>
</li>
<li><a class="reference internal" href="#acer">acer</a><ul>
<li><a class="reference internal" href="#acer-policy-error">acer_policy_error</a></li>
<li><a class="reference internal" href="#acer-value-error">acer_value_error</a></li>
<li><a class="reference internal" href="#acer-trust-region-update">acer_trust_region_update</a></li>
</ul>
</li>
<li><a class="reference internal" href="#adder">adder</a><ul>
<li><a class="reference internal" href="#id1">Adder</a></li>
<li><a class="reference internal" href="#get-gae">get_gae</a></li>
<li><a class="reference internal" href="#get-gae-with-default-last-value">get_gae_with_default_last_value</a></li>
<li><a class="reference internal" href="#get-nstep-return-data">get_nstep_return_data</a></li>
<li><a class="reference internal" href="#get-train-sample">get_train_sample</a></li>
</ul>
</li>
<li><a class="reference internal" href="#beta-function">beta_function</a><ul>
<li><a class="reference internal" href="#cpw">cpw</a></li>
<li><a class="reference internal" href="#cvar">CVaR</a></li>
<li><a class="reference internal" href="#beta-function-map">beta_function_map</a></li>
</ul>
</li>
<li><a class="reference internal" href="#coma">coma</a><ul>
<li><a class="reference internal" href="#coma-error">coma_error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#exploration">exploration</a><ul>
<li><a class="reference internal" href="#get-epsilon-greedy-fn">get_epsilon_greedy_fn</a></li>
<li><a class="reference internal" href="#basenoise">BaseNoise</a></li>
<li><a class="reference internal" href="#gaussiannoise">GaussianNoise</a></li>
<li><a class="reference internal" href="#ounoise">OUNoise</a></li>
<li><a class="reference internal" href="#noise-mapping">noise_mapping</a></li>
<li><a class="reference internal" href="#create-noise-generator">create_noise_generator</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gae">gae</a><ul>
<li><a class="reference internal" href="#gae-data">gae_data</a></li>
<li><a class="reference internal" href="#shape-fn-gae">shape_fn_gae</a></li>
<li><a class="reference internal" href="#id2">gae</a></li>
</ul>
</li>
<li><a class="reference internal" href="#isw">isw</a><ul>
<li><a class="reference internal" href="#compute-importance-weights">compute_importance_weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ppg">ppg</a><ul>
<li><a class="reference internal" href="#ppg-data">ppg_data</a></li>
<li><a class="reference internal" href="#ppg-joint-loss">ppg_joint_loss</a></li>
<li><a class="reference internal" href="#ppg-joint-error">ppg_joint_error</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ppo">ppo</a><ul>
<li><a class="reference internal" href="#ppo-data">ppo_data</a></li>
<li><a class="reference internal" href="#ppo-policy-data">ppo_policy_data</a></li>
<li><a class="reference internal" href="#ppo-value-data">ppo_value_data</a></li>
<li><a class="reference internal" href="#ppo-loss">ppo_loss</a></li>
<li><a class="reference internal" href="#ppo-policy-loss">ppo_policy_loss</a></li>
<li><a class="reference internal" href="#ppo-info">ppo_info</a></li>
<li><a class="reference internal" href="#shape-fn-ppo">shape_fn_ppo</a></li>
<li><a class="reference internal" href="#ppo-error">ppo_error</a></li>
<li><a class="reference internal" href="#ppo-policy-error">ppo_policy_error</a></li>
<li><a class="reference internal" href="#ppo-value-error">ppo_value_error</a></li>
<li><a class="reference internal" href="#ppo-error-continuous">ppo_error_continuous</a></li>
<li><a class="reference internal" href="#ppo-policy-error-continuous">ppo_policy_error_continuous</a></li>
</ul>
</li>
<li><a class="reference internal" href="#retrace">retrace</a><ul>
<li><a class="reference internal" href="#compute-q-retraces">compute_q_retraces</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sampler">sampler</a><ul>
<li><a class="reference internal" href="#argmaxsampler">ArgmaxSampler</a></li>
<li><a class="reference internal" href="#multinomialsampler">MultinomialSampler</a></li>
<li><a class="reference internal" href="#musampler">MuSampler</a></li>
<li><a class="reference internal" href="#reparameterizationsampler">ReparameterizationSampler</a></li>
<li><a class="reference internal" href="#hybridstochasticsampler">HybridStochasticSampler</a></li>
<li><a class="reference internal" href="#hybriddeterminsticsampler">HybridDeterminsticSampler</a></li>
</ul>
</li>
<li><a class="reference internal" href="#td">td</a><ul>
<li><a class="reference internal" href="#q-1step-td-data">q_1step_td_data</a></li>
<li><a class="reference internal" href="#q-1step-td-error">q_1step_td_error</a></li>
<li><a class="reference internal" href="#m-q-1step-td-data">m_q_1step_td_data</a></li>
<li><a class="reference internal" href="#m-q-1step-td-error">m_q_1step_td_error</a></li>
<li><a class="reference internal" href="#q-v-1step-td-data">q_v_1step_td_data</a></li>
<li><a class="reference internal" href="#q-v-1step-td-error">q_v_1step_td_error</a></li>
<li><a class="reference internal" href="#nstep-return-data">nstep_return_data</a></li>
<li><a class="reference internal" href="#nstep-return">nstep_return</a></li>
<li><a class="reference internal" href="#dist-1step-td-data">dist_1step_td_data</a></li>
<li><a class="reference internal" href="#dist-1step-td-error">dist_1step_td_error</a></li>
<li><a class="reference internal" href="#dist-nstep-td-data">dist_nstep_td_data</a></li>
<li><a class="reference internal" href="#shape-fn-dntd">shape_fn_dntd</a></li>
<li><a class="reference internal" href="#dist-nstep-td-error">dist_nstep_td_error</a></li>
<li><a class="reference internal" href="#v-1step-td-data">v_1step_td_data</a></li>
<li><a class="reference internal" href="#v-1step-td-error">v_1step_td_error</a></li>
<li><a class="reference internal" href="#v-nstep-td-data">v_nstep_td_data</a></li>
<li><a class="reference internal" href="#v-nstep-td-error">v_nstep_td_error</a></li>
<li><a class="reference internal" href="#q-nstep-td-data">q_nstep_td_data</a></li>
<li><a class="reference internal" href="#dqfd-nstep-td-data">dqfd_nstep_td_data</a></li>
<li><a class="reference internal" href="#shape-fn-qntd">shape_fn_qntd</a></li>
<li><a class="reference internal" href="#q-nstep-td-error">q_nstep_td_error</a></li>
<li><a class="reference internal" href="#bdq-nstep-td-error">bdq_nstep_td_error</a></li>
<li><a class="reference internal" href="#shape-fn-qntd-rescale">shape_fn_qntd_rescale</a></li>
<li><a class="reference internal" href="#q-nstep-td-error-with-rescale">q_nstep_td_error_with_rescale</a></li>
<li><a class="reference internal" href="#dqfd-nstep-td-error">dqfd_nstep_td_error</a></li>
<li><a class="reference internal" href="#dqfd-nstep-td-error-with-rescale">dqfd_nstep_td_error_with_rescale</a></li>
<li><a class="reference internal" href="#qrdqn-nstep-td-data">qrdqn_nstep_td_data</a></li>
<li><a class="reference internal" href="#qrdqn-nstep-td-error">qrdqn_nstep_td_error</a></li>
<li><a class="reference internal" href="#q-nstep-sql-td-error">q_nstep_sql_td_error</a></li>
<li><a class="reference internal" href="#iqn-nstep-td-data">iqn_nstep_td_data</a></li>
<li><a class="reference internal" href="#fqf-nstep-td-data">fqf_nstep_td_data</a></li>
<li><a class="reference internal" href="#fqf-nstep-td-error">fqf_nstep_td_error</a></li>
<li><a class="reference internal" href="#evaluate-quantile-at-action">evaluate_quantile_at_action</a></li>
<li><a class="reference internal" href="#fqf-calculate-fraction-loss">fqf_calculate_fraction_loss</a></li>
<li><a class="reference internal" href="#td-lambda-data">td_lambda_data</a></li>
<li><a class="reference internal" href="#shape-fn-td-lambda">shape_fn_td_lambda</a></li>
<li><a class="reference internal" href="#td-lambda-error">td_lambda_error</a></li>
<li><a class="reference internal" href="#generalized-lambda-returns">generalized_lambda_returns</a></li>
<li><a class="reference internal" href="#multistep-forward-view">multistep_forward_view</a></li>
</ul>
</li>
<li><a class="reference internal" href="#upgo">upgo</a><ul>
<li><a class="reference internal" href="#upgo-returns">upgo_returns</a></li>
<li><a class="reference internal" href="#upgo-loss">upgo_loss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#value-rescale">value_rescale</a><ul>
<li><a class="reference internal" href="#value-transform">value_transform</a></li>
<li><a class="reference internal" href="#value-inv-transform">value_inv_transform</a></li>
<li><a class="reference internal" href="#symlog">symlog</a></li>
<li><a class="reference internal" href="#inv-symlog">inv_symlog</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vtrace">vtrace</a><ul>
<li><a class="reference internal" href="#vtrace-nstep-return">vtrace_nstep_return</a></li>
<li><a class="reference internal" href="#vtrace-advantage">vtrace_advantage</a></li>
<li><a class="reference internal" href="#vtrace-data">vtrace_data</a></li>
<li><a class="reference internal" href="#vtrace-loss">vtrace_loss</a></li>
<li><a class="reference internal" href="#vtrace-error-discrete-action">vtrace_error_discrete_action</a></li>
<li><a class="reference internal" href="#vtrace-error-continuous-action">vtrace_error_continuous_action</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ding.policy &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="ding.rl_utils" href="rl_utils.html" />
  <link rel="prev" title="ding.model" href="model.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">API Doc</a> &gt;</li>
        
      <li>ding.policy</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/05_api_doc/policy.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="ding-policy">
<h1>ding.policy<a class="headerlink" href="#ding-policy" title="Permalink to this headline">¶</a></h1>
<section id="base-policy">
<h2>Base Policy<a class="headerlink" href="#base-policy" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/base_policy.py</span></code> for more details.</p>
<section id="policy">
<h3>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.Policy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The basic class of Reinforcement Learning (RL) and Imitation Learning (IL) policy in DI-engine.</p>
</dd>
<dt>Property:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">cfg</span></code>, <code class="docutils literal notranslate"><span class="pre">learn_mode</span></code>, <code class="docutils literal notranslate"><span class="pre">collect_mode</span></code>, <code class="docutils literal notranslate"><span class="pre">eval_mode</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy.__init__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize policy instance according to input configures and model. This method will initialize differnent             fields in policy, including <code class="docutils literal notranslate"><span class="pre">learn</span></code>, <code class="docutils literal notranslate"><span class="pre">collect</span></code>, <code class="docutils literal notranslate"><span class="pre">eval</span></code>. The <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is used to train the             policy, the <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is used to collect data for training, and the <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is used to             evaluate the policy. The <code class="docutils literal notranslate"><span class="pre">enable_field</span></code> is used to specify which field to initialize, if it is None,             then all fields will be initialized.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): The final merged config used to initialize policy. For the default config,                 see the <code class="docutils literal notranslate"><span class="pre">config</span></code> attribute and its comments of policy class.</p></li>
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The neural network model used to initialize policy. If it                 is None, then the model will be created according to <code class="docutils literal notranslate"><span class="pre">default_model</span></code> method and <code class="docutils literal notranslate"><span class="pre">cfg.model</span></code> field.                 Otherwise, the model will be set to the <code class="docutils literal notranslate"><span class="pre">model</span></code> instance created by outside caller.</p></li>
<li><p>enable_field (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[str]]</span></code>): The field list to initialize. If it is None, then all fields                 will be initialized. Otherwise, only the fields in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code> will be initialized, which is                 beneficial to save resources.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the derived policy class, it should implement the <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code>, <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code>, <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code>             method to initialize the corresponding field.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy.__repr__">
<span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy.__repr__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy.__repr__" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the string representation of the policy.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>repr (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The string representation of the policy.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._create_model">
<span class="sig-name descname"><span class="pre">_create_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.nn.modules.module.Module</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._create_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._create_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create or validate the neural network model according to input configures and model. If the input model is             None, then the model will be created according to <code class="docutils literal notranslate"><span class="pre">default_model</span></code> method and <code class="docutils literal notranslate"><span class="pre">cfg.model</span></code> field.             Otherwise, the model will be verified as an instance of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> and set to the <code class="docutils literal notranslate"><span class="pre">model</span></code>             instance created by outside caller.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): The final merged config used to initialize policy.</p></li>
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The neural network model used to initialize policy. User can refer to                 the default model defined in corresponding policy to customize its own model.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The created neural network model. The different modes of policy will                 add distinct wrappers and plugins to the model, which is used to train, collect and evaluate.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><ul class="simple">
<li><p>RuntimeError: If the input model is not None and is not an instance of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._forward_collect">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs, or the action logits to calculate the loss in learn             mode. This method is left to be implemented by the subclass, and more arguments can be added in <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>             part if necessary.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._forward_eval">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance, such as interacting with envs or             computing metrics on validation dataset). Forward means that the policy gets some necessary data (mainly             observation) from the envs and then returns the output data, such as the action to interact with the envs.             This method is left to be implemented by the subclass.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._forward_learn">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss value, policy entropy, q value, priority,             and so on. This method is left to be implemented by the subclass, and more arguments can be added in             <code class="docutils literal notranslate"><span class="pre">data</span></code> item if necessary.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, in the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data should be stacked in                 the batch dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The training information of policy forward, including some metrics for                 monitoring training such as loss, priority, q value, policy entropy, and some data for next step                 training such as priority. Note the output data item should be Python native scalar rather than                 PyTorch tensor, which is convenient for the outside to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._get_attribute">
<span class="sig-name descname"><span class="pre">_get_attribute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._get_attribute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._get_attribute" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>In order to control the access of the policy attributes, we expose different modes to outside rather than             directly use the policy instance. And we also provide a method to get the attribute of the policy in             different modes.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>name (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The name of the attribute.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The value of the attribute.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DI-engine’s policy will first try to access <cite>_get_{name}</cite> method, and then try to access <cite>_{name}</cite>             attribute. If both of them are not found, it will raise a <code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._get_train_sample">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. A train sample can be a processed transition (DQN with nstep TD)             or some multi-timestep transitions (DRQN). This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training, such as nstep reward, advantage, etc.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We will vectorize <code class="docutils literal notranslate"><span class="pre">process_transition</span></code> and <code class="docutils literal notranslate"><span class="pre">get_train_sample</span></code> method in the following release version.             And the user can customize the this data processing procecure by overriding this two methods and collector             itself</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._init_collect">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. This method will be             called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>. Almost different policies have             its own collect mode, so this method must be overrided in subclass.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_collect</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_collect</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._init_eval">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. This method will be             called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>. Almost different policies have             its own eval mode, so this method must be overrided in subclass.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_eval</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_eval</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._init_learn">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. This method will be             called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>. Almost different policies have             its own learn mode, so this method must be overrided in subclass.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._init_multi_gpu_setting">
<span class="sig-name descname"><span class="pre">_init_multi_gpu_setting</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bp_update_sync</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._init_multi_gpu_setting"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._init_multi_gpu_setting" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize multi-gpu data parallel training setting, including broadcast model parameters at the beginning             of the training, and prepare the hook function to allreduce the gradients of model parameters.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The neural network model to be trained.</p></li>
<li><p>bp_update_sync (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to synchronize update the model parameters after allreduce the                 gradients of model parameters. Async update can be parallel in different network layers like pipeline                 so that it can save time.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._load_state_dict_collect">
<span class="sig-name descname"><span class="pre">_load_state_dict_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._load_state_dict_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._load_state_dict_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy collect mode, such as load pretrained state_dict, auto-recover             checkpoint, or model replica from learner in distributed training scenarios.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy collect state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._load_state_dict_eval">
<span class="sig-name descname"><span class="pre">_load_state_dict_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._load_state_dict_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._load_state_dict_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy eval mode, such as load auto-recover             checkpoint, or model replica from learner in distributed training scenarios.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy eval state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The default implementation is <code class="docutils literal notranslate"><span class="pre">['cur_lr',</span> <span class="pre">'total_loss']</span></code>. Other derived classes can overwrite this             method to add their own keys if necessary.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._process_transition">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, such as &lt;s, a, r, s’, done&gt;. Some policies             need to do some special process and pack its own necessary attributes (e.g. hidden state and logit),             so this method is left to be implemented by the subclass.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]]</span></code>): The observation of the current timestep.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. Usually, it contains the action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._reset_collect">
<span class="sig-name descname"><span class="pre">_reset_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._reset_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._reset_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for collect mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in collecting in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is not mandatory to be implemented. The sub-class can overwrite this method if necessary.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._reset_eval">
<span class="sig-name descname"><span class="pre">_reset_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._reset_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._reset_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is not mandatory to be implemented. The sub-class can overwrite this method if necessary.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._reset_learn">
<span class="sig-name descname"><span class="pre">_reset_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._reset_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._reset_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for learn mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different trajectories in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is not mandatory to be implemented. The sub-class can overwrite this method if necessary.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._set_attribute">
<span class="sig-name descname"><span class="pre">_set_attribute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._set_attribute"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._set_attribute" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>In order to control the access of the policy attributes, we expose different modes to outside rather than             directly use the policy instance. And we also provide a method to set the attribute of the policy in             different modes. And the new attribute will named as <code class="docutils literal notranslate"><span class="pre">_{name}</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>name (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): The name of the attribute.</p></li>
<li><p>value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): The value of the attribute.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._state_dict_collect">
<span class="sig-name descname"><span class="pre">_state_dict_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._state_dict_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._state_dict_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of collect mode, only including model in usual, which is necessary for distributed             training scenarios to auto-recover collectors.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy collect state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Not all the scenarios need to auto-recover collectors, sometimes, we can directly shutdown the crashed             collector and renew a new one.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._state_dict_eval">
<span class="sig-name descname"><span class="pre">_state_dict_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._state_dict_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._state_dict_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of eval mode, only including model in usual, which is necessary for distributed             training scenarios to auto-recover evaluators.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy eval state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Not all the scenarios need to auto-recover evaluators, sometimes, we can directly shutdown the crashed             evaluator and renew a new one.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.policy.Policy.collect_mode">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">collect_mode</span></span><em class="property"><span class="pre">:</span> <span class="pre">ding.policy.base_policy.collect_function</span></em><a class="headerlink" href="#ding.policy.Policy.collect_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Return the interfaces of collect mode of policy, which is used to train the model. Here we use namedtuple             to define immutable interfaces and restrict the usage of policy in different mode. Moreover, derived             subclass can override the interfaces to customize its own collect mode.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>interfaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.collect_function</span></code>): The interfaces of collect mode of policy, it is a                 namedtuple whose values of distinct fields are different internal methods.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy_collect</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">collect_mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">env_manager</span><span class="o">.</span><span class="n">ready_obs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inference_output</span> <span class="o">=</span> <span class="n">policy_collect</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env_manager</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">inference_output</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy.default_config">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">default_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">easydict.EasyDict</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy.default_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy.default_config" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the default config of policy. This method is used to create the default config of policy.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): The default config of corresponding policy. For the derived policy class,                 it will recursively merge the default config of base class and its own default config.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method will deepcopy the <code class="docutils literal notranslate"><span class="pre">config</span></code> attribute of the class and return the result. So users don’t need             to worry about the modification of the returned config.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about DQN, its registered name is <code class="docutils literal notranslate"><span class="pre">dqn</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.q_learning.DQN</span></code></p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.policy.Policy.eval_mode">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">eval_mode</span></span><em class="property"><span class="pre">:</span> <span class="pre">ding.policy.base_policy.eval_function</span></em><a class="headerlink" href="#ding.policy.Policy.eval_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Return the interfaces of eval mode of policy, which is used to train the model. Here we use namedtuple             to define immutable interfaces and restrict the usage of policy in different mode. Moreover, derived             subclass can override the interfaces to customize its own eval mode.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>interfaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.eval_function</span></code>): The interfaces of eval mode of policy, it is a namedtuple                 whose values of distinct fields are different internal methods.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy_eval</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">env_manager</span><span class="o">.</span><span class="n">ready_obs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inference_output</span> <span class="o">=</span> <span class="n">policy_eval</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env_manager</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">inference_output</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.policy.Policy.learn_mode">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">learn_mode</span></span><em class="property"><span class="pre">:</span> <span class="pre">ding.policy.base_policy.learn_function</span></em><a class="headerlink" href="#ding.policy.Policy.learn_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Return the interfaces of learn mode of policy, which is used to train the model. Here we use namedtuple             to define immutable interfaces and restrict the usage of policy in different mode. Moreover, derived             subclass can override the interfaces to customize its own learn mode.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>interfaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.learn_function</span></code>): The interfaces of learn mode of policy, it is a namedtuple                 whose values of distinct fields are different internal methods.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy_learn</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">learn_mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_output</span> <span class="o">=</span> <span class="n">policy_learn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">policy_learn</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.Policy.sync_gradients">
<span class="sig-name descname"><span class="pre">sync_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.nn.modules.module.Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#Policy.sync_gradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.Policy.sync_gradients" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Synchronize (allreduce) gradients of model parameters in data-parallel multi-gpu training.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>): The model to synchronize gradients.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is only used in multi-gpu training, and it shoule be called after <code class="docutils literal notranslate"><span class="pre">backward</span></code> method and             before <code class="docutils literal notranslate"><span class="pre">step</span></code> method. The user can also use <code class="docutils literal notranslate"><span class="pre">bp_update_sync</span></code> config to control whether to synchronize             gradients allreduce and optimizer updates.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="commandmodepolicy">
<h3>CommandModePolicy<a class="headerlink" href="#commandmodepolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">CommandModePolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#CommandModePolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CommandModePolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy with command mode, which can be used in old version of DI-engine pipeline: <code class="docutils literal notranslate"><span class="pre">serial_pipeline</span></code>.         <code class="docutils literal notranslate"><span class="pre">CommandModePolicy</span></code> uses <code class="docutils literal notranslate"><span class="pre">_get_setting_learn</span></code>, <code class="docutils literal notranslate"><span class="pre">_get_setting_collect</span></code>, <code class="docutils literal notranslate"><span class="pre">_get_setting_eval</span></code> methods         to exchange information between different workers.</p>
</dd>
<dt>Interface:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">_init_command</span></code>, <code class="docutils literal notranslate"><span class="pre">_get_setting_learn</span></code>, <code class="docutils literal notranslate"><span class="pre">_get_setting_collect</span></code>, <code class="docutils literal notranslate"><span class="pre">_get_setting_eval</span></code></p>
</dd>
<dt>Property:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">command_mode</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy._get_setting_collect">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_get_setting_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">command_info</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#CommandModePolicy._get_setting_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CommandModePolicy._get_setting_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Accoding to <code class="docutils literal notranslate"><span class="pre">command_info</span></code>, i.e., global training information (e.g. training iteration, collected env             step, evaluation results, etc.), return the setting of collect mode, which contains dynamically changed             hyperparameters for collect mode, such as <code class="docutils literal notranslate"><span class="pre">eps</span></code>, <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, etc.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>command_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The global training information, which is defined in <code class="docutils literal notranslate"><span class="pre">commander</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>setting (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The latest setting of collect mode, which is usually used as extra                 arguments of the <code class="docutils literal notranslate"><span class="pre">policy._forward_collect</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy._get_setting_eval">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_get_setting_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">command_info</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#CommandModePolicy._get_setting_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CommandModePolicy._get_setting_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Accoding to <code class="docutils literal notranslate"><span class="pre">command_info</span></code>, i.e., global training information (e.g. training iteration, collected env             step, evaluation results, etc.), return the setting of eval mode, which contains dynamically changed             hyperparameters for eval mode, such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, etc.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>command_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The global training information, which is defined in <code class="docutils literal notranslate"><span class="pre">commander</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>setting (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The latest setting of eval mode, which is usually used as extra                 arguments of the <code class="docutils literal notranslate"><span class="pre">policy._forward_eval</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy._get_setting_learn">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_get_setting_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">command_info</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#CommandModePolicy._get_setting_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CommandModePolicy._get_setting_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Accoding to <code class="docutils literal notranslate"><span class="pre">command_info</span></code>, i.e., global training information (e.g. training iteration, collected env             step, evaluation results, etc.), return the setting of learn mode, which contains dynamically changed             hyperparameters for learn mode, such as <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, etc.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>command_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The global training information, which is defined in <code class="docutils literal notranslate"><span class="pre">commander</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>setting (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The latest setting of learn mode, which is usually used as extra                 arguments of the <code class="docutils literal notranslate"><span class="pre">policy._forward_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy._init_command">
<em class="property"><span class="pre">abstract</span> </em><span class="sig-name descname"><span class="pre">_init_command</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#CommandModePolicy._init_command"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CommandModePolicy._init_command" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the command mode of policy, including related attributes and modules. This method will be             called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">command</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>. Almost different policies have             its own command mode, so this method must be overrided in subclass.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_command</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_command_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._command_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="ding.policy.CommandModePolicy.command_mode">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">command_mode</span></span><em class="property"><span class="pre">:</span> <span class="pre">Policy.command_function</span></em><a class="headerlink" href="#ding.policy.CommandModePolicy.command_mode" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Return the interfaces of command mode of policy, which is used to train the model. Here we use namedtuple             to define immutable interfaces and restrict the usage of policy in different mode. Moreover, derived             subclass can override the interfaces to customize its own command mode.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>interfaces (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.command_function</span></code>): The interfaces of command mode, it is a namedtuple                 whose values of distinct fields are different internal methods.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">CommandModePolicy</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy_command</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">command_mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">settings</span> <span class="o">=</span> <span class="n">policy_command</span><span class="o">.</span><span class="n">get_setting_learn</span><span class="p">(</span><span class="n">command_info</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="create-policy">
<h3>create_policy<a class="headerlink" href="#create-policy" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.create_policy">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">create_policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#ding.policy.Policy" title="ding.policy.base_policy.Policy"><span class="pre">ding.policy.base_policy.Policy</span></a></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#create_policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.create_policy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Create a policy instance according to <code class="docutils literal notranslate"><span class="pre">cfg</span></code> and other kwargs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): Final merged policy config.</p></li>
</ul>
</dd>
<dt>ArgumentsKeys:</dt><dd><ul class="simple">
<li><p>type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Policy type set in <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY.register</span></code> method , such as <code class="docutils literal notranslate"><span class="pre">dqn</span></code> .</p></li>
<li><p>import_names (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): A list of module names (paths) to import before creating policy, such             as <code class="docutils literal notranslate"><span class="pre">ding.policy.dqn</span></code> .</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>policy (<a class="reference internal" href="#ding.policy.Policy" title="ding.policy.Policy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy</span></code></a>): The created policy instance.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code class="docutils literal notranslate"><span class="pre">kwargs</span></code> contains other arguments that need to be passed to the policy constructor. You can refer to         the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of the corresponding policy class for details.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more details about how to merge config, please refer to the system document of DI-engine         (<a class="reference external" href="../03_system/config.html">en link</a>).</p>
</div>
</dd></dl>

</section>
<section id="get-policy-cls">
<h3>get_policy_cls<a class="headerlink" href="#get-policy-cls" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.get_policy_cls">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">get_policy_cls</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">type</span></span></span><a class="reference internal" href="../_modules/ding/policy/base_policy.html#get_policy_cls"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.get_policy_cls" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get policy class according to <code class="docutils literal notranslate"><span class="pre">cfg</span></code>, which is used to access related class variables/methods.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): Final merged policy config.</p></li>
</ul>
</dd>
<dt>ArgumentsKeys:</dt><dd><ul class="simple">
<li><p>type (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>): Policy type set in <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY.register</span></code> method , such as <code class="docutils literal notranslate"><span class="pre">dqn</span></code> .</p></li>
<li><p>import_names (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): A list of module names (paths) to import before creating policy, such             as <code class="docutils literal notranslate"><span class="pre">ding.policy.dqn</span></code> .</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>policy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code>): The policy class.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="dqn">
<h2>DQN<a class="headerlink" href="#dqn" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/dqn.py</span></code> for more details.</p>
<section id="dqnpolicy">
<h3>DQNPolicy<a class="headerlink" href="#dqnpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of DQN algorithm, extended by Double DQN/Dueling DQN/PER/multi-step TD.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 36%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>dqn</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling</div>
<div class="line">Weight to correct biased update. If</div>
<div class="line">True, priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.97,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>1,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.dueling</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">dueling head architecture</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.encoder</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_hidden</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_size_list</span></code></div>
</div>
</td>
<td><p>list
(int)</p></td>
<td><p>[32, 64,
64, 128]</p></td>
<td><div class="line-block">
<div class="line">Sequence of <code class="docutils literal notranslate"><span class="pre">hidden_size</span></code> of</div>
<div class="line">subsequent conv layers and the</div>
<div class="line">final dense layer.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">default kernel_size</div>
<div class="line">is [8, 4, 3]</div>
<div class="line">default stride is</div>
<div class="line">[4, 2 ,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.dropout</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>None</p></td>
<td><div class="line-block">
<div class="line">Dropout rate for dropout layers.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
<div class="line">If set to <code class="docutils literal notranslate"><span class="pre">None</span></code></div>
<div class="line">means no dropout</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>3</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection.</div>
<div class="line">Only valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>64</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>100</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">theta</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
<div class="line">Only one of [target_update_freq,</div>
<div class="line">target_theta] should be set</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Soft(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>17</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>[8, 128]</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>18</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_episode</span></code></p></td>
<td><p>int</p></td>
<td><p>8</p></td>
<td><div class="line-block">
<div class="line">The number of training episodes of a</div>
<div class="line">call of collector</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">only one of [n_sample</div>
<div class="line">,n_episode] should</div>
<div class="line">be set</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>19</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.type</span></code></div>
</div>
</td>
<td><p>str</p></td>
<td><p>exp</p></td>
<td><div class="line-block">
<div class="line">exploration rate decay type</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Support [‘exp’,</div>
<div class="line">‘linear’].</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>21</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">start</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.95</p></td>
<td><div class="line-block">
<div class="line">start value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>22</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">end</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">end value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>23</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">decay</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">decay length of exploration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">greater than 0. set</div>
<div class="line">decay=10000 means</div>
<div class="line">the exploration rate</div>
<div class="line">decay from start</div>
<div class="line">value to end value</div>
<div class="line">during decay length.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs. Besides, this policy also needs <code class="docutils literal notranslate"><span class="pre">eps</span></code> argument for             exploration, i.e., classic epsilon-greedy exploration strategy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The epsilon value for exploration.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_dqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_dqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, q value, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For DQN, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_dqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In DQN with nstep TD, a train sample is a processed transition.             This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 in the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is similar in format                 to input transitions, but may contain more data for training, such as nstep reward and target obs.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For DQN, it contains the             collect_model to balance the exploration and exploitation with epsilon-greedy sample mechanism, and other             algorithm-specific arguments such as unroll_len and nstep.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and nstep in DQN. This             design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For DQN, it contains the             eval model to greedily select action with argmax q_value mechanism.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DQN, it mainly contains             optimizer, algorithm-specific arguments such as nstep and gamma, main and target model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For DQN, it contains obs, next_obs, action, reward, done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For DQN, it contains the action and the logit (q_value) of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about DQN, its registered name is <code class="docutils literal notranslate"><span class="pre">dqn</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.q_learning</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="dqnstdimpolicy">
<h3>DQNSTDIMPolicy<a class="headerlink" href="#dqnstdimpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DQNSTDIMPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of DQN algorithm, extended by ST-DIM auxiliary objectives.
ST-DIM paper link: <a class="reference external" href="https://arxiv.org/abs/1906.08226">https://arxiv.org/abs/1906.08226</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>dqn_stdim</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line">to correct biased update. If True,</div>
<div class="line">priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.97,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>1,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">_gpu</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>3</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>64</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>100</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>[8, 128]</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.type</span></code></div>
</div>
</td>
<td><p>str</p></td>
<td><p>exp</p></td>
<td><div class="line-block">
<div class="line">exploration rate decay type</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Support [‘exp’,</div>
<div class="line">‘linear’].</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">start</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.95</p></td>
<td><div class="line-block">
<div class="line">start value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">end</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">end value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">decay</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">decay length of exploration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">greater than 0. set</div>
<div class="line">decay=10000 means</div>
<div class="line">the exploration rate</div>
<div class="line">decay from start</div>
<div class="line">value to end value</div>
<div class="line">during decay length.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">aux_loss</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">the ratio of the auxiliary loss to</div>
<div class="line">the TD loss</div>
</div>
</td>
<td><div class="line-block">
<div class="line">any real value,</div>
<div class="line">typically in</div>
<div class="line">[-0.1, 0.1].</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, q value, priority, aux_loss.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For DQNSTDIM, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>,                 <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as                 <code class="docutils literal notranslate"><span class="pre">weight</span></code> and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DQNSTDIM, it first             call super class’s <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, then initialize extra auxiliary model, its optimizer, and the             loss weight. This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._model_encode">
<span class="sig-name descname"><span class="pre">_model_encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._model_encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._model_encode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the encoding of the main model as input for the auxiliary model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, same as the _forward_learn input.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[torch.Tensor]</span></code>): the tuple of two tensors to apply contrastive embedding learning.                 In ST-DIM algorithm, these two variables are the dqn encoding of <cite>obs</cite> and <cite>next_obs</cite> respectively.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DQNSTDIMPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dqn.html#DQNSTDIMPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DQNSTDIMPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="ppo">
<h2>PPO<a class="headerlink" href="#ppo" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/ppo.py</span></code> for more details.</p>
<section id="ppopolicy">
<h3>PPOPolicy<a class="headerlink" href="#ppopolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PPOPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of on-policy version PPO algorithm. Paper link: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (action logit and value) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code>                 method. The key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to add more tricks on this policy, like temperature factor in multinomial sample, you can pass             related data as extra keyword arguments of this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs. <code class="docutils literal notranslate"><span class="pre">_forward_eval</span></code> in PPO often uses deterministic sample method to get             actions while <code class="docutils literal notranslate"><span class="pre">_forward_collect</span></code> usually uses stochastic sample method for balance exploration and             exploitation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, clipfrac, approx_kl.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including the latest                 collected training samples for on-policy algorithms like PPO. For each element in list, the key of the                 dict is the name of data items and the value is the corresponding data. Usually, the value is                 torch.Tensor or np.ndarray or there dict/list combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data                 often need to first be stacked in the batch dimension by some utility functions such as                 <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For PPO, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">logit</span></code>, <code class="docutils literal notranslate"><span class="pre">value</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>return_infos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The information list that indicated training result, each                 training iteration contains append a information dict into the final list. The list will be precessed                 and recorded in text log and tensorboard. The value of the dict must be python scalar or a list of                 scalars. For the detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The training procedure of PPO is two for loops. The outer loop trains all the collected training samples             with <code class="docutils literal notranslate"><span class="pre">epoch_per_collect</span></code> epochs. The inner loop splits all the data into different mini-batch with             the length of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In PPO, a train sample is a processed transition with new computed             <code class="docutils literal notranslate"><span class="pre">traj_flag</span></code> and <code class="docutils literal notranslate"><span class="pre">adv</span></code> field. This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training, such as GAE advantage.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For PPO, it contains the             collect_model to balance the exploration and exploitation (e.g. the multinomial sample mechanism in             discrete action space), and other algorithm-specific arguments such as unroll_len and gae_lambda.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and gae_lambda in PPO.             This design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For PPO, it contains the             eval model to select optimial action (e.g. greedily select action with argmax mechanism in discrete action).
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For PPO, it mainly contains             optimizer, algorithm-specific arguments such as loss weight, clip_ratio and recompute_adv. This method             also executes some special network initializations and prepares running mean/std monitor for value.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For PPO, it contains obs, next_obs, action, reward, done, logit, value.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For PPO, it contains the state value, action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">next_obs</span></code> is used to calculate nstep return when necessary, so we place in into transition by default.             You can delete this field to save memory occupancy if you do not need nstep return.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about PPO, its registered name is <code class="docutils literal notranslate"><span class="pre">ppo</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.vac</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because now PPO supports both single-agent and multi-agent usages, so we can implement these functions             with the same policy and two different default models, which is controled by <code class="docutils literal notranslate"><span class="pre">self._cfg.multi_agent</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="ppopgpolicy">
<h3>PPOPGPolicy<a class="headerlink" href="#ppopgpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PPOPGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of on policy version PPO algorithm (pure policy gradient without value network).
Paper link: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (action logit) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code>                 method. The key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to add more tricks on this policy, like temperature factor in multinomial sample, you can pass             related data as extra keyword arguments of this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs. <code class="docutils literal notranslate"><span class="pre">_forward_eval</span></code> in PPO often uses deterministic sample method to get             actions while <code class="docutils literal notranslate"><span class="pre">_forward_collect</span></code> usually uses stochastic sample method for balance exploration and             exploitation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, clipfrac, approx_kl.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including the latest                 collected training samples for on-policy algorithms like PPO. For each element in list, the key of the                 dict is the name of data items and the value is the corresponding data. Usually, the value is                 torch.Tensor or np.ndarray or there dict/list combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data                 often need to first be stacked in the batch dimension by some utility functions such as                 <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For PPOPG, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">return</span></code>, <code class="docutils literal notranslate"><span class="pre">logit</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>return_infos (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The information list that indicated training result, each                 training iteration contains append a information dict into the final list. The list will be precessed                 and recorded in text log and tensorboard. The value of the dict must be python scalar or a list of                 scalars. For the detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The training procedure of PPOPG is two for loops. The outer loop trains all the collected training samples             with <code class="docutils literal notranslate"><span class="pre">epoch_per_collect</span></code> epochs. The inner loop splits all the data into different mini-batch with             the length of <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given entire episode data (a list of transition), process it into a list of sample that             can be used for training directly. In PPOPG, a train sample is a processed transition with new computed             <code class="docutils literal notranslate"><span class="pre">return</span></code> field. This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The episode data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training, such as discounted episode return.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For PPOPG, it contains             the collect_model to balance the exploration and exploitation (e.g. the multinomial sample mechanism in             discrete action space), and other algorithm-specific arguments such as unroll_len and gae_lambda.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and gae_lambda in PPO.             This design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For PPOPG, it contains the             eval model to select optimial action (e.g. greedily select action with argmax mechanism in discrete action).
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For PPOPG, it mainly             contains optimizer, algorithm-specific arguments such as loss weight and clip_ratio. This method             also executes some special network initializations.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For PPOPG, it contains obs, action, reward, done, logit.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For PPOPG, it contains the action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOPGPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOPGPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOPGPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="ppooffpolicy">
<h3>PPOOffPolicy<a class="headerlink" href="#ppooffpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PPOOffPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of off-policy version PPO algorithm. Paper link: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>.
This version is more suitable for large-scale distributed training.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (action logit and value) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code>                 method. The key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to add more tricks on this policy, like temperature factor in multinomial sample, you can pass             related data as extra keyword arguments of this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOOffPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs. <code class="docutils literal notranslate"><span class="pre">_forward_eval</span></code> in PPO often uses deterministic sample method to get             actions while <code class="docutils literal notranslate"><span class="pre">_forward_collect</span></code> usually uses stochastic sample method for balance exploration and             exploitation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PPOOffPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ppo</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, clipfrac and approx_kl.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For PPOOff, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">adv</span></code>,                 <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">logit</span></code>, <code class="docutils literal notranslate"><span class="pre">value</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In PPO, a train sample is a processed transition with new computed             <code class="docutils literal notranslate"><span class="pre">traj_flag</span></code> and <code class="docutils literal notranslate"><span class="pre">adv</span></code> field. This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training, such as GAE advantage.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For PPOOff, it contains             collect_model to balance the exploration and exploitation (e.g. the multinomial sample mechanism in             discrete action space), and other algorithm-specific arguments such as unroll_len and gae_lambda.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and gae_lambda in PPOOff.
This design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For PPOOff, it contains the             eval model to select optimial action (e.g. greedily select action with argmax mechanism in discrete action).
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For PPOOff, it mainly             contains optimizer, algorithm-specific arguments such as loss weight and clip_ratio. This method             also executes some special network initializations and prepares running mean/std monitor for value.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For PPO, it contains obs, next_obs, action, reward, done, logit, value.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For PPO, it contains the state value, action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">next_obs</span></code> is used to calculate nstep return when necessary, so we place in into transition by default.             You can delete this field to save memory occupancy if you do not need nstep return.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOOffPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOOffPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOOffPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="ppostdimpolicy">
<h3>PPOSTDIMPolicy<a class="headerlink" href="#ppostdimpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PPOSTDIMPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of on policy version PPO algorithm with ST-DIM auxiliary model.
PPO paper link: <a class="reference external" href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>.
ST-DIM paper link: <a class="reference external" href="https://arxiv.org/abs/1906.08226">https://arxiv.org/abs/1906.08226</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Forward and backward function of learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>):
Including current lr, total_loss, policy_loss, value_loss, entropy_loss,                         adv_abs_max, approx_kl, clipfrac</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Learn mode init method. Called by <code class="docutils literal notranslate"><span class="pre">self.__init__</span></code>.
Init the auxiliary model, its optimizer, and the axuliary loss weight to the main loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._model_encode">
<span class="sig-name descname"><span class="pre">_model_encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._model_encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._model_encode" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Get the encoding of the main model as input for the auxiliary model.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">dict</span></code>): Dict type data, same as the _forward_learn input.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>(<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[Tensor]</span></code>): the tuple of two tensors to apply contrastive embedding learning.</dt><dd><p>In ST-DIM algorithm, these two variables are the dqn encoding of <cite>obs</cite> and <cite>next_obs</cite>                respectively.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PPOSTDIMPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ppo.html#PPOSTDIMPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PPOSTDIMPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, optimizer and aux_optimizer for             representation learning.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="bc">
<h2>BC<a class="headerlink" href="#bc" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/bc.py</span></code> for more details.</p>
<section id="behaviourcloningpolicy">
<h3>BehaviourCloningPolicy<a class="headerlink" href="#behaviourcloningpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">BehaviourCloningPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Behaviour Cloning (BC) policy class, which supports both discrete and continuous action space.         The policy is trained by supervised learning, and the data is a offline dataset collected by expert.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss and time.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For BC, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>BC policy uses offline dataset so it does not need to collect data. However, sometimes we need to use the             trained BC policy to collect data for other purposes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For BC, it contains the             eval model to greedily select action with argmax q_value mechanism for discrete action space.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For BC, it mainly contains             optimizer, algorithm-specific arguments such as lr_scheduler, loss, etc.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.BehaviourCloningPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/bc.html#BehaviourCloningPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.BehaviourCloningPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about discrete BC, its registered name is <code class="docutils literal notranslate"><span class="pre">discrete_bc</span></code> and the             import_names is <code class="docutils literal notranslate"><span class="pre">ding.model.template.bc</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="ddpg">
<h2>DDPG<a class="headerlink" href="#ddpg" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/ddpg.py</span></code> for more details.</p>
<section id="ddpgpolicy">
<h3>DDPGPolicy<a class="headerlink" href="#ddpgpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DDPGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of DDPG algorithm. Paper link: <a class="reference external" href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 33%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">type</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>str</p></td>
<td><p>ddpg</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cuda</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>25000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 25000 for</div>
<div class="line">DDPG/TD3, 10000 for</div>
<div class="line">sac.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.twin_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">critic</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use two critic</div>
<div class="line">networks or only one.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default False for</div>
<div class="line">DDPG, Clipped Double</div>
<div class="line">Q-learning method in</div>
<div class="line">TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_actor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rate for actor</div>
<div class="line">network(aka. policy).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_critic</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rates for critic</div>
<div class="line">network (aka. Q-network).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.actor_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>2</p></td>
<td><div class="line-block">
<div class="line">When critic network updates</div>
<div class="line">once, how many times will actor</div>
<div class="line">network update.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default 1 for DDPG,</div>
<div class="line">2 for TD3. Delayed</div>
<div class="line">Policy Updates method</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.noise</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to add noise on target</div>
<div class="line">network’s action.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default False for</div>
<div class="line">DDPG, True for TD3.</div>
<div class="line">Target Policy Smoo-</div>
<div class="line">thing Regularization</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in halfcheetah env.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver-</div>
<div class="line">aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">noise_sigma</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">Used for add noise during co-</div>
<div class="line">llection, through controlling</div>
<div class="line">the sigma of distribution</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Sample noise from dis-</div>
<div class="line">tribution, Ornstein-</div>
<div class="line">Uhlenbeck process in</div>
<div class="line">DDPG paper, Gaussian</div>
<div class="line">process in ours.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For DDPG, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">logit</span></code> which is used for hybrid action space.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In DDPG, a train sample is a processed transition (unroll_len=1).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For DDPG, it contains the             collect_model to balance the exploration and exploitation with the perturbed noise mechanism, and other             algorithm-specific arguments such as unroll_len.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For DDPG, it contains the             eval model to greedily select action type with argmax q_value mechanism for hybrid action space.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DDPG, it mainly             contains two optimizers, algorithm-specific arguments such as gamma and twin_critic, main and target model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For DDPG, it contains obs, next_obs, action, reward, done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For DDPG, it contains the action and the logit of the action (in hybrid action space).</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizers.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DDPGPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DDPGPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="td3">
<h2>TD3<a class="headerlink" href="#td3" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/td3.py</span></code> for more details.</p>
<section id="td3policy">
<h3>TD3Policy<a class="headerlink" href="#td3policy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.TD3Policy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">TD3Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/td3.html#TD3Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.TD3Policy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of TD3 algorithm. Since DDPG and TD3 share many common things, we can easily derive this TD3         class from DDPG class by changing <code class="docutils literal notranslate"><span class="pre">_actor_update_freq</span></code>, <code class="docutils literal notranslate"><span class="pre">_twin_critic</span></code> and noise in model wrapper.
Paper link: <a class="reference external" href="https://arxiv.org/pdf/1802.09477.pdf">https://arxiv.org/pdf/1802.09477.pdf</a></p>
</dd>
</dl>
<p>Config:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 8%" />
<col style="width: 17%" />
<col style="width: 32%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">type</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>str</p></td>
<td><p>td3</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cuda</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>25000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 25000 for</div>
<div class="line">DDPG/TD3, 10000 for</div>
<div class="line">sac.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.twin_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">critic</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to use two critic</div>
<div class="line">networks or only one.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default True for TD3,</div>
<div class="line">Clipped Double</div>
<div class="line">Q-learning method in</div>
<div class="line">TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_actor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rate for actor</div>
<div class="line">network(aka. policy).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_critic</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rates for critic</div>
<div class="line">network (aka. Q-network).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.actor_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>2</p></td>
<td><div class="line-block">
<div class="line">When critic network updates</div>
<div class="line">once, how many times will actor</div>
<div class="line">network update.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default 2 for TD3, 1</div>
<div class="line">for DDPG. Delayed</div>
<div class="line">Policy Updates method</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.noise</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to add noise on target</div>
<div class="line">network’s action.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default True for TD3,</div>
<div class="line">False for DDPG.</div>
<div class="line">Target Policy Smoo-</div>
<div class="line">thing Regularization</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.noise_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">range</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>dict</p></td>
<td><div class="line-block">
<div class="line">dict(min=-0.5,</div>
<div class="line-block">
<div class="line">max=0.5,)</div>
<div class="line"><br /></div>
</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Limit for range of target</div>
<div class="line">policy smoothing noise,</div>
<div class="line">aka. noise_clip.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in halfcheetah env.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver</div>
<div class="line">-aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">noise_sigma</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">Used for add noise during co-</div>
<div class="line">llection, through controlling</div>
<div class="line">the sigma of distribution</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Sample noise from dis</div>
<div class="line">-tribution, Ornstein-</div>
<div class="line">Uhlenbeck process in</div>
<div class="line">DDPG paper, Gaussian</div>
<div class="line">process in ours.</div>
</div>
</td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For DDPG, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">logit</span></code> which is used for hybrid action space.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DDPGPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_ddpg</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In DDPG, a train sample is a processed transition (unroll_len=1).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For DDPG, it contains the             collect_model to balance the exploration and exploitation with the perturbed noise mechanism, and other             algorithm-specific arguments such as unroll_len.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For DDPG, it contains the             eval model to greedily select action type with argmax q_value mechanism for hybrid action space.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DDPG, it mainly             contains two optimizers, algorithm-specific arguments such as gamma and twin_critic, main and target model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/td3.html#TD3Policy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.TD3Policy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For DDPG, it contains obs, next_obs, action, reward, done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For DDPG, it contains the action and the logit of the action (in hybrid action space).</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizers.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.TD3Policy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#ding.policy.TD3Policy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="sac">
<h2>SAC<a class="headerlink" href="#sac" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/sac.py</span></code> for more details.</p>
<section id="sacpolicy">
<h3>SACPolicy<a class="headerlink" href="#sacpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.SACPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">SACPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of continuous SAC algorithm. Paper link: <a class="reference external" href="https://arxiv.org/pdf/1801.01290.pdf">https://arxiv.org/pdf/1801.01290.pdf</a></p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 33%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>sac</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">SAC is an off-policy</div>
<div class="line">algorithm.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use priority</div>
<div class="line">sampling in buffer.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling</div>
<div class="line">weight to correct biased update</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 10000 for</div>
<div class="line">SAC, 25000 for DDPG/</div>
<div class="line">TD3.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_q</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for soft q</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3</div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_policy</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for policy</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3</div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.2</p></td>
<td><div class="line-block">
<div class="line">Entropy regularization</div>
<div class="line">coefficient.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">alpha is initiali-</div>
<div class="line">zation for auto</div>
<div class="line">alpha, when</div>
<div class="line">auto_alpha is True</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to use</div>
<div class="line">auto temperature parameter</div>
<div class="line">alpha.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Temperature parameter</div>
<div class="line">determines the</div>
<div class="line">relative importance</div>
<div class="line">of the entropy term</div>
<div class="line">against the reward.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in env like Pendulum</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver</div>
<div class="line">aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">logit</span></code> in SAC means the mu and sigma of Gaussioan distribution. Here we use this name for consistency.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for SACPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">logit</span></code> in SAC means the mu and sigma of Gaussioan distribution. Here we use this name for consistency.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for SACPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For SAC, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for SACPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In continuous SAC, a train sample is a processed transition             (unroll_len=1).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For SAC, it contains the             collect_model other algorithm-specific arguments such as unroll_len.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For SAC, it contains the             eval model, which is equipped with <code class="docutils literal notranslate"><span class="pre">base</span></code> model wrapper to ensure compability.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For SAC, it mainly             contains three optimizers, algorithm-specific arguments such as gamma and twin_critic, main and target             model. Especially, the <code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code> mechanism for balancing max entropy target is also initialized here.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For continuous SAC, it contains obs, next_obs, action, reward, done. The logit             will be also added when <code class="docutils literal notranslate"><span class="pre">collector_logit</span></code> is True.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For continuous SAC, it contains the action and the logit (mu and sigma) of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizers.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SACPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SACPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SACPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="discretesacpolicy">
<h3>DiscreteSACPolicy<a class="headerlink" href="#discretesacpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteSACPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of discrete SAC algorithm. Paper link: <a class="reference external" href="https://arxiv.org/abs/1910.07207">https://arxiv.org/abs/1910.07207</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs. Besides, this policy also needs <code class="docutils literal notranslate"><span class="pre">eps</span></code> argument for             exploration, i.e., classic epsilon-greedy exploration strategy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The epsilon value for exploration.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DiscreteSACPolicy:             <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_discrete_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DiscreteSACPolicy:             <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_discrete_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For SAC, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">logit</span></code>, <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys like <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for DiscreteSACPolicy:             <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_discrete_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In discrete SAC, a train sample is a processed transition (unroll_len=1).</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For SAC, it contains the             collect_model to balance the exploration and exploitation with the epsilon and multinomial sample             mechanism, and other algorithm-specific arguments such as unroll_len.             This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For DiscreteSAC, it contains             the eval model to greedily select action type with argmax q_value mechanism.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DiscreteSAC, it mainly             contains three optimizers, algorithm-specific arguments such as gamma and twin_critic, main and target             model. Especially, the <code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code> mechanism for balancing max entropy target is also initialized here.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For discrete SAC, it contains obs, next_obs, logit, action, reward, done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For discrete SAC, it contains the action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizers.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteSACPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#DiscreteSACPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteSACPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="sqilsacpolicy">
<h3>SQILSACPolicy<a class="headerlink" href="#sqilsacpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.SQILSACPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">SQILSACPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/sac.html#SQILSACPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SQILSACPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of continuous SAC algorithm with SQIL extension.
SAC paper link: <a class="reference external" href="https://arxiv.org/pdf/1801.01290.pdf">https://arxiv.org/pdf/1801.01290.pdf</a>
SQIL paper link: <a class="reference external" href="https://arxiv.org/abs/1905.11108">https://arxiv.org/abs/1905.11108</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SQILSACPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SQILSACPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SQILSACPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For SAC, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For SQIL + SAC, input data is composed of two parts with the same size: agent data and expert data.             Both of them are relabelled with new reward according to SQIL algorithm.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for SACPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_sac</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SQILSACPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SQILSACPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SQILSACPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For SAC, it mainly             contains three optimizers, algorithm-specific arguments such as gamma and twin_critic, main and target             model. Especially, the <code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code> mechanism for balancing max entropy target is also initialized here.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.SQILSACPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/sac.html#SQILSACPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.SQILSACPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="r2d2">
<h2>R2D2<a class="headerlink" href="#r2d2" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/r2d2.py</span></code> for more details.</p>
<section id="r2d2policy">
<h3>R2D2Policy<a class="headerlink" href="#r2d2policy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">R2D2Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of R2D2, from paper <cite>Recurrent Experience Replay in Distributed Reinforcement Learning</cite> .
R2D2 proposes that several tricks should be used to improve upon DRQN, namely some recurrent experience replay         tricks and the burn-in mechanism for off-policy training.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>r2d2</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line">to correct biased update. If True,</div>
<div class="line">priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.997,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>3,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">burnin_step</span></code></p></td>
<td><p>int</p></td>
<td><p>2</p></td>
<td><div class="line-block">
<div class="line">The timestep of burnin operation,</div>
<div class="line">which is designed to RNN hidden state</div>
<div class="line">difference caused by off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>64</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.value_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">rescale</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether use value_rescale function for</div>
<div class="line">predicted value</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>100</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>[8, 128]</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs. Besides, this policy also needs <code class="docutils literal notranslate"><span class="pre">eps</span></code> argument for             exploration, i.e., classic epsilon-greedy exploration strategy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The epsilon value for exploration.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (prev_state) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RNN’s hidden states are maintained in the policy, so we don’t need pass them into data but to reset the             hidden states with <code class="docutils literal notranslate"><span class="pre">_reset_collect</span></code> method when episode ends. Besides, the previous hidden states are             necessary for training, so we need to return them in <code class="docutils literal notranslate"><span class="pre">_process_transition</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for R2D2Policy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_r2d2</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data (trajectory for R2D2) from the replay buffer and then             returns the output result, including various training information such as loss, q value, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[Dict[int,</span> <span class="pre">Any]]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each dict element, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the time and                 batch dimension by the utility functions <code class="docutils literal notranslate"><span class="pre">self._data_preprocess_learn</span></code>.                 For R2D2, each element in list is a trajectory with the length of <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code>, and the element in                 trajectory list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">prev_state</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for R2D2Policy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_r2d2</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In R2D2, a train sample is processed transitions with unroll_len             length. This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each sample is a fixed-length                 trajectory, and each element in a sample is the similar format as input transitions, but may contain                 more data for training, such as nstep reward and value_gamma factor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For R2D2, it contains the             collect_model to balance the exploration and exploitation with epsilon-greedy sample mechanism and             maintain the hidden state of rnn. Besides, there are some initialization operations about other             algorithm-specific arguments such as burnin_step, unroll_len and nstep.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and nstep in R2D2. This             design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including some attributes and modules. For R2D2, it mainly contains             optimizer, algorithm-specific arguments such as burnin_step, value_rescale and gamma, main and target             model. Because of the use of RNN, all the models should be wrappered with <code class="docutils literal notranslate"><span class="pre">hidden_state</span></code> which needs to             be initialized with proper size.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For R2D2, it contains obs, action, prev_state, reward, and done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network given the observation                 as input. For R2D2, it contains the action and the prev_state of RNN.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._reset_collect">
<span class="sig-name descname"><span class="pre">_reset_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._reset_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._reset_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e., RNN hidden_state in R2D2) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._reset_eval">
<span class="sig-name descname"><span class="pre">_reset_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._reset_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._reset_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e., RNN hidden_state in R2D2) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._reset_learn">
<span class="sig-name descname"><span class="pre">_reset_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._reset_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._reset_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for learn mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different trajectories in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e. RNN hidden_state in R2D2) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.R2D2Policy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/r2d2.html#R2D2Policy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.R2D2Policy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about R2D2, its registered name is <code class="docutils literal notranslate"><span class="pre">drqn</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.q_learning</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="impala">
<h2>IMPALA<a class="headerlink" href="#impala" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/impala.py</span></code> for more details.</p>
<section id="impalapolicy">
<h3>IMPALAPolicy<a class="headerlink" href="#impalapolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">IMPALAPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of IMPALA algorithm. Paper link: <a class="reference external" href="https://arxiv.org/abs/1802.01561">https://arxiv.org/abs/1802.01561</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>impala</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><ol class="arabic simple" start="4">
<li></li>
</ol>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">IS_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">If True, priority</div>
<div class="line">must be True</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">unroll_len</span></code></p></td>
<td><p>int</p></td>
<td><p>32</p></td>
<td><div class="line-block">
<div class="line">trajectory length to calculate v-trace</div>
<div class="line">target</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>4</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (action logit and value) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code>                 method. The key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to add more tricks on this policy, like temperature factor in multinomial sample, you can pass             related data as extra keyword arguments of this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to unittest for IMPALAPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_impala</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs. <code class="docutils literal notranslate"><span class="pre">_forward_eval</span></code> in IMPALA often uses deterministic sample to get             actions while <code class="docutils literal notranslate"><span class="pre">_forward_collect</span></code> usually uses stochastic sample method for balance exploration and             exploitation.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to unittest for IMPALAPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_impala</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss and current learning rate.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For IMPALA, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>,                 <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">logit</span></code>, <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such                 as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to unittest for IMPALAPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_impala</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training. In IMPALA, a train sample is processed transitions with unroll_len length.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For IMPALA, it contains             the collect_model to balance the exploration and exploitation (e.g. the multinomial sample mechanism in             discrete action space), and other algorithm-specific arguments such as unroll_len.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For IMPALA, it contains the             eval model to select optimial action (e.g. greedily select action with argmax mechanism in discrete action).
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For IMPALA, it mainly             contains optimizer, algorithm-specific arguments such as loss weight and gamma, main (learn) model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For IMPALA, it contains obs, next_obs, action, reward, done, logit.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For IMPALA, it contains the action and the logit of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.IMPALAPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/impala.html#IMPALAPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.IMPALAPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about IMPALA , its registered name is <code class="docutils literal notranslate"><span class="pre">vac</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.vac</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="qmix">
<h2>QMIX<a class="headerlink" href="#qmix" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/qmix.py</span></code> for more details.</p>
<section id="qmixpolicy">
<h3>QMIXPolicy<a class="headerlink" href="#qmixpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">QMIXPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of QMIX algorithm. QMIX is a multi-agent reinforcement learning algorithm,         you can view the paper in the following link <a class="reference external" href="https://arxiv.org/abs/1803.11485">https://arxiv.org/abs/1803.11485</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>qmix</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><ol class="arabic simple" start="4">
<li></li>
</ol>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">IS_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling</div>
<div class="line">Weight to correct biased update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">IS weight</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>20</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">this args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_theta</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Target network update momentum</div>
<div class="line">parameter.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">between[0,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.discount</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.99</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">may be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs. Besides, this policy also needs <code class="docutils literal notranslate"><span class="pre">eps</span></code> argument for             exploration, i.e., classic epsilon-greedy exploration strategy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The epsilon value for exploration.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data (prev_state) for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>RNN’s hidden states are maintained in the policy, so we don’t need pass them into data but to reset the             hidden states with <code class="docutils literal notranslate"><span class="pre">_reset_collect</span></code> method when episode ends. Besides, the previous hidden states are             necessary for training, so we need to return them in <code class="docutils literal notranslate"><span class="pre">_process_transition</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for QMIXPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_qmix</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data (trajectory for QMIX) from the replay buffer and then             returns the output result, including various training information such as loss, q value, grad_norm.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[List[Dict[int,</span> <span class="pre">Any]]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each dict element, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the time and                 batch dimension by the utility functions <code class="docutils literal notranslate"><span class="pre">self._data_preprocess_learn</span></code>.                 For QMIX, each element in list is a trajectory with the length of <code class="docutils literal notranslate"><span class="pre">unroll_len</span></code>, and the element in                 trajectory list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">prev_state</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for QMIXPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_qmix</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In QMIX, a train sample is processed transitions with unroll_len             length. This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each sample is a fixed-length                 trajectory, and each element in a sample is the similar format as input transitions.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For QMIX, it contains the             collect_model to balance the exploration and exploitation with epsilon-greedy sample mechanism and             maintain the hidden state of rnn. Besides, there are some initialization operations about other             algorithm-specific arguments such as burnin_step, unroll_len and nstep.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including some attributes and modules. For QMIX, it mainly contains             optimizer, algorithm-specific arguments such as gamma, main and target model. Because of the use of RNN,             all the models should be wrappered with <code class="docutils literal notranslate"><span class="pre">hidden_state</span></code> which needs to be initialized with proper size.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For multi-agent algorithm, we often need to use <code class="docutils literal notranslate"><span class="pre">agent_num</span></code> to initialize some necessary variables.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.
- agent_num (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>): Since this is a multi-agent algorithm, we need to input the agent num.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For QMIX, it contains obs, next_obs, action, prev_state, reward, done.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, usually including <code class="docutils literal notranslate"><span class="pre">agent_obs</span></code>                 and <code class="docutils literal notranslate"><span class="pre">global_obs</span></code> in multi-agent environment like MPE and SMAC.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For QMIX, it contains the action and the prev_state of RNN.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._reset_collect">
<span class="sig-name descname"><span class="pre">_reset_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._reset_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._reset_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e., RNN hidden_state in QMIX) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._reset_eval">
<span class="sig-name descname"><span class="pre">_reset_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._reset_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._reset_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e., RNN hidden_state in QMIX) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._reset_learn">
<span class="sig-name descname"><span class="pre">_reset_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._reset_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._reset_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for learn mode when necessary, such as the hidden state of RNN or the             memory bank of some special algortihms. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different trajectories in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different hidden state in RNN.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 (i.e. RNN hidden_state in QMIX) specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target_model and optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.QMIXPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/qmix.html#QMIXPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.QMIXPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default model setting for demonstration.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): model name and mode import_names</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For QMIX, <code class="docutils literal notranslate"><span class="pre">ding.model.qmix.qmix</span></code></p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="cql">
<h2>CQL<a class="headerlink" href="#cql" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/cql.py</span></code> for more details.</p>
<section id="cqlpolicy">
<h3>CQLPolicy<a class="headerlink" href="#cqlpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.CQLPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">CQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/cql.html#CQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CQLPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of CQL algorithm for continuous control. Paper link: <a class="reference external" href="https://arxiv.org/abs/2006.04779">https://arxiv.org/abs/2006.04779</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 33%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>cql</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 10000 for</div>
<div class="line">SAC, 25000 for DDPG/</div>
<div class="line">TD3.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.policy_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for policy</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.soft_q_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for soft q</div>
<div class="line">network.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.value_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">embedding_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><div class="line-block">
<div class="line">Linear layer size for value</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to None when</div>
<div class="line">model.value_network</div>
<div class="line">is False.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_q</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for soft q</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3, when</div>
<div class="line">model.value_network</div>
<div class="line">is True.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_policy</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for policy</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to 1e-3, when</div>
<div class="line">model.value_network</div>
<div class="line">is True.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_value</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>3e-4</p></td>
<td><div class="line-block">
<div class="line">Learning rate for policy</div>
<div class="line">network.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Defalut to None when</div>
<div class="line">model.value_network</div>
<div class="line">is False.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.2</p></td>
<td><div class="line-block">
<div class="line">Entropy regularization</div>
<div class="line">coefficient.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">alpha is initiali-</div>
<div class="line">zation for auto</div>
<div class="line"><cite>alpha</cite>, when</div>
<div class="line">auto_alpha is True</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.repara_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">meterization</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>True</p></td>
<td><div class="line-block">
<div class="line">Determine whether to use</div>
<div class="line">reparameterization trick.</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to use</div>
<div class="line">auto temperature parameter</div>
<div class="line"><cite>alpha</cite>.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Temperature parameter</div>
<div class="line">determines the</div>
<div class="line">relative importance</div>
<div class="line">of the entropy term</div>
<div class="line">against the reward.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in halfcheetah env.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver</div>
<div class="line">aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CQLPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/cql.html#CQLPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CQLPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the offline dataset and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For CQL, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.CQLPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/cql.html#CQLPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.CQLPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For SAC, it mainly             contains three optimizers, algorithm-specific arguments such as gamma, min_q_weight, with_lagrange and             with_q_entropy, main and target model. Especially, the <code class="docutils literal notranslate"><span class="pre">auto_alpha</span></code> mechanism for balancing max entropy             target is also initialized here.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="discretecqlpolicy">
<h3>DiscreteCQLPolicy<a class="headerlink" href="#discretecqlpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DiscreteCQLPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteCQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/cql.html#DiscreteCQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteCQLPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of discrete CQL algorithm in discrete action space environments.
Paper link: <a class="reference external" href="https://arxiv.org/abs/2006.04779">https://arxiv.org/abs/2006.04779</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteCQLPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/cql.html#DiscreteCQLPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteCQLPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the offline dataset and then returns the output             result, including various training information such as loss, action, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For DiscreteCQL, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>,                 <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys like <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code> for nstep return computation.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteCQLPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/cql.html#DiscreteCQLPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteCQLPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For DiscreteCQL, it mainly             contains the optimizer, algorithm-specific arguments such as gamma, nstep and min_q_weight, main and             target model. This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DiscreteCQLPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/cql.html#DiscreteCQLPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DiscreteCQLPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="decisiontransformer">
<h2>DecisionTransformer<a class="headerlink" href="#decisiontransformer" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/dt.py</span></code> for more details.</p>
<section id="dtpolicy">
<h3>DTPolicy<a class="headerlink" href="#dtpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.DTPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">DTPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy class of Decision Transformer algorithm in discrete environments.
Paper link: <a class="reference external" href="https://arxiv.org/abs/2106.01345">https://arxiv.org/abs/2106.01345</a>.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance, such as interacting with envs.             Forward means that the policy gets some input data (current obs/return-to-go and historical information)             from the envs and then returns the output data, such as the action to interact with the envs.         Arguments:
- data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs and                 reward to calculate running return-to-go. The key of the dict is environment id and the value is the                 corresponding data of the env.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Decision Transformer will do different operations for different types of envs in evaluation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the offline dataset and then returns the output             result, including various training information such as loss, current learning rate.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[torch.Tensor]</span></code>): The input data used for policy forward, including a series of                 processed torch.Tensor data, i.e., timesteps, states, actions, returns_to_go, traj_mask.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For DQN, it contains the             eval model, some algorithm-specific parameters such as context_len, max_eval_ep_len, etc.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For the evaluation of complete episodes, we need to maintain some historical information for transformer             inference. These variables need to be initialized in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> and reset in <code class="docutils literal notranslate"><span class="pre">_reset_eval</span></code> when             necessary.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For Decision Transformer,             it mainly contains the optimizer, algorithm-specific arguments such as rtg_scale and lr scheduler.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.DTPolicy._reset_eval">
<span class="sig-name descname"><span class="pre">_reset_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/dt.html#DTPolicy._reset_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.DTPolicy._reset_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Reset some stateful variables for eval mode when necessary, such as the historical info of transformer             for decision transformer. If <code class="docutils literal notranslate"><span class="pre">data_id</span></code> is None, it means to reset all the stateful             varaibles. Otherwise, it will reset the stateful variables according to the <code class="docutils literal notranslate"><span class="pre">data_id</span></code>. For example,             different environments/episodes in evaluation in <code class="docutils literal notranslate"><span class="pre">data_id</span></code> will have different history.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data_id (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Optional[List[int]]</span></code>): The id of the data, which is used to reset the stateful variables                 specified by <code class="docutils literal notranslate"><span class="pre">data_id</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="pdqn">
<h2>PDQN<a class="headerlink" href="#pdqn" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/pdqn.py</span></code> for more details.</p>
<section id="pdqnpolicy">
<h3>PDQNPolicy<a class="headerlink" href="#pdqnpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PDQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of PDQN algorithm, which extends the DQN algorithm on discrete-continuous hybrid action spaces.
Paper link: <a class="reference external" href="https://arxiv.org/abs/1810.06394">https://arxiv.org/abs/1810.06394</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>pdqn</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This value is always</div>
<div class="line">False for PDQN</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line">to correct biased update. If True,</div>
<div class="line">priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.97,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>1,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>3</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_gpu</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>64</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>100</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>[8, 128]</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.noise</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_sigma</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">add noise to continuous args</div>
<div class="line">during collection</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.type</span></code></div>
</div>
</td>
<td><p>str</p></td>
<td><p>exp</p></td>
<td><div class="line-block">
<div class="line">exploration rate decay type</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Support [‘exp’,</div>
<div class="line">‘linear’].</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">start</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.95</p></td>
<td><div class="line-block">
<div class="line">start value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">end</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.05</p></td>
<td><div class="line-block">
<div class="line">end value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">decay</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>10000</p></td>
<td><div class="line-block">
<div class="line">decay length of exploration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">greater than 0. set</div>
<div class="line">decay=10000 means</div>
<div class="line">the exploration rate</div>
<div class="line">decay from start</div>
<div class="line">value to end value</div>
<div class="line">during decay length.</div>
</div>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._forward_collect">
<span class="sig-name descname"><span class="pre">_forward_collect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._forward_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._forward_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of collect mode (collecting training data by interacting with envs). Forward means             that the policy gets some necessary data (mainly observation) from the envs and then returns the output             data, such as the action to interact with the envs. Besides, this policy also needs <code class="docutils literal notranslate"><span class="pre">eps</span></code> argument for             exploration, i.e., classic epsilon-greedy exploration strategy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
<li><p>eps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>): The epsilon value for exploration.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action and                 other necessary data for learn mode defined in <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method. The key of the                 dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PDQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_pdqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._forward_eval">
<span class="sig-name descname"><span class="pre">_forward_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._forward_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._forward_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of eval mode (evaluation policy performance by interacting with envs). Forward             means that the policy gets some necessary data (mainly observation) from the envs and then returns the             action to interact with the envs.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The input data used for policy forward, including at least the obs. The                 key of the dict is environment id and the value is the corresponding data of the env.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[int,</span> <span class="pre">Any]</span></code>): The output data of policy forward, including at least the action. The                 key of the dict is the same as the input data, i.e. environment id.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PDQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_pdqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, q value, target_q_value, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For PDQN, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for PDQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_pdqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._get_train_sample">
<span class="sig-name descname"><span class="pre">_get_train_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">transitions</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._get_train_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._get_train_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>For a given trajectory (transitions, a list of transition) data, process it into a list of sample that             can be used for training directly. In PDQN, a train sample is a processed transition.             This method is usually used in collectors to execute necessary             RL data preprocessing before training, which can help learner amortize revelant time consumption.             In addition, you can also implement this method as an identity function and do the data processing             in <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code> method.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>transitions (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]</span></code>): The trajectory data (a list of transition), each element is                 the same format as the return value of <code class="docutils literal notranslate"><span class="pre">self._process_transition</span></code> method.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>samples (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>): The processed train samples, each element is the similar format                 as input transitions, but may contain more data for training, such as nstep reward and target obs.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._init_collect">
<span class="sig-name descname"><span class="pre">_init_collect</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._init_collect"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._init_collect" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the collect mode of policy, including related attributes and modules. For PDQN, it contains the             collect_model to balance the exploration and exploitation with epsilon-greedy sample mechanism and             continuous action mechanism, besides, other algorithm-specific arguments such as unroll_len and nstep are             also initialized here.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">collect</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_collect</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_collect_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._collect_attr1</span></code>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some variables need to initialize independently in different modes, such as gamma and nstep in PDQN. This             design is for the convenience of parallel execution of different policy modes.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._init_eval">
<span class="sig-name descname"><span class="pre">_init_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._init_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._init_eval" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the eval mode of policy, including related attributes and modules. For PDQN, it contains the             eval model to greedily select action with argmax q_value mechanism.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">eval</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_eval</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_eval_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._eval_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For PDQN, it mainly             contains two optimizers, algorithm-specific arguments such as nstep and gamma, main and target model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._load_state_dict_learn">
<span class="sig-name descname"><span class="pre">_load_state_dict_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._load_state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._load_state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Load the state_dict variable into policy learn mode.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of policy learn state saved before.</p></li>
</ul>
</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to only load some parts of model, you can simply set the <code class="docutils literal notranslate"><span class="pre">strict</span></code> argument in             load_state_dict to <code class="docutils literal notranslate"><span class="pre">False</span></code>, or refer to <code class="docutils literal notranslate"><span class="pre">ding.torch_utils.checkpoint_helper</span></code> for more             complicated operation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._process_transition">
<span class="sig-name descname"><span class="pre">_process_transition</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_output</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.namedtuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._process_transition"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._process_transition" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Process and pack one timestep transition data into a dict, which can be directly used for training and             saved in replay buffer. For PDQN, it contains obs, next_obs, action, reward, done and logit.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): The env observation of current timestep, such as stacked 2D image in Atari.</p></li>
<li><p>policy_output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The output of the policy network with the observation                 as input. For PDQN, it contains the hybrid action and the logit (discrete part q_value) of the action.</p></li>
<li><p>timestep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">namedtuple</span></code>): The execution result namedtuple returned by the environment step method,                 except all the elements have been transformed into tensor data. Usually, it contains the next obs,                 reward, done, info, etc.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>transition (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The processed transition data of the current timestep.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy._state_dict_learn">
<span class="sig-name descname"><span class="pre">_state_dict_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy._state_dict_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy._state_dict_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the state_dict of learn mode, usually including model, target model, discrete part optimizer, and             continuous part optimizer.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>state_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): the dict of current policy learn state, for saving and restoring.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PDQNPolicy.default_model">
<span class="sig-name descname"><span class="pre">default_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/pdqn.html#PDQNPolicy.default_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PDQNPolicy.default_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return this algorithm default neural network model setting for demonstration. <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method will             automatically call this method to get the default model setting and create model.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>model_info (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Tuple[str,</span> <span class="pre">List[str]]</span></code>): The registered model name and model’s import_names.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The user can define and use customized network model but must obey the same inferface definition indicated             by import_names path. For example about PDQN, its registered name is <code class="docutils literal notranslate"><span class="pre">pdqn</span></code> and the import_names is             <code class="docutils literal notranslate"><span class="pre">ding.model.template.pdqn</span></code>.</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="mdqn">
<h2>MDQN<a class="headerlink" href="#mdqn" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/mdqn.py</span></code> for more details.</p>
<section id="mdqnpolicy">
<h3>MDQNPolicy<a class="headerlink" href="#mdqnpolicy" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.MDQNPolicy">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">MDQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/mdqn.html#MDQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.MDQNPolicy" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of Munchausen DQN algorithm, extended by auxiliary objectives.
Paper link: <a class="reference external" href="https://arxiv.org/abs/2007.14430">https://arxiv.org/abs/2007.14430</a>.</p>
</dd>
<dt>Config:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 19%" />
<col style="width: 7%" />
<col style="width: 13%" />
<col style="width: 37%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">type</span></code></p></td>
<td><p>str</p></td>
<td><p>mdqn</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer to</div>
<div class="line">registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cuda</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This arg can be diff-</div>
<div class="line">erent from modes</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">on_policy</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether the RL algorithm is on-policy</div>
<div class="line">or off-policy</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">priority</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use priority(PER)</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Priority sample,</div>
<div class="line">update priority</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">priority_IS</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_weight</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether use Importance Sampling Weight</div>
<div class="line">to correct biased update. If True,</div>
<div class="line">priority must be True.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">discount_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">factor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.97,
[0.95, 0.999]</p></td>
<td><div class="line-block">
<div class="line">Reward’s future discount factor, aka.</div>
<div class="line">gamma</div>
</div>
</td>
<td><div class="line-block">
<div class="line">May be 1 when sparse</div>
<div class="line">reward env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nstep</span></code></p></td>
<td><p>int</p></td>
<td><p>1,
[3, 5]</p></td>
<td><div class="line-block">
<div class="line">N-step reward discount sum for target</div>
<div class="line">q_value estimation</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.update</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">per_collect</span></code></div>
</div>
<div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">_gpu</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">How many updates(iterations) to train</div>
<div class="line">after collector’s one collection. Only</div>
<div class="line">valid in serial training</div>
</div>
</td>
<td><div class="line-block">
<div class="line">This args can be vary</div>
<div class="line">from envs. Bigger val</div>
<div class="line">means more off-policy</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.batch_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">size</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>32</p></td>
<td><div class="line-block">
<div class="line">The number of samples of an iteration</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">Gradient step length of an iteration.</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.target_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>2000</p></td>
<td><div class="line-block">
<div class="line">Frequence of target network update.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Hard(assign) update</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.ignore_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether ignore done for target value</div>
<div class="line">calculation.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Enable it for some</div>
<div class="line">fake termination env</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">collect.n_sample</span></code></p></td>
<td><p>int</p></td>
<td><p>4</p></td>
<td><div class="line-block">
<div class="line">The number of training samples of a</div>
<div class="line">call of collector.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">It varies from</div>
<div class="line">different envs</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.unroll</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_len</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><div class="line-block">
<div class="line">unroll length of an iteration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">In RNN, unroll_len&gt;1</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.type</span></code></div>
</div>
</td>
<td><p>str</p></td>
<td><p>exp</p></td>
<td><div class="line-block">
<div class="line">exploration rate decay type</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Support [‘exp’,</div>
<div class="line">‘linear’].</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">start</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.01</p></td>
<td><div class="line-block">
<div class="line">start value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">end</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.001</p></td>
<td><div class="line-block">
<div class="line">end value of exploration rate</div>
</div>
</td>
<td><div class="line-block">
<div class="line">[0,1]</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">other.eps.</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">decay</span></code></div>
</div>
</td>
<td><p>int</p></td>
<td><p>250000</p></td>
<td><div class="line-block">
<div class="line">decay length of exploration</div>
</div>
</td>
<td><div class="line-block">
<div class="line">greater than 0. set</div>
<div class="line">decay=250000 means</div>
<div class="line">the exploration rate</div>
<div class="line">decay from start</div>
<div class="line">value to end value</div>
<div class="line">during decay length.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">entropy_tau</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.003</p></td>
<td><div class="line-block">
<div class="line">the ration of entropy in TD loss</div>
</div>
</td>
<td></td>
</tr>
<tr class="row-odd"><td><p>21</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">alpha</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.9</p></td>
<td><div class="line-block">
<div class="line">the ration of Munchausen term to the</div>
<div class="line">TD loss</div>
</div>
</td>
<td></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.MDQNPolicy._forward_learn">
<span class="sig-name descname"><span class="pre">_forward_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/mdqn.html#MDQNPolicy._forward_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.MDQNPolicy._forward_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy forward function of learn mode (training policy and updating parameters). Forward means             that the policy inputs some training batch data from the replay buffer and then returns the output             result, including various training information such as loss, action_gap, clip_frac, priority.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Dict[int,</span> <span class="pre">Any]]</span></code>): The input data used for policy forward, including a batch of                 training samples. For each element in list, the key of the dict is the name of data items and the                 value is the corresponding data. Usually, the value is torch.Tensor or np.ndarray or there dict/list                 combinations. In the <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, data often need to first be stacked in the batch                 dimension by some utility functions such as <code class="docutils literal notranslate"><span class="pre">default_preprocess_learn</span></code>.                 For MDQN, each element in list is a dict containing at least the following keys: <code class="docutils literal notranslate"><span class="pre">obs</span></code>, <code class="docutils literal notranslate"><span class="pre">action</span></code>,                 <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">next_obs</span></code>, <code class="docutils literal notranslate"><span class="pre">done</span></code>. Sometimes, it also contains other keys such as <code class="docutils literal notranslate"><span class="pre">weight</span></code>                 and <code class="docutils literal notranslate"><span class="pre">value_gamma</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>info_dict (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Any]</span></code>): The information dict that indicated training result, which will be                 recorded in text log and tensorboard, values must be python scalar or a list of scalars. For the                 detailed definition of the dict, refer to the code of <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input value can be torch.Tensor or dict/list combinations and current policy supports all of them.             For the data type that not supported, the main reason is that the corresponding model does not support it.             You can implement you own model rather than use the default model. For more information, please raise an             issue in GitHub repo and we will continue to follow up.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more detailed examples, please refer to our unittest for MDQNPolicy: <code class="docutils literal notranslate"><span class="pre">ding.policy.tests.test_mdqn</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.MDQNPolicy._init_learn">
<span class="sig-name descname"><span class="pre">_init_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/policy/mdqn.html#MDQNPolicy._init_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.MDQNPolicy._init_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Initialize the learn mode of policy, including related attributes and modules. For MDQN, it contains             optimizer, algorithm-specific arguments such as entropy_tau, m_alpha and nstep, main and target model.
This method will be called in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if <code class="docutils literal notranslate"><span class="pre">learn</span></code> field is in <code class="docutils literal notranslate"><span class="pre">enable_field</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be saved and loaded, please refer to the <code class="docutils literal notranslate"><span class="pre">_state_dict_learn</span></code>             and <code class="docutils literal notranslate"><span class="pre">_load_state_dict_learn</span></code> methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the member variables that need to be monitored, please refer to the <code class="docutils literal notranslate"><span class="pre">_monitor_vars_learn</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to set some spacial member variables in <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> method, you’d better name them             with prefix <code class="docutils literal notranslate"><span class="pre">_learn_</span></code> to avoid conflict with other modes, such as <code class="docutils literal notranslate"><span class="pre">self._learn_attr1</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.MDQNPolicy._monitor_vars_learn">
<span class="sig-name descname"><span class="pre">_monitor_vars_learn</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/mdqn.html#MDQNPolicy._monitor_vars_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.MDQNPolicy._monitor_vars_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Return the necessary keys for logging the return dict of <code class="docutils literal notranslate"><span class="pre">self._forward_learn</span></code>. The logger module, such             as text logger, tensorboard logger, will use these keys to save the corresponding data.</p>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>necessary_keys (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[str]</span></code>): The list of the necessary keys to be logged.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="policy-factory">
<h2>Policy Factory<a class="headerlink" href="#policy-factory" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/policy_factory.py</span></code> for more details.</p>
<section id="policyfactory">
<h3>PolicyFactory<a class="headerlink" href="#policyfactory" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="ding.policy.PolicyFactory">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">PolicyFactory</span></span><a class="reference internal" href="../_modules/ding/policy/policy_factory.html#PolicyFactory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PolicyFactory" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Policy factory class, used to generate different policies for general purpose. Such as random action policy,         which is used for initial sample collecting for better exploration when <code class="docutils literal notranslate"><span class="pre">random_collect_size</span></code> &gt; 0.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">get_random_policy</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="ding.policy.PolicyFactory.get_random_policy">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">get_random_policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><span class="pre">Policy.collect_mode</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">gym.spaces.Space</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><span class="pre">Policy.collect_mode</span></a></span></span><a class="reference internal" href="../_modules/ding/policy/policy_factory.html#PolicyFactory.get_random_policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.PolicyFactory.get_random_policy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>According to the given action space, define the forward function of the random policy, then pack it with             other interfaces of the given policy, and return the final collect mode interfaces of policy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>policy (<a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.collect_mode</span></code></a>): The collect mode interfaces of the policy.</p></li>
<li><p>action_space (<code class="xref py py-obj docutils literal notranslate"><span class="pre">gym.spaces.Space</span></code>): The action space of the environment, gym-style.</p></li>
<li><p>forward_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): It action space is too complex, you can define your own forward function                 and pass it to this function, note you should set <code class="docutils literal notranslate"><span class="pre">action_space</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>random_policy (<a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.collect_mode</span></code></a>): The collect mode intefaces of the random policy.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="get-random-policy">
<h3>get_random_policy<a class="headerlink" href="#get-random-policy" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.get_random_policy">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">get_random_policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Policy.collect_mode</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">BaseEnvManager</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Policy.collect_mode</span></span></span><a class="reference internal" href="../_modules/ding/policy/policy_factory.html#get_random_policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.get_random_policy" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The entry function to get the corresponding random policy. If a policy needs special data items in a         transition, then return itself, otherwise, we will use <code class="docutils literal notranslate"><span class="pre">PolicyFactory</span></code> to return a general random policy.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>cfg (<code class="xref py py-obj docutils literal notranslate"><span class="pre">EasyDict</span></code>): The EasyDict-type dict configuration.</p></li>
<li><p>policy (<a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.collect_mode</span></code></a>): The collect mode interfaces of the policy.</p></li>
<li><p>env (<code class="xref py py-obj docutils literal notranslate"><span class="pre">BaseEnvManager</span></code>): The env manager instance, which is used to get the action space for random             action generation.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>random_policy (<a class="reference internal" href="#ding.policy.Policy.collect_mode" title="ding.policy.Policy.collect_mode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Policy.collect_mode</span></code></a>): The collect mode intefaces of the random policy.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="common-utilities">
<h2>Common Utilities<a class="headerlink" href="#common-utilities" title="Permalink to this headline">¶</a></h2>
<p>Please refer to <code class="docutils literal notranslate"><span class="pre">ding/policy/common_utils.py</span></code> for more details.</p>
<section id="default-preprocess-learn">
<h3>default_preprocess_learn<a class="headerlink" href="#default-preprocess-learn" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.default_preprocess_learn">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">default_preprocess_learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_priority_IS_weight</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_priority</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_nstep</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_done</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/policy/common_utils.html#default_preprocess_learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.default_preprocess_learn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Default data pre-processing in policy’s <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> method, including stacking batch data, preprocess         ignore done, nstep and priority IS weight.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List[Any]</span></code>): The list of a training batch samples, each sample is a dict of PyTorch Tensor.</p></li>
<li><p>use_priority_IS_weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use priority IS weight correction, if True, this function             will set the weight of each sample to the priority IS weight.</p></li>
<li><p>use_priority (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use priority, if True, this function will set the priority IS weight.</p></li>
<li><p>use_nstep (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use nstep TD error, if True, this function will reshape the reward.</p></li>
<li><p>ignore_done (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to ignore done, if True, this function will set the done to 0.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): The preprocessed dict data whose values can be directly used for             the following model forward and loss computation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="single-env-forward-wrapper">
<h3>single_env_forward_wrapper<a class="headerlink" href="#single-env-forward-wrapper" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.single_env_forward_wrapper">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">single_env_forward_wrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="reference internal" href="../_modules/ding/policy/common_utils.html#single_env_forward_wrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.single_env_forward_wrapper" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Wrap policy to support gym-style interaction between policy and single environment.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>forward_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The original forward function of policy.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>wrapped_forward_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The wrapped forward function of policy.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">DQNPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forward_fn</span> <span class="o">=</span> <span class="n">single_env_forward_wrapper</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">eval_mode</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">forward_fn</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
<section id="single-env-forward-wrapper-ttorch">
<h3>single_env_forward_wrapper_ttorch<a class="headerlink" href="#single-env-forward-wrapper-ttorch" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="ding.policy.single_env_forward_wrapper_ttorch">
<span class="sig-prename descclassname"><span class="pre">ding.policy.</span></span><span class="sig-name descname"><span class="pre">single_env_forward_wrapper_ttorch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_fn</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cuda</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="reference internal" href="../_modules/ding/policy/common_utils.html#single_env_forward_wrapper_ttorch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ding.policy.single_env_forward_wrapper_ttorch" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Wrap policy to support gym-style interaction between policy and single environment for treetensor (ttorch) data.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>forward_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The original forward function of policy.</p></li>
<li><p>cuda (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>): Whether to use cuda in policy, if True, this function will move the input data to cuda.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>wrapped_forward_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>): The wrapped forward function of policy.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">PPOFPolicy</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forward_fn</span> <span class="o">=</span> <span class="n">single_env_forward_wrapper_ttorch</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">eval</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="n">forward_fn</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">next_obs</span><span class="p">,</span> <span class="n">rew</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="rl_utils.html" class="btn btn-neutral float-right" title="ding.rl_utils" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="model.html" class="btn btn-neutral" title="ding.model" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">ding.policy</a><ul>
<li><a class="reference internal" href="#base-policy">Base Policy</a><ul>
<li><a class="reference internal" href="#policy">Policy</a></li>
<li><a class="reference internal" href="#commandmodepolicy">CommandModePolicy</a></li>
<li><a class="reference internal" href="#create-policy">create_policy</a></li>
<li><a class="reference internal" href="#get-policy-cls">get_policy_cls</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dqn">DQN</a><ul>
<li><a class="reference internal" href="#dqnpolicy">DQNPolicy</a></li>
<li><a class="reference internal" href="#dqnstdimpolicy">DQNSTDIMPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ppo">PPO</a><ul>
<li><a class="reference internal" href="#ppopolicy">PPOPolicy</a></li>
<li><a class="reference internal" href="#ppopgpolicy">PPOPGPolicy</a></li>
<li><a class="reference internal" href="#ppooffpolicy">PPOOffPolicy</a></li>
<li><a class="reference internal" href="#ppostdimpolicy">PPOSTDIMPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#bc">BC</a><ul>
<li><a class="reference internal" href="#behaviourcloningpolicy">BehaviourCloningPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ddpg">DDPG</a><ul>
<li><a class="reference internal" href="#ddpgpolicy">DDPGPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#td3">TD3</a><ul>
<li><a class="reference internal" href="#td3policy">TD3Policy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sac">SAC</a><ul>
<li><a class="reference internal" href="#sacpolicy">SACPolicy</a></li>
<li><a class="reference internal" href="#discretesacpolicy">DiscreteSACPolicy</a></li>
<li><a class="reference internal" href="#sqilsacpolicy">SQILSACPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#r2d2">R2D2</a><ul>
<li><a class="reference internal" href="#r2d2policy">R2D2Policy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#impala">IMPALA</a><ul>
<li><a class="reference internal" href="#impalapolicy">IMPALAPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#qmix">QMIX</a><ul>
<li><a class="reference internal" href="#qmixpolicy">QMIXPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cql">CQL</a><ul>
<li><a class="reference internal" href="#cqlpolicy">CQLPolicy</a></li>
<li><a class="reference internal" href="#discretecqlpolicy">DiscreteCQLPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decisiontransformer">DecisionTransformer</a><ul>
<li><a class="reference internal" href="#dtpolicy">DTPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pdqn">PDQN</a><ul>
<li><a class="reference internal" href="#pdqnpolicy">PDQNPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mdqn">MDQN</a><ul>
<li><a class="reference internal" href="#mdqnpolicy">MDQNPolicy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#policy-factory">Policy Factory</a><ul>
<li><a class="reference internal" href="#policyfactory">PolicyFactory</a></li>
<li><a class="reference internal" href="#get-random-policy">get_random_policy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#common-utilities">Common Utilities</a><ul>
<li><a class="reference internal" href="#default-preprocess-learn">default_preprocess_learn</a></li>
<li><a class="reference internal" href="#single-env-forward-wrapper">single_env_forward_wrapper</a></li>
<li><a class="reference internal" href="#single-env-forward-wrapper-ttorch">single_env_forward_wrapper_ttorch</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model-Based Reinforcement Learning &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Imitation Learning" href="imitation_learning.html" />
  <link rel="prev" title="RL Algorithm Taxonomy" href="index.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">RL Algorithm Taxonomy</a> &gt;</li>
        
      <li>Model-Based Reinforcement Learning</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/02_algo/model_based_rl.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="model-based-reinforcement-learning">
<h1>Model-Based Reinforcement Learning<a class="headerlink" href="#model-based-reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<p>Model-Based Reinforcement Learning (Model-Based RL) is an important branch of reinforcement learning. The agent learns a dynamics model by interacting with the environment and then uses the model to generate data to optimize policy or use the model for planning.
The Model-Based RL method first learns a dynamics model from the data obtained by interacting with the environment and then uses the dynamics model to generate a large number of simulated samples. In this way, the number of interactions with the real environment will be reduced, or in other words, the sample efficiency can be greatly improved.</p>
<a class="reference internal image-reference" href="../_images/model-based_rl.png"><img alt="../_images/model-based_rl.png" class="align-center" src="../_images/model-based_rl.png" style="width: 958.1px; height: 400.40000000000003px;" /></a>
<p>The environment model can generally be abstracted mathematically into a state transition function and a reward function.
In the ideal case, the agent does not need to interact with the real environment anymore after learning the dynamics model. The agent can now query the dynamics model to produce simulated samples, by which the cumulative discount reward can be maximized to obtain the optimal policy.</p>
<section id="problem-definition-and-research-motivation">
<h2>Problem Definition and Research Motivation<a class="headerlink" href="#problem-definition-and-research-motivation" title="Permalink to this headline">¶</a></h2>
<p>In general, the problems of Model-Based RL research can be divided into two categories: how to learn an accurate dynamics model, and how to use the dynamics model for policy optimization.</p>
<p><strong>How to build an accurate environment model?</strong></p>
<p>Model learning mainly emphasizes the process of building an environment model by the Model-Based RL algorithm. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://worldmodels.github.io/">World Model</a> <a class="footnote-reference brackets" href="#id21" id="id1">3</a> proposes an environment model based on unsupervised learning and uses this model to transfer tasks from simulation to reality.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1707.06203">I2A</a> <a class="footnote-reference brackets" href="#id22" id="id2">4</a> proposes an imagination-augmented-based model structure, based on which the future trajectory is predicted, and the trajectory information is encoded to assist policy learning.</p></li>
</ul>
<p>But Model-Based RL also has several problems in the model learning part, for example,</p>
<ul class="simple">
<li><p>There will be errors in the dynamics model, and with the iterative interaction between the agent and the dynamics model, the error induced by the model will compound over time, making it difficult for the algorithm to converge to the optimal solution.</p></li>
<li><p>The environment model lacks generality, and every time a problem is changed, the model must be re-modeled.</p></li>
</ul>
</div></blockquote>
<p><strong>How to use the environment model for policy optimization?</strong></p>
<p>Model utilization mainly emphasizes that Model-Based RL algorithms use dynamics models to assist policy learning, such as model-based planning or model-based policy learning.</p>
<blockquote>
<div><ul class="simple">
<li><p>Both <a class="reference external" href="https://arxiv.org/abs/1705.08439">ExIt</a> <a class="footnote-reference brackets" href="#id23" id="id3">5</a> and <a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a> <a class="footnote-reference brackets" href="#id24" id="id4">6</a> are based on expert iteration and Monte Carlo tree search methods to learn strategies.</p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=H1exf64KwH">POPLIN</a> <a class="footnote-reference brackets" href="#id25" id="id5">7</a> does online planning based on the environment model, and proposes optimization ideas for action space and parameter space respectively.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2010.04893">M2AC</a> <a class="footnote-reference brackets" href="#id26" id="id6">8</a> proposes a mask mechanism based on model uncertainty, which enhances policy improvement.</p></li>
</ul>
</div></blockquote>
</section>
<section id="research-direction">
<h2>Research Direction<a class="headerlink" href="#research-direction" title="Permalink to this headline">¶</a></h2>
<p>The papers of Model-Based RL in recent years have been sorted out and summarized in <a class="reference external" href="https://github.com/opendilab/awesome-model-based-RL">awesome-model-based-RL</a> <a class="footnote-reference brackets" href="#id19" id="id7">1</a>.
One of the most classic Model-Based RL algorithms is Dyna-style reinforcement learning, which is a type of algorithm that combines Model-Based RL and Model-Free RL.
In addition to the classic Dyna-style reinforcement learning, there are roughly the following categories of model-based reinforcement learning:</p>
<ol class="arabic simple">
<li><p>Model-Based Planning Algorithms</p></li>
<li><p>Model-Based Value Extension Reinforcement Learning</p></li>
<li><p>Policy Optimization Combined with Model Gradient Backhaul</p></li>
</ol>
<section id="model-based-planning-algorithms">
<h3>Model-Based Planning Algorithms<a class="headerlink" href="#model-based-planning-algorithms" title="Permalink to this headline">¶</a></h3>
<p>After learning the dynamics model of the environment, the model can be directly used for planning. At this time, reinforcement learning can be transformed into an optimal control problem: the optimal strategy can be obtained through the planning algorithm, and the planning algorithm can also be used to generate better samples to assist learning.
The most common of this type of algorithms is the Cross Entropy Method (CEM). Its idea is to assume that the action sequence obeys a certain prior distribution, sample actions to obtain trajectories, and select a good trajectory to update the prior distribution a posteriori.</p>
<p>The model-based planning algorithm is roughly divided into three steps in each iteration:</p>
<ul class="simple">
<li><p>In the first step, after performing an action, predict the next state according to the environment dynamics model.</p></li>
<li><p>The second step is to use algorithms such as CEM to solve the action sequence.</p></li>
<li><p>In the third step, perform the first action solved in the second step, and so on.</p></li>
</ul>
<p>Typical algorithms of this type are <a class="reference external" href="https://dspace.mit.edu/handle/1721.1/28914">RS</a> <a class="footnote-reference brackets" href="#id27" id="id8">9</a>, <a class="reference external" href="https://arxiv.org/abs/1805.12114">PETS</a> <a class="footnote-reference brackets" href="#id28" id="id9">10</a>, <a class="reference external" href="https://openreview.net/forum?id=H1exf64KwH">POPLIN</a> <a class="footnote-reference brackets" href="#id25" id="id11">7</a>.
However, when solving high-dimensional control tasks, the difficulty of planning and the required computation will increase significantly, and the planning effect will become worse, so it is suitable for simple models with low action dimensions.</p>
</section>
<section id="model-based-value-extension-reinforcement-learning">
<h3>Model-Based Value Extension Reinforcement Learning<a class="headerlink" href="#model-based-value-extension-reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<p>The model-based planning algorithm inputs a state every time, and needs to plan again to obtain the output action, while a trained strategy directly maps the state to an action, and the trained strategy is faster than the planning algorithm in practical applications.
In the combined mode of Model-Based and Model-Free, the model error will reduce the performance of the entire algorithm.
<a class="reference external" href="https://arxiv.org/abs/1803.00101">MVE</a> <a class="footnote-reference brackets" href="#id29" id="id12">11</a> estimates the value function by using the environment model rollout to generate a fixed number of H-step trajectories for Model-Based Value Expansion.
Therefore, the estimation of the Q value integrates the short-term prediction based on the environmental dynamics model and the long-term prediction based on the target Q value network. The number of steps H limits the accumulation of compound errors and improves the accuracy of the Q value.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1807.01675">STEVE</a> <a class="footnote-reference brackets" href="#id30" id="id13">12</a> pointed out that MVE needs to rely on the adjustment of the number of steps H of the rollout, that is, in a complex environment, if the number of steps in the model is too large, a large error will be introduced, while in a simple environment, if the number of steps is too small, it will reduce the estimation accuracy of the Q value.
Therefore, STEVE deploys different specific steps in different environments, calculates the uncertainty of each step, dynamically adjusts and integrates the weight of the Q value between different steps, so that the Q value prediction under each environmental task is more accurate.</p>
</section>
<section id="policy-optimization-combined-with-model-gradient-backhaul">
<h3>Policy Optimization Combined with Model Gradient Backhaul<a class="headerlink" href="#policy-optimization-combined-with-model-gradient-backhaul" title="Permalink to this headline">¶</a></h3>
<p>In addition to using the virtual expansion of the model to generate data, if the model is a neural network or other differentiable functions, the differentiable characteristics of the model can also be used to directly assist the learning of the strategy. This method further utilizes the model.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1510.09142">SVG</a> <a class="footnote-reference brackets" href="#id31" id="id14">13</a> uses real samples to fit the model, and optimizes the value function by using the differentiability of the model, that is, using the chain rule and the differentiability of the model to directly derive the value function, and use the gradient ascent method to optimize the value function and learn the strategy.
Only real samples are used in the optimization process, and the model is not used to generate virtual data.
The advantage of this is that it can alleviate the impact of inaccurate models, but at the same time, because the model is not used to generate dummy data, the sample efficiency has not been greatly improved.</p>
<p>In addition to using the gradient of the model, <a class="reference external" href="https://arxiv.org/abs/2005.08068">MAAC</a> <a class="footnote-reference brackets" href="#id32" id="id15">14</a> uses the Q-value function of H-step bootstrapping as the objective function of reinforcement learning.
At the same time, the data in the replay buffer includes both the data interacting with the real environment and the data of the virtual expansion of the model. The hyperparameter H can make the objective function trade-off between the accuracy of the model and the accuracy of the Q-value function.
Calculating gradients with backpropagation using model differentiability may encounter a class of problems that exist in deep learning, gradient vanishing and gradient exploding.
The Terminal Q-Function is used in MAAC to alleviate this problem. SVG <a class="footnote-reference brackets" href="#id31" id="id16">13</a> and <a class="reference external" href="https://arxiv.org/abs/1912.01603">Dreamer</a> <a class="footnote-reference brackets" href="#id33" id="id17">15</a> are implemented using gradient clipping tricks.
In addition, using the differentiability of the model may also fall into the problem of local optima during gradient optimization. <a class="footnote-reference brackets" href="#id20" id="id18">2</a></p>
</section>
</section>
<section id="future-study">
<h2>Future Study<a class="headerlink" href="#future-study" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Model-based reinforcement learning has high sample efficiency, but the training process of environmental models is often time-intensive, so “how to improve the learning efficiency of the model” is very necessary.</p></li>
<li><p>In addition, due to the lack of generality of the environment model, it is often necessary to re-model every time a problem is changed. In order to solve the problem of model generalization between different tasks, “how to introduce the ideas and techniques of transfer learning and meta-learning into model-based reinforcement learning” is also a very important research question.</p></li>
<li><p>Model-based reinforcement learning modeling and decision-making on high-dimensional image observations, as well as model-based reinforcement learning combined with Offline RL, will be sufficient conditions for future reinforcement learning to lead to Sim2Real.</p></li>
</ol>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id7">1</a></span></dt>
<dd><p>Repo: awesome-model-based-RL. <a class="reference external" href="https://github.com/opendilab/awesome-model-based-RL">https://github.com/opendilab/awesome-model-based-RL</a></p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id18">2</a></span></dt>
<dd><p>Sun S, Lan X, Zhang H, Zheng N. Model-Based Reinforcement Learning in Robotics: A Survey[J]. Pattern Recognition and Artificial Intelligence, 2022, 35(01): 1-16. DOI: 10.16451/j.cnki.issn1003-6059.202201001.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>Ha D, Schmidhuber J. World models[J]. arXiv preprint arXiv:1803.10122, 2018.</p>
</dd>
<dt class="label" id="id22"><span class="brackets"><a class="fn-backref" href="#id2">4</a></span></dt>
<dd><p>Racanière S, Weber T, Reichert D, et al. Imagination-augmented agents for deep reinforcement learning[J]. Advances in neural information processing systems, 2017, 30.</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id3">5</a></span></dt>
<dd><p>Anthony T, Tian Z, Barber D. Thinking fast and slow with deep learning and tree search[J]. Advances in Neural Information Processing Systems, 2017, 30.</p>
</dd>
<dt class="label" id="id24"><span class="brackets"><a class="fn-backref" href="#id4">6</a></span></dt>
<dd><p>Silver D, Hubert T, Schrittwieser J, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm[J]. arXiv preprint arXiv:1712.01815, 2017.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">7</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id11">2</a>)</span></dt>
<dd><p>Wang T, Ba J. Exploring Model-based Planning with Policy Networks[C]//International Conference on Learning Representations. 2019.</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p>Pan F, He J, Tu D, et al. Trust the model when it is confident: Masked model-based actor-critic[J]. Advances in neural information processing systems, 2020, 33: 10537-10546.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>Richards A G. Robust constrained model predictive control[D]. Massachusetts Institute of Technology, 2005.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Chua K, Calandra R, McAllister R, et al. Deep reinforcement learning in a handful of trials using probabilistic dynamics models[J]. Advances in neural information processing systems, 2018, 31.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id12">11</a></span></dt>
<dd><p>Feinberg V, Wan A, Stoica I, et al. Model-based value estimation for efficient model-free reinforcement learning[J]. arXiv preprint arXiv:1803.00101, 2018.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id13">12</a></span></dt>
<dd><p>Buckman J, Hafner D, Tucker G, et al. Sample-efficient reinforcement learning with stochastic ensemble value expansion[J]. Advances in neural information processing systems, 2018, 31.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">13</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Heess N, Wayne G, Silver D, et al. Learning continuous control policies by stochastic value gradients[J]. Advances in neural information processing systems, 2015, 28.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id15">14</a></span></dt>
<dd><p>Clavera I, Fu V, Abbeel P. Model-augmented actor-critic: Backpropagating through paths[J]. arXiv preprint arXiv:2005.08068, 2020.</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id17">15</a></span></dt>
<dd><p>Hafner D, Lillicrap T, Ba J, et al. Dream to control: Learning behaviors by latent imagination[J]. arXiv preprint arXiv:1912.01603, 2019.</p>
</dd>
</dl>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="imitation_learning.html" class="btn btn-neutral float-right" title="Imitation Learning" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="index.html" class="btn btn-neutral" title="RL Algorithm Taxonomy" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Model-Based Reinforcement Learning</a><ul>
<li><a class="reference internal" href="#problem-definition-and-research-motivation">Problem Definition and Research Motivation</a></li>
<li><a class="reference internal" href="#research-direction">Research Direction</a><ul>
<li><a class="reference internal" href="#model-based-planning-algorithms">Model-Based Planning Algorithms</a></li>
<li><a class="reference internal" href="#model-based-value-extension-reinforcement-learning">Model-Based Value Extension Reinforcement Learning</a></li>
<li><a class="reference internal" href="#policy-optimization-combined-with-model-gradient-backhaul">Policy Optimization Combined with Model Gradient Backhaul</a></li>
</ul>
</li>
<li><a class="reference internal" href="#future-study">Future Study</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Offline Reinforcement Learning &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Distributed Reinforcement Learning" href="distributed_rl.html" />
  <link rel="prev" title="Multi-Agent Reinforcement Learning" href="multi_agent_cooperation_rl.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../12_policies/index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">RL Algorithm Taxonomy</a> &gt;</li>
        
      <li>Offline Reinforcement Learning</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/02_algo/offline_rl.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="offline-reinforcement-learning">
<h1>Offline Reinforcement Learning<a class="headerlink" href="#offline-reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<section id="problem-definition-and-motivation">
<h2>Problem Definition and Motivation<a class="headerlink" href="#problem-definition-and-motivation" title="Permalink to this headline">¶</a></h2>
<p>Offline Reinforcement Learning(RL), also known as Batch Reinforcement Learning, is a variant of RL that effectively leverages large, previously collected datasets for large-scale real-world applications.
The use of static datasets means that during the training process of the agent, offline RL does not perform any form of online interaction and exploration, which is also the most significant difference from online reinforcement learning methods.
For convenience, we refer to non-offline reinforcement learning, including both on-policy and off-policy RL, as online reinforcement learning (Online RL) in the following sections.</p>
<a class="reference internal image-reference" href="../_images/offline_no_words.png"><img alt="../_images/offline_no_words.png" class="align-center" src="../_images/offline_no_words.png" style="width: 1376.5px; height: 385.5px;" /></a>
<p>In the figure, (a) stands for On-policy RL, where the agent uses the current policy <span class="math notranslate nohighlight">\(\pi_k\)</span> to interact with the environment. Only data generated by currently learned policy can be used to update the network.
(b) describes off-policy RL, which stores data of all historical policies in the experience buffer <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> when interacting with the environment. In other words, <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> contains data collected with policies <span class="math notranslate nohighlight">\(\pi_0, \pi_1, ..., \pi_k\)</span>, and all of this will be used to update the network <span class="math notranslate nohighlight">\(\pi_{k+ 1}\)</span>.
For offline RL in (c), the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is collected from some (possibly unknown) behavioral policy <span class="math notranslate nohighlight">\(\pi_{\beta}\)</span> in advance, and will not change during training. The training process does not interact with the MDP at all, and the policy is only deployed after being fully trained.</p>
<p><strong>Why Study Offline RL?</strong></p>
<p>Offline RL has become a hot research topic recently, and the reasons can be attributed to two folds:</p>
<p>The first one is the advantage of offline RL itself. Deep reinforcement learning has achieved great success in simulation tasks such as games, and by effectively interacting with the environment, we can obtain agents with outstanding performance.
However, it is usually too expensive to explore the environment and collect large-scale data for training repeatedly in real-world tasks. Especially it can be dangerous in environments such as autonomous driving and robotic operations.
In contrast, offline RL studies how to learn an optimal policy from a fixed dataset, which can significantly mitigate potential risks and costs by not requiring any additional exploration.</p>
<p>Furthermore, the success of machine learning methods over the past decade can largely be attributed to the advent of scalable data-driven learning methods, which use more data to obtain better training results. Compared to online RL, taking full advantage of large-scale static datasets is also a significant advantage of offline RL.</p>
<p><strong>Offline RL Training</strong></p>
<p>Offline RL prohibits any kinds of interaction and exploration during training。
Under this setting, we train agents utilizing static dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, which is collected by some behavioral policy <span class="math notranslate nohighlight">\(\pi_{\beta}(\mathbf{a}\mid \mathbf{s})\)</span>.
Given <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{ (\mathbf{s}, \mathbf{a}, r, \mathbf{s}^{\prime})\right\}\)</span>, the value iteration and policy optimization process can be represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{Q}^{k+1} \leftarrow \arg\min_{Q} \mathbb{E}_{\mathbf{s}, \mathbf{a} \sim \mathcal{D}} \left[ \left(\hat{\mathcal{B}}^\pi \hat{Q}(\mathbf{s}, \mathbf{a})  - Q(\mathbf{s}, \mathbf{a}) \right)^2 \right],
\\
\hat{\pi}^{k+1} \leftarrow \arg\max_{\pi} \mathbb{E}_{\mathbf{s} \sim \mathcal{D}, \mathbf{a} \sim \pi^{k}(\mathbf{a} \mid \mathbf{s})}\left[\hat{Q}^{k+1}(\mathbf{s}, \mathbf{a})\right],\end{split}\]</div>
<p>where the Bellman Operator <span class="math notranslate nohighlight">\(\hat{\mathcal{B}}^\pi\)</span> of policy <span class="math notranslate nohighlight">\(\hat{\pi} \left(\mathbf{a} \mid \mathbf{s}\right)\)</span> is <span class="math notranslate nohighlight">\(\hat{\mathcal{B}}^\pi \hat{Q}\left(\mathbf{s}, \mathbf{a}\right) = \mathbb{E}_{\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime} \sim \mathcal{D}}[ r(\mathbf{s}, \mathbf{a})+\gamma \mathbb{E}_{\mathbf{a}^{\prime} \sim \hat{\pi}^{k}\left(\mathbf{a}^{\prime} \mid \mathbf{s}^{\prime}\right)}\left[\hat{Q}^{k}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right] ]\)</span>.</p>
<p><strong>Offline RL VS Imitation Learning</strong></p>
<p>Offline RL is closely related to imitation learning (IL) in that the latter also learns from a fixed dataset without exploration. However, there are several key differences:</p>
<ul class="simple">
<li><p>So far, offline RL algorithms have been built on top of standard off-policy Deep Reinforcement Learning (Deep RL) algorithms, which tend to optimize some form of a Bellman equation or TD difference error.</p></li>
<li><p>Most IL problems assume an optimal, or at least a high-performing, demonstrator which provides data, whereas offline RL may have to handle highly suboptimal data.</p></li>
<li><p>Most IL problems do not have a reward function. Offline RL considers rewards, which furthermore can be processed after-the-fact and modified.</p></li>
<li><p>Some IL problems require the data to be labeled as expert versus non-expert, while offline RL does not make this assumption.</p></li>
</ul>
<p><strong>Offline RL VS Off-policy RL</strong></p>
<p>Off-policy RL generally refers to a class of RL algorithms that allow the policy which interacts with the environment to generate training samples to be different from the policy to be updated currently.
Q-learning based algorithms, Actor-Critic algorithms that utilize Q-functions, and many Model-based RL algorithms belong to this category.
Nevertheless, off-policy RL still often uses additional interactions (i.e. online data collection) during the learning process.</p>
<p><strong>The Obstacle of Applying Online RL Algorithms to Offline Setting</strong></p>
<p>Many previous research works have shown that online reinforcement learning algorithms perform poorly in offline RL scenarios.
In paper [6], the author shows that it is because the policy tends to choose out-of-distribution actions (out-of-distribution, OOD).
The estimation of the Q-function is accurate only when the distribution of the data to be estimated follows the distribution of training data.
The relationship is shown in the following figure:</p>
<img alt="../_images/offline_ood.png" class="align-center" src="../_images/offline_ood.png" />
<p>When the agent performs online exploration, the dataset is updated as well as the policy.
The Markov static state distribution of the policy and the actual state distribution in the dataset is always the same (on-policy setting) or at least similar (off-policy setting).
However, there will be a distributional shift compared to the original dataset in the offline scenario.
During the expected reward maximization process, if the Q-function overestimates unseen <span class="math notranslate nohighlight">\((\mathbf{s}, \mathbf{a})\)</span>  pairs, it is possible to select actions with low returns, resulting in poor performances.</p>
</section>
<section id="main-research-directions">
<h2>Main Research Directions<a class="headerlink" href="#main-research-directions" title="Permalink to this headline">¶</a></h2>
<p>Accoding to NeurIPS 2020 Tutorial <a class="footnote-reference brackets" href="#id10" id="id1">1</a> by Aviral Kumar and Sergey Levine, model-free offline RL could be classified as the following three categories:</p>
<ol class="arabic simple">
<li><p>Policy constraint methods</p></li>
<li><p>Uncertainty-based methods</p></li>
<li><p>Value regularization methods</p></li>
</ol>
<p>In addition, there are also works for Model-based RL in offline settings, which will not be carried out here.
Interested readers can refer to <a class="footnote-reference brackets" href="#id16" id="id2">7</a> <a class="footnote-reference brackets" href="#id17" id="id3">8</a> and other documents. For the overall development roadmap of offline RL, you can refer to the overview diagram in <a class="footnote-reference brackets" href="#id18" id="id4">9</a>:</p>
<a class="reference internal image-reference" href="../_images/offline_roadmap.png"><img alt="../_images/offline_roadmap.png" class="align-center" src="../_images/offline_roadmap.png" style="width: 709.1999999999999px; height: 1155.6px;" /></a>
<p><strong>Policy Constraint Methods</strong></p>
<p>This kind of method aims to keep learned policy <span class="math notranslate nohighlight">\(\pi(\mathbf{a} \mid \mathbf{s})\)</span> close enough to the behavioral policy <span class="math notranslate nohighlight">\(\pi_{\beta}(\mathbf{a} \mid \mathbf{s})\)</span>, thus ensuring a precise Q-estimation.
The distance between the aformentioned two policies could be represented as <span class="math notranslate nohighlight">\(\mathbf{D}(\pi, \pi_{\beta})\)</span>。In explicit constraints, the distance is constraint to be smaller than a specific value <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mathbf{D}_f(\pi, \pi_{\beta}) \le \mathcal{C}, \forall \pi\]</div>
<p>There are also implicit constraints such as policy reconstruction, mimicking the behavioral policy <span class="math notranslate nohighlight">\(\pi_{\beta}(\mathbf{a} \mid \mathbf{s})\)</span> with a trim level of perturbation.
In BCQ <a class="footnote-reference brackets" href="#id11" id="id5">2</a>, researchers propose to train a generative model (VAE) to simulate actions in the dataset.
During the update process, the policy selects the action with the highest Q-value from the actions generated by the VAE perturbation, thereby ensuring that the selected action is similar to the action in the dataset.
Based on BCQ, use TD3 as the network structure, then the TD3BC algorithm is derived. For details, please refer to <a class="footnote-reference brackets" href="#id12" id="id6">3</a>.</p>
<p>Moreover, the distance <span class="math notranslate nohighlight">\(\mathbf{D}(\pi, \pi_{\beta})\)</span> could be regarded as a penalty term added to the objective or reward functions.</p>
<p><strong>Uncertainty-based Methods</strong></p>
<p>Aside from directly constraining the policy, we can also mitigate the effect of out-of-distribution actions by making the Q-function resilient to such queries, via effective uncertainty estimation.
This kind of methods requires learning an uncertainty set or distribution <span class="math notranslate nohighlight">\(\mathcal{P}(\mathbf{Q}^{\pi})\)</span>. Details are provided in <a class="footnote-reference brackets" href="#id13" id="id7">4</a> <a class="footnote-reference brackets" href="#id14" id="id8">5</a>. Then we can desgin a penalty term <span class="math notranslate nohighlight">\(\mathcal{P}(\mathbf{Q}^{\pi})\)</span> added to the Q-function.</p>
<div class="math notranslate nohighlight">
\[\pi_{k+1} \leftarrow \arg\max_{\pi}\mathbb{E}_{\mathbf{s} \sim \mathcal{D}}[\mathbb{E}_{\mathbf{a} \sim \pi(\mathbf{a} \mid \mathbf{s})}[\mathbb{E}_{\mathbf{Q}_{k+1}^{\pi} \sim \mathcal{P}(\mathbf{Q}^{\pi})}[\mathbf{Q}_{k+1}^{\pi}(\mathbf{s}, \mathbf{a})] - \alpha \mathbf{Unc}(\mathcal{P}(\mathbf{Q}^{\pi}))]],\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Unc}(\cdot)\)</span> denotes a metric of uncertainty, such that subtracting it provides a conservative estimate of the actual Q-function.</p>
<p><strong>Value Regularization Methods</strong></p>
<p>In CQL <a class="footnote-reference brackets" href="#id15" id="id9">6</a>, a regularization term is plugged into the objective.
This approach can be appealing for several reasons, such as being applicable to both actor-critic and Q-learning methods, even when a policy is not represented explicitly, and avoiding the need for explicit modeling of the behavior policy.</p>
<p>Similar to uncertainty-based method, CQL aims to derive a conservative Q-estimation.</p>
<div class="math notranslate nohighlight">
\[\hat{\mathcal{E}}(\mathcal{B}, \mathcal{\phi}) = \alpha\mathcal{C}(\mathcal{B}, \mathcal{\phi}) + \mathcal{E}(\mathcal{B}, \mathbf{\phi}),\]</div>
<p>where the bellman error <span class="math notranslate nohighlight">\(\mathcal{E}(\mathcal{B}, \mathcal{\phi})\)</span> is the objective in classic DQN, and <span class="math notranslate nohighlight">\(\mathcal{C}(\mathcal{B}, \mathcal{\phi})\)</span> denotes the additional conservative penalty term.
Different choices for <span class="math notranslate nohighlight">\(\mathcal{C}(\mathcal{B}, \mathcal{\phi})\)</span> lead to algorithms with different properties.</p>
<div class="math notranslate nohighlight">
\[\mathcal{C}_{CQL_0}(\mathcal{B}, \mathbf{\phi}) = \mathbb{E}_{\mathbf{s} \sim \mathcal{D}}\mathbb{E}_{\mathbf{a} \sim \mu(\mathbf{a} \mid \mathbf{s})}[\mathbf{Q}_{\phi}(\mathbf{s}, \mathbf{a})],\]</div>
<p>the effect is that the conservative penalty will push down on high Q-values under some distribution <span class="math notranslate nohighlight">\(\mu(\mathbf{a} \mid \mathbf{s})\)</span>. A simple and practical choice for <span class="math notranslate nohighlight">\(\mu(\mathbf{a} \mid \mathbf{s})\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\mu = \arg\max_{\mu} \mathbb{E}_{\mathbf{s} \sim \mathcal{D}}[\mathbb{E}_{\mathbf{a} \sim \mu(\mathbf{a} \mid \mathbf{s})}[\mathbf{Q}_{\phi}(\mathbf{s}, \mathbf{a})] + \mathcal{H}(\mu(\cdot \mid \mathbf{s}))],\]</div>
<p>The meaning is the policy that maximize the expected discounted return given the current data. Therefore, if the penalty weight <span class="math notranslate nohighlight">\(\alpha\)</span> is chosen appropriately, the conservative penalty should mostly push down on Q-values for out-of-distribution actions, since in-distribution actions would be “anchored” by the Bellman error <span class="math notranslate nohighlight">\(\mathcal{E}(\mathcal{B}, \mathcal{\phi})\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\mathcal{C}_{CQL_0}(\mathcal{B}, \mathbf{\phi})\)</span> is too conservative on the Q-estimation, we can choose</p>
<div class="math notranslate nohighlight">
\[\mathcal{C}_{CQL_1}(\mathcal{B}, \mathbf{\phi}) = \mathbb{E}_{\mathbf{s} \sim \mathcal{D}}\mathbb{E}_{\mathbf{a} \sim \mu(\mathbf{a} \mid \mathbf{s})}[\mathbf{Q}_{\phi}(\mathbf{s}, \mathbf{a})] - \mathbb{E}_{(\mathbf{s}, \mathbf{a}) \sim \mathcal{D}}[\mathbf{Q}_{\phi}(\mathbf{s}, \mathbf{a})].\]</div>
</section>
<section id="future-outlooks">
<h2>Future Outlooks<a class="headerlink" href="#future-outlooks" title="Permalink to this headline">¶</a></h2>
<p>Standard off-policy RL algorithms have conventionally focused on dynamic programming methods that can utilize off-policy data.
However, both of these classes of approaches struggle when coming to the fully offline condition.
More recently, a number of improvements for ofﬂine RL methods have been proposed that take into account the statistics of the distributional shift via either policy constraints, uncertainty estimation, or value regularization.
Generally speaking, such methods shed light on the fact that offline RL is actually a counter-factual inference problem: given data that resulted from a given set of decisions, infer the consequence of a different set of decisions.
In conventional machine learning, we usually assume that the training and testing data are independently and identically distributed (i.i.d.). But offline RL drops this assumption, which is exceptionally challenging.
To make this possible, new innovations are required to implement sophisticated statistical methods and combine them with the fundamentals of sequential decision-making in online RL.
Methods such as solving distribution shifts, constraining action distribution, and evaluating the lower boundary of the distribution are all likely to achieve breakthroughs at the current offline RL research level.</p>
<p>In machine learning, a large part of the fantastic achievements of the past decade or so can be attributed to the data-driven learning paradigm.
In computer vision and natural language, the increasing size and diversity of datasets have been an essential driver of progress despite the rapid performance gains driven by improvements in architectures and models, especially in real-world applications.
Ofﬂine RL offers the possibility of turning reinforcement learning - which is conventionally viewed as a fundamentally active learning paradigm - into a data-driven discipline.
However, in the standard setting of most online reinforcement learning methods, collecting large and diverse datasets is often impractical. The risks and costs are enormous in many applications, such as autonomous driving and human-computer interaction.
Therefore, we look forward to witnessing a new generation of data-driven reinforcement learning in the future.
It enables reinforcement learning not only to solve a range of real-world problems that were previously unsolvable, but also to take full advantage of larger, more diverse, and more expressive datasets in existing applications (driving, robotics, etc.).</p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Levine, S., A. Kumar, G. Tucker, and J. Fu (2020). Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643.</p>
</dd>
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Fujimoto, S., Meger, D., and Precup, D. (2018). Off-policy deep reinforcement learning without exploration. arXiv preprint arXiv:1812.02900.</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p>Fujimoto, S., Gu, S. S. (2021). A Minimalist Approach to Offline Reinforcement Learning. arXiv preprint arXiv:2106.06860.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id7">4</a></span></dt>
<dd><p>Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563–1600.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>O’Donoghue, B., Osband, I., Munos, R., and Mnih, V. (2018). The uncertainty bellman equation and exploration. In International Conference on Machine Learning, pages 3836–3845.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020b). Conservative q-learning for ofﬂine reinforcement learning. In Neural Information Processing Systems (NeurIPS).</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id2">7</a></span></dt>
<dd><p>Lerer, A., Gross, S., and Fergus, R. (2016). Learning physical intuition of block towers by example. arXiv preprint arXiv:1603.01312.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id3">8</a></span></dt>
<dd><p>Battaglia, P., Pascanu, R., Lai, M., Rezende, D. J., et al. (2016). Interaction networks for learning about objects, relations and physics. In Advances in neural information processing systems, pages 4502–4510.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id4">9</a></span></dt>
<dd><p>Rafael Figueiredo Prudencio, Marcos R. O. A. Maximo, Esther Luna Colombini. A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. CoRR abs/2203.01387 (2022)</p>
</dd>
</dl>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="distributed_rl.html" class="btn btn-neutral float-right" title="Distributed Reinforcement Learning" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="multi_agent_cooperation_rl.html" class="btn btn-neutral" title="Multi-Agent Reinforcement Learning" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Offline Reinforcement Learning</a><ul>
<li><a class="reference internal" href="#problem-definition-and-motivation">Problem Definition and Motivation</a></li>
<li><a class="reference internal" href="#main-research-directions">Main Research Directions</a></li>
<li><a class="reference internal" href="#future-outlooks">Future Outlooks</a></li>
<li><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
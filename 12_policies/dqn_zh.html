


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>DQN &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Rainbow" href="rainbow_zh.html" />
  <link rel="prev" title="强化学习算法" href="index_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">用户指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index_zh.html">DI-engine 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index_zh.html">快速开始</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index_zh.html">强化学习算法分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index_zh.html">系统设计</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index_zh.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">强化学习教程</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index_zh.html">从 DI-zoo 开始学习</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index_zh.html">强化学习环境示例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者规范</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index_zh.html">代码规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index_zh.html">代码风格指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index_zh.html">单元测试指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index_zh.html">图像与可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index_zh.html">Github 合作模式</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index_zh.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index_zh.html">强化学习算法</a> &gt;</li>
        
      <li>DQN</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/12_policies/dqn_zh.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="dqn">
<h1>DQN<a class="headerlink" href="#dqn" title="Permalink to this headline">¶</a></h1>
<section id="id1">
<h2>综述<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>传统的 Q-learning 维护一张 <code class="docutils literal notranslate"><span class="pre">M*N</span></code> 的Q值表（其中 M表示状态个数，N表示动作个数），通过贝尔曼方程（Bellman equation）来迭代更新 Q-value。这种算法在状态/动作空间变得很大的时候就会出现维度灾难的问题。而DQN与传统强化学习方法不同，它将 Q-learning 与深度神经网络相结合，使用深度神经网络来估计 Q 值，并通过计算时序差分（TD, Temporal-Difference） 损失，利用梯度下降算法进行更新，从而在高维空间的问题决策中（例如Atari游戏）达到了媲美甚至超过人类玩家的水平。</p>
</section>
<section id="id2">
<h2>快速了解<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>DQN 是一个 <strong>无模型（model-free）</strong> 且 <strong>基于值函数（value-based）</strong> 的强化学习算法。</p></li>
<li><p>DQN 只支持 <strong>离散（discrete）</strong> 动作空间。</p></li>
<li><p>DQN 是一个 <strong>异策略（off-policy）</strong> 算法.</p></li>
<li><p>通常，DQN 使用 <strong>epsilon贪心（eps-greedy）</strong> 或 <strong>多项分布采样（multinomial sample）</strong> 来做探索（exploration）。</p></li>
<li><p>DQN + RNN = DRQN</p></li>
<li><p>DI-engine 中实现的 DQN 支持 <strong>多维度离散（multi-discrete）</strong> 动作空间，即在一个step下执行多个离散动作。</p></li>
</ol>
</section>
<section id="id3">
<h2>重要公示/重要图示<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>DQN 中的 TD-loss 是：</p>
<div class="math notranslate nohighlight">
\[\mathrm{L}(w)=\mathbb{E}\left[(\underbrace{r+\gamma \max _{a^{\prime}} Q_{\text {target }}\left(s^{\prime}, a^{\prime}, \theta^{-}\right)}-Q(s, a, \theta))^{2}\right]\]</div>
<p>其中目标网络 <span class="math notranslate nohighlight">\(Q_{\text {target}}\)</span> ，带有参数 <span class="math notranslate nohighlight">\(\theta^{-}\)</span> ，与在线网络相同，只是它的参数会每 <code class="docutils literal notranslate"><span class="pre">target_update_freq</span></code> 个环境步数从在线网络复制更新一次（超参数 <code class="docutils literal notranslate"><span class="pre">target_update_freq</span></code> 可以在配置文件中修改。请参考 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/model/wrapper/model_wrappers.py">TargetNetworkWrapper</a> 了解更多详情）。</p>
</section>
<section id="id4">
<h2>伪代码<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<img alt="../_images/DQN.png" class="align-center" src="../_images/DQN.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>与发表在 Nature 的版本相比，现代的 DQN 在算法和实现方面都得到了显著改进。譬如，在算法部分，<strong>TD-loss, PER, n-step, target network</strong> and <strong>dueling head</strong> 等技巧被广泛使用，感兴趣的读者可参考论文 <a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>。</p>
</div>
</section>
<section id="id5">
<h2>扩展<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>DQN 可以和以下方法相结合：</p>
<blockquote>
<div><ul>
<li><p>优先级经验回放 （PER，<a class="reference external" href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a> ）</p>
<p>Prioritized Experience Replay 用一种特殊定义的“优先级”来代替经验回放池中的均匀采样。该优先级可由各种指标定义，如绝对TD误差、观察的新颖性等。通过优先采样，DQN的收敛速度和性能可以得到很大的提高。</p>
<p>优先级经验回放（PER）有两种实现方式，其中一种较常用方式的伪代码如下图所示：</p>
<img alt="../_images/PERDQN.png" class="align-center" src="../_images/PERDQN.png" />
<p>在DI-engine中，PER可以通过修改配置文件中的 <code class="docutils literal notranslate"><span class="pre">priority</span></code> 和 <code class="docutils literal notranslate"><span class="pre">priority_IS_weight</span></code> 两个字段来控制，具体的代码实现可以参考 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/dev-treetensor/ding/worker/replay_buffer/advanced_buffer.py">PER code</a> 。具体的示例讲解可以参考
<a class="reference external" href="../best_practice/priority.html">PER example</a></p>
</li>
<li><p>多步（Multi-step） TD-loss</p>
<p>在 Single-step TD-loss 中，Q-learning 通过贝尔曼方程更新 <span class="math notranslate nohighlight">\(Q(s,a)\)</span>:</p>
<div class="math notranslate nohighlight">
\[r(s,a)+\gamma \max_{a^{'}}Q(s',a')\]</div>
<p>在 Multi-step TD-loss 中，贝尔曼方程是:</p>
<div class="math notranslate nohighlight">
\[\sum_{t=0}^{n-1}\gamma^t r(s_t,a_t) + \gamma^n \max_{a^{'}}Q(s_n,a')\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>在DQN中使用 Multi-step TD-loss 有一个潜在的问题：采用 epsilon 贪心收集数据时， Q值的估计是有偏的。 因为t &gt;= 1时，<span class="math notranslate nohighlight">\(r(s_t,a_t)\)</span> 是在 epsilon-greedy 策略下采样的，而不是通过正在学习的策略本身来采样。但实践中发现 Multi-step TD-loss 与 epsilon-greedy 结合使用，一般都可以明显提升智能体的最终性能。</p>
</div>
<p>在DI-engine中，Multi-step TD-loss 可以通过修改配置文件中的 <code class="docutils literal notranslate"><span class="pre">nstep</span></code> 字段来控制，详细的损失函数计算代码可以参考 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/dev-treetensor/ding/rl_utils/td.py">nstep code</a> 中的 <code class="docutils literal notranslate"><span class="pre">q_nstep_td_error</span></code></p>
</li>
<li><p>目标网络（target network/Double DQN）</p>
<p>Double DQN, 在 <a class="reference external" href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a> 中被提出，是 DQN 的一种常见变种。</p>
<p>标准 Q 学习或 DQN 中在计算目标网络时的 max 算子使用同一个的 Q 值来选择和评估动作。这使得它选择的动作的价值更有可能被高估，从而导致过度乐观的价值估计。为了防止这种情况，我们可以将选择与评估分离。更具体地说，以下两个公式展示了二者的差异：</p>
<p>（1）标记的Q-learning和（2）标记的Double DQN中的目标如下图所示：</p>
<a class="reference internal image-reference" href="../_images/DQN_and_DDQN.png"><img alt="../_images/DQN_and_DDQN.png" class="align-center" src="../_images/DQN_and_DDQN.png" style="width: 336.8px; height: 44.0px;" /></a>
<p>区别于传统DQN，Double DQN中的目标网络不会选择当前网络中离散动作空间中的最大Q值，而是首先查找  <strong>在线网络</strong>  中Q值最大的动作（对应上面公式中的  <span class="math notranslate nohighlight">\(argmax_a Q(S_{t+1},a;\theta_t)\)</span>），然后根据该动作从 <strong>目标网络</strong>  计算得到Q值
(对应上面公示中的  <span class="math notranslate nohighlight">\(Q(S_{t+1},argmax_a Q(S_{t+1},a;\theta_t);\theta'_t)\)</span>）。</p>
<p>综上所述，Double Q-learning 可以抑制 Q 值的高估，从而减少相关的负面影响。</p>
<p>DI-engine 默认实现并使用 Double DQN ，没有关闭选项。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>过高估计可能是由函数近似误差（近似Q值的神经网络）、环境噪声、数值不稳定等原因造成的。</p>
</div>
</li>
<li><p>Dueling head (<a class="reference external" href="https://arxiv.org/pdf/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>)</p>
<p>Dueling head 结构通过对每个动作的状态-价值和优势的分解，并由上述两个部分构建最终的Q值，从而更好地评估一些与动作选择无关的状态的价值。下图展示了具体的分解结构（图片来自论文 Dueling Network Architectures for Deep Reinforcement Learning）：</p>
<blockquote>
<div><a class="reference internal image-reference" href="../_images/DuelingDQN.png"><img alt="../_images/DuelingDQN.png" class="align-center" src="../_images/DuelingDQN.png" style="height: 300px;" /></a>
</div></blockquote>
<p>在DI-engine中，Dueling head 可以通过修改配置文件中模型部分的 <code class="docutils literal notranslate"><span class="pre">dueling</span></code> 字段来控制，具体网络结构的实现可以参考 <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/model/common/head.py">Dueling Head</a> 中的 <code class="docutils literal notranslate"><span class="pre">DuelingHead</span></code></p>
</li>
<li><p>RNN (DRQN, R2D2)</p>
<p>DQN与RNN结合的方法，可以参考本系列文档中的 <a class="reference external" href="./r2d2.html">R2D2部分</a></p>
</li>
</ul>
</div></blockquote>
</section>
<section id="id6">
<h2>实现<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>DQNPolicy 的默认 config 如下所示：</p>
<p>其中使用的神经网络接口如下所示：</p>
</section>
<section id="benchmark">
<h2>实验 Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<table class="colwidths-given docutils align-default" id="id8">
<caption><span class="caption-text">Benchmark and comparison of dqn algorithm</span><a class="headerlink" href="#id8" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 15%" />
<col style="width: 30%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">Pong</div>
<div class="line">(PongNoFrameskip-v4)</div>
</div>
</td>
<td><p>20</p></td>
<td><img alt="../_images/pong_dqn.png" src="../_images/pong_dqn.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/pong/pong_dqn_config.py">config_link_p</a></p></td>
<td><div class="line-block">
<div class="line">Tianshou(20) Sb3(20)</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">Qbert</div>
<div class="line">(QbertNoFrameskip-v4)</div>
</div>
</td>
<td><p>17966</p></td>
<td><img alt="../_images/qbert_dqn.png" src="../_images/qbert_dqn.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/qbert/qbert_dqn_config.py">config_link_q</a></p></td>
<td><div class="line-block">
<div class="line">Tianshou(7307) Rllib(7968) Sb3(9496)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">SpaceInvaders</div>
<div class="line">(SpaceInvadersNoFrameskip-v4)</div>
</div>
</td>
<td><p>2403</p></td>
<td><img alt="../_images/spaceinvaders_dqn.png" src="../_images/spaceinvaders_dqn.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py">config_link_s</a></p></td>
<td><div class="line-block">
<div class="line">Tianshou(812) Rllib(1001) Sb3(622)</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>注：</p>
<ol class="arabic simple">
<li><p>以上结果是在5个不同的随机种子（即0，1，2，3，4）运行相同的配置得到</p></li>
<li><p>对于DQN这样的离散动作空间算法，一般选择Atari环境集进行测试（其中包括子环境Pong等），而Atari环境，一般是通过训练10M个env_step下所得的最高平均奖励来进行评价，详细的环境信息可以查看 <a class="reference external" href="../env_tutorial/atari_zh.html">Atari环境的介绍文档</a></p></li>
</ol>
</section>
<section id="id7">
<h2>参考文献<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” nature 518.7540 (2015): 529-533.</p></li>
<li><p>Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., &amp; Freitas, N. (2016, June). Dueling network architectures for deep reinforcement learning. In International conference on machine learning (pp. 1995-2003). PMLR.</p></li>
<li><p>Van Hasselt, H., Guez, A., &amp; Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 30, No. 1).</p></li>
<li><p>Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.</p></li>
</ul>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="rainbow_zh.html" class="btn btn-neutral float-right" title="Rainbow" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="index_zh.html" class="btn btn-neutral" title="强化学习算法" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">DQN</a><ul>
<li><a class="reference internal" href="#id1">综述</a></li>
<li><a class="reference internal" href="#id2">快速了解</a></li>
<li><a class="reference internal" href="#id3">重要公示/重要图示</a></li>
<li><a class="reference internal" href="#id4">伪代码</a></li>
<li><a class="reference internal" href="#id5">扩展</a></li>
<li><a class="reference internal" href="#id6">实现</a></li>
<li><a class="reference internal" href="#benchmark">实验 Benchmark</a></li>
<li><a class="reference internal" href="#id7">参考文献</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
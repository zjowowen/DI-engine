


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ICM &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="MCTS" href="mcts.html" />
  <link rel="prev" title="HER" href="her.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index.html">RL Algorithm Taxonomy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index.html">System Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning Tutorial</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index.html">Introduction to RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index.html">Learn From DI-zoo</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">RL Algorithms Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index.html">RL Env Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index.html">Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index.html">Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index.html">Unit Test Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index.html">Diagrams and Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index.html">Github Cooperation</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">RL Algorithms Cheat Sheet</a> &gt;</li>
        
      <li>ICM</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/12_policies/icm.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="icm">
<h1>ICM<a class="headerlink" href="#icm" title="Permalink to this headline">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>ICM (Intrinsic Curiosity Module) was first proposed in the paper <a class="reference external" href="http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf">Curiosity-driven Exploration by Self-supervised Prediction</a>,
It is used to study how to let the agent explore more unexperienced states and learn skills in a sparse reward environment. Its main idea is to use ‘curiosity’ as a signal of intrinsic reward, allowing the agent to explore the environment more efficiently.</p>
<p>The difficulties the algorithm trying to solve:</p>
<ol class="arabic">
<li><p>High-dimensional continuous state space (such as image information) is difficult to establish an intuitive dynamic model, ie <span class="math notranslate nohighlight">\(p_\theta(s_{t+1}, a_t)\)</span> ;</p></li>
<li><p>The correlation between the observation in the environment and the agent’s own behavior is different, which can be roughly divided into:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>Elements that the agent can directly control (such as the position and speed of the vehicle in the autonomous driving scene);</p></li>
<li><p>Elements which are not controlled by the agent, but will affect the agent (such as the position and speed of his car in the automatic driving scene);</p></li>
<li><p>Elements that are neither controlled by the agent nor affect the agent (such as the intensity of sunlight in an autonomous driving scene, although it will affect the sensor, it will not affect the driving behavior in essence).</p></li>
</ol>
</div></blockquote>
</li>
</ol>
<p>For the above three types of observation elements, we want to extract the environmental features in (a) and (b) two contexts, (these two environmental features are related to the action of the agent), while ignoring (c) the contextual features (this This kind of environment feature has nothing to do with the action of the agent).</p>
<p>Features: <strong>Description of feature space</strong> Use a feature space to represent the environment, instead of directly using the original observation to represent the environment, so that features only related to agent actions can be extracted, and features unrelated to environmental features can be ignored. .
Based on the representation of this feature space, a reward module and a forward model are proposed.
<strong>Reward model</strong> The core idea is to estimate the action value adopted by the current state through the representation of the current state and the state at the next moment. The more accurate the estimation of the current action, the better the representation of the environmental elements that the agent can control.
<strong>Forward model</strong> The core idea is to estimate the state representation of the next moment through the current state representation and the current action. This model can make the learned state representations more predictable.</p>
<p>The agent of ICM has two subsystems: one is the <strong>intrinsic reward generator</strong>, which takes the prediction error of the forward model as the intrinsic reward (so the total reward is the sum of the intrinsic reward and the sparse environment reward); the other The subsystem is a <strong>policy network</strong> that outputs a sequence of actions. The optimization goal of training the policy network is the expectation of the total score, so the optimization of the policy will not only consider getting more rewards from the sparse environment, but also explore actions that have not been seen before in order to get more intrinsic rewards.</p>
</section>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>The baseline reinforcement learning algorithm of ICM is <a class="reference external" href="http://proceedings.mlr.press/v48/mniha16.pdf">A3C</a> , you can refer to our implementation <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/a2c.py">A2C</a> , if you want to implement A3C, you can use multiple environments to train at the same time.</p></li>
<li><p>In the follow-up work <a class="reference external" href="https://arxiv.org/pdf/1808.04355v1.pdf">Large-Scale Study of Curiosity-Driven Learning</a>, the baseline algorithm used is PPO, you can refer to our implementation <a class="reference external" href="https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo.py">PPO</a>, through the PPO algorithm, only a small amount of hyperparameter fine-tuning is required to obtain a robust learning effect .</p></li>
<li><p>Although both the reward model and the forward model will participate in the calculation of loss, only the forward model will be used as an intrinsic reward. The larger the loss of the forward model, the more inaccurate the estimation of the state characteristics at the next moment based on the current state characteristics and the current action, that is, this state has not been encountered before, and it is worth exploring; the reward model is not an intrinsic reward, its role is mainly to better help characterize the environmental features related to agent actions in the process of feature space extraction.</p></li>
<li><p>Reward normalization. Since the reward is unstable, it is necessary to normalize the reward to [0, 1] to make the learning more stable. Here we use the maximum and minimum normalization method.</p></li>
<li><p>Feature normalization. By integrating intrinsic and extrinsic rewards, it is important to ensure that intrinsic rewards scale across different feature representations, which can be achieved through batch normalization.</p></li>
<li><p>More actors (more collectors in DI-engine): Adding more parallel actors can make training more stable.</p></li>
</ol>
</section>
<section id="key-equations-or-key-graphs">
<h2>Key Equations or Key Graphs<a class="headerlink" href="#key-equations-or-key-graphs" title="Permalink to this headline">¶</a></h2>
<p>The overall training and calculation process of the ICM algorithm is as follows:</p>
<a class="reference internal image-reference" href="../_images/ICM_illustraion.png"><img alt="../_images/ICM_illustraion.png" class="align-center" src="../_images/ICM_illustraion.png" style="width: 853.3px; height: 294.0px;" /></a>
<p>1. As shown in the figure on the left, the agent samples the action a in the state <span class="math notranslate nohighlight">\(s_t\)</span> through the current policy  <span class="math notranslate nohighlight">\(\pi\)</span> and executes it, and finally obtains the state  <span class="math notranslate nohighlight">\(s_{t+1}\)</span>. The total reward is the sum of two partial rewards, one part is the external reward <span class="math notranslate nohighlight">\(r_t^e\)</span>, that is, the sparse reward obtained in the environment; the other part is the intrinsic reward obtained by the ICM <span class="math notranslate nohighlight">\(r_t^ｉ\)</span> (The specific calculation process is given in step 4), the final strategy needs to achieve the purpose of training by optimizing the total reward.
The specific formula is as follows:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_t=r_t^i + r_t^e\)</span></p>
<p><span class="math notranslate nohighlight">\({\max}_{\theta_p}\mathbb{E}_{\pi(s_t;\theta_p)}[\Sigma_t r_t]\)</span></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>In the reward module of ICM, it first extracts the eigenvectors after characterization by <span class="math notranslate nohighlight">\(s_t\)</span> and <span class="math notranslate nohighlight">\(s_{t+1}\)</span>,:math:<cite>Phi(s_t; theta_E)</cite> and <span class="math notranslate nohighlight">\(\Phi(s_{t+1}; \theta_E)\)</span> as input (reduce them to <span class="math notranslate nohighlight">\(\Phi(s_t)\)</span> and <span class="math notranslate nohighlight">\(\Phi(s_{t+1 })\)</span>), and output the predicted action value <span class="math notranslate nohighlight">\(a_t\)</span></p></li>
</ol>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\hat{a_t}=g(\Phi(s_t),\Phi(s_{t+1}) ; \theta_I)\)</span></p>
<p><span class="math notranslate nohighlight">\({\min}_{\theta_I, \theta_E} L_i(\hat{a_t},a_t)\)</span></p>
</div></blockquote>
<p>Here <span class="math notranslate nohighlight">\(\hat{a_t}\)</span> is the predicted value of <span class="math notranslate nohighlight">\(a_t\)</span> and <span class="math notranslate nohighlight">\(L_I\)</span> describes the difference between the two (cross entropy loss). The smaller the difference, the more accurate the estimation of the current action, and the better the representation of the environmental elements that the agent can control.</p>
<p>3. The forward model of ICM will take <span class="math notranslate nohighlight">\(\Phi(s_t)\)</span>  and action value　<span class="math notranslate nohighlight">\(a_t\)</span> as input, and output the predicted value of the feature vector of the state at the next moment:math:<cite>hat{Phi}(s_{t+1})</cite>
The error between the predicted feature vector at the next moment and the real feature vector is used as the intrinsic reward.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\hat{\phi(s_{t+1})}=f(\Phi(s_t),a_t) ; \theta_F)\)</span></p>
<p><span class="math notranslate nohighlight">\({\min}_{\theta_F, \theta_E} L_F(\hat{\phi(s_{t+1})},\phi(s_{t+1}))\)</span></p>
</div></blockquote>
<p>Here, <span class="math notranslate nohighlight">\(L_F\)</span> describes the difference between <span class="math notranslate nohighlight">\(\hat{\phi(s_{t+1})}\)</span> and <span class="math notranslate nohighlight">\(\phi(s_{t+1})\)</span> (L2 loss), through the learning of the forward model, the learned feature representation can be more predictable.</p>
<ol class="arabic simple" start="4">
<li><p>The intrinsic reward can be characterized by the difference between <span class="math notranslate nohighlight">\(\hat{\phi(s_{t+1})}\)</span> and <span class="math notranslate nohighlight">\(\phi(s_{t+1})\)</span>:</p></li>
</ol>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(r_i^t = \frac{\eta}{2} (\| \hat{\phi(s_{t+1})} - \phi(s_{t+1}) \|)_2^ 2\)</span></p>
</div></blockquote>
<p><strong>Summarize</strong>:
Through the forward model and the reward model, ICM will extract more features of environmental elements that will be affected by the agent; for those environmental elements (such as noise) that cannot be affected by the agent’s actions, there will be no intrinsic reward, thus improving the exploration strategy in robustness.
At the same time, 1-4 can also be written as an optimization function:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\({\min}_{\theta_P,\theta_I,\theta_F,\theta_E} [- \lambda \mathbb{E}_{\pi(s_t;\theta_p)}[\Sigma_t r_t] + (1 -\beta)L_I + \beta LF]\)</span></p>
</div></blockquote>
<p>Here: math:<cite>beta in [0,1]</cite> is used to weigh the weight of forward model error and reward model error; <span class="math notranslate nohighlight">\(\lambda &gt;0\)</span> is used to characterize the importance of policy gradient error to the intrinsic signal degree.</p>
</section>
<section id="important-implementation-details">
<h2>Important Implementation Details<a class="headerlink" href="#important-implementation-details" title="Permalink to this headline">¶</a></h2>
<p>1. Reward normalization. Since the agent is in different stages and environments, the magnitude of the reward may change drastically. If it is directly used for subsequent calculations, it is easy to cause instability in subsequent learning.
In our implementation, it is normalized to [0, 1] according to the following maximum and minimum normalization formula:</p>
<p><code class="docutils literal notranslate"><span class="pre">reward</span> <span class="pre">=</span> <span class="pre">(reward</span> <span class="pre">-</span> <span class="pre">reward.min())</span> <span class="pre">/</span> <span class="pre">(reward.max()</span> <span class="pre">-</span> <span class="pre">reward.min()</span> <span class="pre">+</span> <span class="pre">1e-8)</span></code></p>
<p>2. Use a residual network to fit the forward model. Since the representation dimension of observation is relatively large, the action value is often a discrete value.
Therefore, when calculating the forward model, the residual network can better retain the information of the action value, so as to obtain a better environmental representation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">encode_state</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_1</span><span class="p">(</span><span class="n">pred_next_state_feature_orig</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">pred_next_state_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature_orig</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">+</span> <span class="n">pred_next_state_feature_orig</span>
<span class="n">pred_next_state_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature_orig</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="implementations">
<h2>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h2>
<p>The interface for the Intrinsic Curiosity Model ( <code class="docutils literal notranslate"><span class="pre">ICMRewardModel</span></code> ) is defined as follows:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.reward_model.icm_reward_model.</span></span><span class="sig-name descname"><span class="pre">ICMRewardModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">easydict.EasyDict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tb_logger</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">SummaryWriter</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/reward_model/icm_reward_model.html#ICMRewardModel"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The ICM reward model class (<a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">https://arxiv.org/pdf/1705.05363.pdf</a>)</p>
</dd>
<dt>Interface:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">estimate</span></code>, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">collect_data</span></code>, <code class="docutils literal notranslate"><span class="pre">clear_data</span></code>,             <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">_train</span></code>,</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">clear_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/reward_model/icm_reward_model.html#ICMRewardModel.clear_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Clearing training data.             This can be a side effect function which clears the data attribute in <code class="docutils literal notranslate"><span class="pre">self</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">collect_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/reward_model/icm_reward_model.html#ICMRewardModel.collect_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Collecting training data in designated formate or with designated transition.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): Raw training data (e.g. some form of states, actions, obs, etc)</p></li>
</ul>
</dd>
<dt>Returns / Effects:</dt><dd><ul class="simple">
<li><p>This can be a side effect function which updates the data attribute in <code class="docutils literal notranslate"><span class="pre">self</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">estimate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/reward_model/icm_reward_model.html#ICMRewardModel.estimate"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>estimate reward</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">List</span></code>): the list of data used for estimation</p></li>
</ul>
</dd>
<dt>Returns / Effects:</dt><dd><ul class="simple">
<li><p>This can be a side effect function which updates the reward value</p></li>
<li><p>If this function returns, an example returned object can be reward (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): the estimated reward</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/ding/reward_model/icm_reward_model.html#ICMRewardModel.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>Training the reward model</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>data (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Any</span></code>): Data used for training</p></li>
</ul>
</dd>
<dt>Effects:</dt><dd><ul class="simple">
<li><p>This is mostly a side effect function which updates the reward model</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<section id="icmnetwork">
<h3>ICMNetwork<a class="headerlink" href="#icmnetwork" title="Permalink to this headline">¶</a></h3>
<p>First we define the class <code class="docutils literal notranslate"><span class="pre">ICMNetwork</span></code> which involves four kinds of neural networks:</p>
<p>self.feature: extract the features of observation;</p>
<p>self.inverse_net: The inverse model of the ICM network, which outputs a predicted action by taking two successive frames of feature features as input</p>
<p>self.residual: Participate in the forward model of the ICM network, and make the features more obvious by concat the output of the action and the intermediate layer for many times</p>
<p>self.forward_net: Participate in the forward model of the ICM network, responsible for outputting the feature at the moment of <span class="math notranslate nohighlight">\(s_{t+1}\)</span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ICMNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Intrinsic Curiosity Model (ICM Module)</span>
<span class="sd">Implementation of:</span>
<span class="sd">[1] Curiosity-driven Exploration by Self-supervised Prediction</span>
<span class="sd">Pathak, Agrawal, Efros, and Darrell - UC Berkeley - ICML 2017.</span>
<span class="sd">https://arxiv.org/pdf/1705.05363.pdf</span>
<span class="sd">[2] Code implementation reference:</span>
<span class="sd">https://github.com/pathak22/noreward-rl</span>
<span class="sd">https://github.com/jcwleo/curiosity-driven-exploration-pytorch</span>

<span class="sd">1) Embedding observations into a latent space</span>
<span class="sd">2) Predicting the action logit given two consecutive embedded observations</span>
<span class="sd">3) Predicting the next embedded obs, given the embedded former observation and action</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceType</span><span class="p">],</span> <span class="n">hidden_size_list</span><span class="p">:</span> <span class="n">SequenceType</span><span class="p">,</span> <span class="n">action_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ICMNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">FCEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">ConvEncoder</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
            <span class="s2">&quot;not support obs_shape for pre-defined encoder: </span><span class="si">{}</span><span class="s2">, please customize your own ICM model&quot;</span><span class="o">.</span>
            <span class="nb">format</span><span class="p">(</span><span class="n">obs_shape</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">action_shape</span> <span class="o">=</span> <span class="n">action_shape</span>
    <span class="n">feature_output</span> <span class="o">=</span> <span class="n">hidden_size_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feature_output</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">action_shape</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">action_shape</span> <span class="o">+</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">action_shape</span> <span class="o">+</span> <span class="n">feature_output</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">action_shape</span> <span class="o">+</span> <span class="mi">512</span><span class="p">,</span> <span class="n">feature_output</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">next_state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">action_long</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Overview:</span>
<span class="sd">        Use observation, next_observation and action to genearte ICM module</span>
<span class="sd">        Parameter updates with ICMNetwork forward setup.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        - state (:obj:`torch.Tensor`):</span>
<span class="sd">            The current state batch</span>
<span class="sd">        - next_state (:obj:`torch.Tensor`):</span>
<span class="sd">            The next state batch</span>
<span class="sd">        - action_long (:obj:`torch.Tensor`):</span>
<span class="sd">            The action batch</span>
<span class="sd">    Returns:</span>
<span class="sd">        - real_next_state_feature (:obj:`torch.Tensor`):</span>
<span class="sd">            Run with the encoder. Return the real next_state&#39;s embedded feature.</span>
<span class="sd">        - pred_next_state_feature (:obj:`torch.Tensor`):</span>
<span class="sd">            Run with the encoder and residual network. Return the predicted next_state&#39;s embedded feature.</span>
<span class="sd">        - pred_action_logit (:obj:`torch.Tensor`):</span>
<span class="sd">            Run with the encoder. Return the predicted action logit.</span>
<span class="sd">    Shapes:</span>
<span class="sd">        - state (:obj:`torch.Tensor`): :math:`(B, N)`, where B is the batch size and N is &#39;&#39;obs_shape&#39;&#39;</span>
<span class="sd">        - next_state (:obj:`torch.Tensor`): :math:`(B, N)`, where B is the batch size and N is &#39;&#39;obs_shape&#39;&#39;</span>
<span class="sd">        - action_long (:obj:`torch.Tensor`): :math:`(B)`, where B is the batch size&#39;&#39;</span>
<span class="sd">        - real_next_state_feature (:obj:`torch.Tensor`): :math:`(B, M)`, where B is the batch size</span>
<span class="sd">        and M is embedded feature size</span>
<span class="sd">        - pred_next_state_feature (:obj:`torch.Tensor`): :math:`(B, M)`, where B is the batch size</span>
<span class="sd">        and M is embedded feature size</span>
<span class="sd">        - pred_action_logit (:obj:`torch.Tensor`): :math:`(B, A)`, where B is the batch size</span>
<span class="sd">        and A is the &#39;&#39;action_shape&#39;&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">action_long</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">action_shape</span><span class="p">)</span>
    <span class="n">encode_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">encode_next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>
    <span class="c1"># get pred action logit</span>
    <span class="n">concat_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">encode_state</span><span class="p">,</span> <span class="n">encode_next_state</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">pred_action_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_net</span><span class="p">(</span><span class="n">concat_state</span><span class="p">)</span>
    <span class="c1"># ---------------------</span>

    <span class="c1"># get pred next state</span>
    <span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">encode_state</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_1</span><span class="p">(</span><span class="n">pred_next_state_feature_orig</span><span class="p">)</span>

    <span class="c1"># residual</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">pred_next_state_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature_orig</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">pred_next_state_feature_orig</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">](</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span> <span class="o">+</span> <span class="n">pred_next_state_feature_orig</span>
    <span class="n">pred_next_state_feature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_net_2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pred_next_state_feature_orig</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">real_next_state_feature</span> <span class="o">=</span> <span class="n">encode_next_state</span>
    <span class="k">return</span> <span class="n">real_next_state_feature</span><span class="p">,</span> <span class="n">pred_next_state_feature</span><span class="p">,</span> <span class="n">pred_action_logit</span>
</pre></div>
</div>
</section>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<p>Environment: MiniGrid-DoorKey-8x8-v0;
Baseline algorithm: ppo_offpolicy,
The three lines of the experiment are three seeds, the ids are: 0, 10, 20</p>
<img alt="../_images/tb_icm_doorkey8.png" class="align-center" src="../_images/tb_icm_doorkey8.png" />
<p>Environment: PongNoFrameskip-v4;
Baseline algorithm: ppo_offpolicy,
The three lines of the experiment are three seeds, the ids are: 0, 10, 20</p>
<img alt="../_images/tb_icm_pong.png" class="align-center" src="../_images/tb_icm_pong.png" />
<p>Environment: MiniGrid-FourRooms-v0;
Baseline algorithm: ppo_offpolicy,
The three lines of the experiment are three seeds, the ids are: 0, 10, 20</p>
<img alt="../_images/tb_icm_fourroom.png" class="align-center" src="../_images/tb_icm_fourroom.png" />
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>Pathak D, Agrawal P, Efros A A, et al. Curiosity-driven exploration by self-supervised prediction[C]//International conference on machine learning. PMLR, 2017: 2778-2787.</p></li>
<li><p>Burda Y, Edwards H, Storkey A, et al. Exploration by random network distillation[J]. <a class="reference external" href="https://arxiv.org/abs/1810.12894v1">https://arxiv.org/abs/1810.12894v1</a>. arXiv:1810.12894, 2018.</p></li>
</ol>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="mcts.html" class="btn btn-neutral float-right" title="MCTS" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="her.html" class="btn btn-neutral" title="HER" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">ICM</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li><a class="reference internal" href="#key-equations-or-key-graphs">Key Equations or Key Graphs</a></li>
<li><a class="reference internal" href="#important-implementation-details">Important Implementation Details</a></li>
<li><a class="reference internal" href="#implementations">Implementations</a><ul>
<li><a class="reference internal" href="#icmnetwork">ICMNetwork</a></li>
</ul>
</li>
<li><a class="reference internal" href="#results">Results</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>
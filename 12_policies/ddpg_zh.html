


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>DDPG &mdash; DI-engine 0.1.0 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="ICM" href="icm_zh.html" />
  <link rel="prev" title="IQN" href="iqn_zh.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">


  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">用户指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../00_intro/index_zh.html">DI-engine 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_quickstart/index_zh.html">快速开始</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_algo/index_zh.html">强化学习算法分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_system/index_zh.html">系统设计</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_best_practice/index_zh.html">最佳实践</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_faq/index_zh.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">强化学习教程</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../10_concepts/index_zh.html">强化学习基础概念介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_dizoo/index_zh.html">从 DI-zoo 开始学习</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index_zh.html">强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_envs/index_zh.html">强化学习环境示例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">开发者规范</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../20_spec/index_zh.html">代码规范</a></li>
<li class="toctree-l1"><a class="reference internal" href="../21_code_style/index_zh.html">代码风格指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../22_test/index_zh.html">单元测试指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../23_visual/index_zh.html">图像与可视化</a></li>
<li class="toctree-l1"><a class="reference internal" href="../24_cooperation/index_zh.html">Github 合作模式</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index_zh.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index_zh.html">强化学习算法</a> &gt;</li>
        
      <li>DDPG</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/12_policies/ddpg_zh.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="ddpg">
<h1>DDPG<a class="headerlink" href="#ddpg" title="Permalink to this headline">¶</a></h1>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>DDPG (Deep Deterministic Policy Gradient) 首次在论文
<a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a> 中提出,
是一种同时学习Q函数和策略函数的算法。</p>
<p>DDPG 是基于 DPG (deterministic policy gradient) 的 <strong>无模型（model-free）</strong> 算法，属于 <strong>演员—评委（actor-critic）</strong> 方法中的一员，可以在高维、连续的动作空间上运行。
其中算法 DPG <a class="reference external" href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic policy gradient algorithms</a> 与算法 NFQCA <a class="reference external" href="https://link.springer.com/content/pdf/10.1007/s10994-011-5235-x.pdf?pdf=button">Reinforcement learning in feedback control</a> 相似。</p>
</section>
<section id="id2">
<h2>核心要点<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>DDPG 仅支持 <strong>连续动作空间</strong> （例如： MuJoCo）.</p></li>
<li><p>DDPG 是一种 <strong>异策略（off-policy）</strong> 算法.</p></li>
<li><p>DDPG 是一种 <strong>无模型（model-free）</strong> 和 <strong>演员—评委（actor-critic）</strong> 的强化学习算法，它会分别优化策略网络和Q网络。</p></li>
<li><p>通常, DDPG 使用 <strong>奥恩斯坦-乌伦贝克过程（Ornstein-Uhlenbeck process）</strong> 或 <strong>高斯过程（Gaussian process）</strong> （在我们的实现中默认使用高斯过程）来探索环境。</p></li>
</ol>
</section>
<section id="id3">
<h2>关键方程或关键框图<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>DDPG 包含一个参数化的策略函数（actor） <span class="math notranslate nohighlight">\(\mu\left(s \mid \theta^{\mu}\right)\)</span> ,
此函数通过将每一个状态确定性地映射到一个具体的动作从而明确当前策略。
此外，算法还包含一个参数化的Q函数（critic） <span class="math notranslate nohighlight">\(Q(s, a)\)</span> 。
正如 Q-learning 算法，此函数通过贝尔曼方程优化自身。</p>
<p>策略网络通过将链式法则应用于初始分布的预期收益 <span class="math notranslate nohighlight">\(J\)</span> 来更新自身参数。</p>
<p>具体而言，为了最大化预期收益 <span class="math notranslate nohighlight">\(J\)</span> ，算法需要计算 <span class="math notranslate nohighlight">\(J\)</span> 对策略函数参数 <span class="math notranslate nohighlight">\(\theta^{\mu}\)</span> 的梯度。 <span class="math notranslate nohighlight">\(J\)</span> 是 <span class="math notranslate nohighlight">\(Q(s, a)\)</span> 的期望，所以问题转化为计算 <span class="math notranslate nohighlight">\(Q^{\mu}(s, \mu(s))\)</span> 对 <span class="math notranslate nohighlight">\(\theta^{\mu}\)</span> 的梯度。</p>
<p>根据链式法则，<span class="math notranslate nohighlight">\(\nabla_{\theta^{\mu}} Q^{\mu}(s, \mu(s)) = \nabla_{\theta^{\mu}}\mu(s)\nabla_{a}Q^\mu(s,a)|_{ a=\mu\left(s\right)}+\nabla_{\theta^{\mu}} Q^{\mu}(s, a)|_{ a=\mu\left(s\right)}\)</span>。</p>
<p><a class="reference external" href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic policy gradient algorithms</a> 采取了与 <a class="reference external" href="https://arxiv.org/pdf/1205.4839.pdf">Off-Policy Actor-Critic</a> 中推导 <strong>异策略版本的随机性策略梯度定理</strong> 类似的做法，舍去了上式第二项，
从而得到了近似后的 <strong>确定性策略梯度定理</strong> ：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla_{\theta^{\mu}} J &amp; \approx \mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[\left.\nabla_{\theta^{\mu}} Q\left(s, a \mid \theta^{Q}\right)\right|_{s=s_{t}, a=\mu\left(s_{t} \mid \theta^{\mu}\right)}\right] \\
&amp;=\mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[\left.\left.\nabla_{a} Q\left(s, a \mid \theta^{Q}\right)\right|_{s=s_{t}, a=\mu\left(s_{t}\right)} \nabla_{\theta^{\mu}} \mu\left(s \mid \theta^{\mu}\right)\right|_{s=s_{t}}\right]
\end{aligned}\end{split}\]</div>
<p>DDPG 使用了一个 <strong>经验回放池（replay buffer）</strong> 来保证样本分布独立一致。</p>
<p>为了使神经网络稳定优化，DDPG 使用 <strong>软更新（“soft” target updates）</strong> 的方式来优化目标网络，而不是像 DQN 中的 hard target updates 那样定期直接复制网络的参数。
具体而言，DDPG 分别拷贝了 actor 网络 <span class="math notranslate nohighlight">\(\mu' \left(s \mid \theta^{\mu'}\right)\)</span> 和 critic 网络 <span class="math notranslate nohighlight">\(Q'(s, a|\theta^{Q'})\)</span> 用于计算目标值。
然后通过让这些目标网络缓慢跟踪学习到的网络来更新这些目标网络的权重：</p>
<div class="math notranslate nohighlight">
\[\theta' \leftarrow \tau \theta + (1 - \tau)\theta',\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\tau&lt;&lt;1\)</span>。这意味着目标值被限制为缓慢变化，大大提高了学习的稳定性。</p>
<p>在连续行动空间中学习的一个主要挑战是探索。然而，对于像DDPG这样的 <strong>异策略（off-policy）</strong> 算法来说，它的一个优势是可以独立于算法中的学习过程来处理探索问题。具体来说，我们通过将噪声过程 <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> 采样的噪声添加到 actor 策略中来构建探索策略:</p>
<div class="math notranslate nohighlight">
\[\mu^{\prime}\left(s_{t}\right)=\mu\left(s_{t} \mid \theta_{t}^{\mu}\right)+\mathcal{N}\]</div>
</section>
<section id="id5">
<h2>伪代码<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}:nowrap:\\\begin{split}\begin{algorithm}[H]
    \caption{Deep Deterministic Policy Gradient}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ}} \leftarrow \phi$
    \REPEAT
        \STATE Observe state $s$ and select action $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{Low}, a_{High})$, where $\epsilon \sim \mathcal{N}$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{however many updates}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets
                \begin{equation*}
                    y(r,s',d) = r + \gamma (1-d) Q_{\phi_{\text{targ}}}(s', \mu_{\theta_{\text{targ}}}(s'))
                \end{equation*}
                \STATE Update Q-function by one step of gradient descent using
                \begin{equation*}
                    \nabla_{\phi} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi}(s,a) - y(r,s',d) \right)^2
                \end{equation*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi}(s, \mu_{\theta}(s))
                \end{equation*}
                \STATE Update target networks with
                \begin{align*}
                    \phi_{\text{targ}} &amp;\leftarrow \rho \phi_{\text{targ}} + (1-\rho) \phi \\
                    \theta_{\text{targ}} &amp;\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}\end{split}\end{aligned}\end{align} \]</div>
<a class="reference internal image-reference" href="../_images/DDPG.jpg"><img alt="../_images/DDPG.jpg" class="align-center" src="../_images/DDPG.jpg" style="width: 751.5px; height: 556.5px;" /></a>
</section>
<section id="id6">
<h2>扩展<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<dl>
<dt>DDPG 可以与以下技术相结合使用:</dt><dd><ul>
<li><p>目标网络</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a> 提出了利用软目标更新保持网络训练稳定的方法。
因此我们通过 <code class="docutils literal notranslate"><span class="pre">model_wrap</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">TargetNetworkWrapper</span></code> 和配置 <code class="docutils literal notranslate"><span class="pre">learn.target_theta</span></code> 来实现 <strong>演员—评委（actor-critic）</strong> 的软更新目标网络。</p>
</div></blockquote>
</li>
<li><p>遵循随机策略的经验回放池初始采集</p>
<blockquote>
<div><p>在优化模型参数前，我们需要让经验回放池存有足够数目的遵循随机策略的 transition 数据，从而确保在算法初期模型不会对经验回放池数据过拟合。
因此我们通过配置 <code class="docutils literal notranslate"><span class="pre">random-collect-size</span></code> 来控制初始经验回放池中的 transition 数目。
DDPG/TD3 的 <code class="docutils literal notranslate"><span class="pre">random-collect-size</span></code> 默认设置为25000, SAC 为10000。
我们只是简单地遵循 SpinningUp 默认设置，并使用随机策略来收集初始化数据。</p>
</div></blockquote>
</li>
<li><p>采集过渡过程中的高斯噪声</p>
<blockquote>
<div><p>对于探索噪声过程，DDPG使用时间相关噪声，以提高具有惯性的物理控制问题的探索效率。
具体而言，DDPG 使用 Ornstein-Uhlenbeck 过程，其中 <span class="math notranslate nohighlight">\(\theta = 0.15\)</span> 且 <span class="math notranslate nohighlight">\(\sigma = 0.2\)</span>。Ornstein-Uhlenbeck 过程模拟了带有摩擦的布朗粒子的速度，其结果是以 0 为中心的时间相关值。
然而，由于 Ornstein-Uhlenbeck 噪声的超参数太多，我们使用高斯噪声代替了 Ornstein-Uhlenbeck 噪声。
我们通过配置 <code class="docutils literal notranslate"><span class="pre">collect.noise_sigma</span></code> 来控制探索程度。</p>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</section>
<section id="id8">
<h2>实现<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>默认配置定义如下:</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.policy.ddpg.</span></span><span class="sig-name descname"><span class="pre">DDPGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">type</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_field</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/policy/ddpg.html#DDPGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>Policy class of DDPG algorithm.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1509.02971.pdf">https://arxiv.org/pdf/1509.02971.pdf</a></p>
</dd>
<dt>Property:</dt><dd><p>learn_mode, collect_mode, eval_mode</p>
</dd>
</dl>
<p>Config:</p>
<blockquote>
<div><table class="docutils align-default">
<colgroup>
<col style="width: 2%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 33%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ID</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Other(Shape)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">type</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>str</p></td>
<td><p>ddpg</p></td>
<td><div class="line-block">
<div class="line">RL policy register name, refer</div>
<div class="line">to registry <code class="docutils literal notranslate"><span class="pre">POLICY_REGISTRY</span></code></div>
</div>
</td>
<td><div class="line-block">
<div class="line">this arg is optional,</div>
<div class="line">a placeholder</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">cuda</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use cuda for network</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">random_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect_size</span></code></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>25000</p></td>
<td><div class="line-block">
<div class="line">Number of randomly collected</div>
<div class="line">training samples in replay</div>
<div class="line">buffer when training starts.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default to 25000 for</div>
<div class="line">DDPG/TD3, 10000 for</div>
<div class="line">sac.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">model.twin_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">critic</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to use two critic</div>
<div class="line">networks or only one.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default False for</div>
<div class="line">DDPG, Clipped Double</div>
<div class="line">Q-learning method in</div>
<div class="line">TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_actor</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rate for actor</div>
<div class="line">network(aka. policy).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.learning</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">_rate_critic</span></code></div>
</div>
</td>
<td><p>float</p></td>
<td><p>1e-3</p></td>
<td><div class="line-block">
<div class="line">Learning rates for critic</div>
<div class="line">network (aka. Q-network).</div>
</div>
</td>
<td><div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.actor_</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">update_freq</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>int</p></td>
<td><p>2</p></td>
<td><div class="line-block">
<div class="line">When critic network updates</div>
<div class="line">once, how many times will actor</div>
<div class="line">network update.</div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default 1 for DDPG,</div>
<div class="line">2 for TD3. Delayed</div>
<div class="line">Policy Updates method</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.noise</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Whether to add noise on target</div>
<div class="line">network’s action.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Default False for</div>
<div class="line">DDPG, True for TD3.</div>
<div class="line">Target Policy Smoo-</div>
<div class="line">thing Regularization</div>
<div class="line">in TD3 paper.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">ignore_done</span></code></div>
</div>
</td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><div class="line-block">
<div class="line">Determine whether to ignore</div>
<div class="line">done flag.</div>
</div>
</td>
<td><div class="line-block">
<div class="line">Use ignore_done only</div>
<div class="line">in halfcheetah env.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">learn.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">target_theta</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.005</p></td>
<td><div class="line-block">
<div class="line">Used for soft update of the</div>
<div class="line">target network.</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">aka. Interpolation</div>
<div class="line">factor in polyak aver-</div>
<div class="line">aging for target</div>
<div class="line">networks.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><div class="line-block">
<div class="line"><code class="docutils literal notranslate"><span class="pre">collect.-</span></code></div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">noise_sigma</span></code></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><p>float</p></td>
<td><p>0.1</p></td>
<td><div class="line-block">
<div class="line">Used for add noise during co-</div>
<div class="line">llection, through controlling</div>
<div class="line">the sigma of distribution</div>
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</td>
<td><div class="line-block">
<div class="line">Sample noise from dis-</div>
<div class="line">tribution, Ornstein-</div>
<div class="line">Uhlenbeck process in</div>
<div class="line">DDPG paper, Gaussian</div>
<div class="line">process in ours.</div>
</div>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
</dd></dl>

<section id="id9">
<h3>模型<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>在这里，我们提供了 <cite>QAC</cite> 模型作为 <cite>DDPG</cite> 的默认模型的示例。</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">ding.model.template.qac.</span></span><span class="sig-name descname"><span class="pre">QAC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">ding.utils.type_helper.SequenceType</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_shape</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span> </span><span class="pre">ding.utils.type_helper.SequenceType</span><span class="p"><span class="pre">,</span> </span><span class="pre">easydict.EasyDict</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_space</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">twin_critic</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_head_hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_head_layer_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_head_hidden_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_head_layer_num</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.nn.modules.module.Module</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">ReLU()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_hidden_size_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ding.utils.type_helper.SequenceType</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">[32,</span> <span class="pre">64,</span> <span class="pre">256]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_encoder</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/ding/model/template/qac.html#QAC"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The QAC network, which is used in DDPG/TD3/SAC.</p>
</dd>
<dt>Interfaces:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">compute_actor</span></code>, <code class="docutils literal notranslate"><span class="pre">compute_critic</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_actor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/model/template/qac.html#QAC.compute_actor"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>The forward computation graph of compute_actor mode, uses observation tensor to produce actor output,
such as <code class="docutils literal notranslate"><span class="pre">action</span></code>, <code class="docutils literal notranslate"><span class="pre">logit</span></code> and so on.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Observation tensor data, now supports a batch of 1-dim vector data,                 i.e. <code class="docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">obs_shape)</span></code>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>outputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Union[torch.Tensor,</span> <span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]]]</span></code>): Actor output varying                 from action_space: <code class="docutils literal notranslate"><span class="pre">regression</span></code>, <code class="docutils literal notranslate"><span class="pre">reparameterization</span></code>, <code class="docutils literal notranslate"><span class="pre">hybrid</span></code>.</p></li>
</ul>
</dd>
<dt>ReturnsKeys (either):</dt><dd><ul>
<li><dl class="simple">
<dt>regression action_space</dt><dd><ul class="simple">
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Continuous action with same size as <code class="docutils literal notranslate"><span class="pre">action_shape</span></code>, usually in DDPG.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>reparameterization action_space</dt><dd><ul>
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): Reparameterization logit, usually in SAC.</p>
<blockquote>
<div><ul class="simple">
<li><p>mu (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Mean of parameterization gaussion distribution.</p></li>
<li><p>sigma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Standard variation of parameterization gaussion distribution.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>hybrid action_space</dt><dd><ul class="simple">
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Discrete action type logit.</p></li>
<li><p>action_args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Continuous action arguments.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N0)\)</span>, B is batch size and N0 corresponds to <code class="docutils literal notranslate"><span class="pre">obs_shape</span></code>.</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N1)\)</span>, B is batch size and N1 corresponds to <code class="docutils literal notranslate"><span class="pre">action_shape</span></code>.</p></li>
<li><p>logit.mu (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N1)\)</span>, B is batch size and N1 corresponds to <code class="docutils literal notranslate"><span class="pre">action_shape</span></code>.</p></li>
<li><p>logit.sigma (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N1)\)</span>, B is batch size.</p></li>
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N2)\)</span>, B is batch size and N2 corresponds to                 <code class="docutils literal notranslate"><span class="pre">action_shape.action_type_shape</span></code>.</p></li>
<li><p>action_args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N3)\)</span>, B is batch size and N3 corresponds to                 <code class="docutils literal notranslate"><span class="pre">action_shape.action_args_shape</span></code>.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Regression mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">QAC</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;regression&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actor_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span><span class="s1">&#39;compute_actor&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">actor_outputs</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Reparameterization Mode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">QAC</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s1">&#39;reparameterization&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actor_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span><span class="s1">&#39;compute_actor&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">actor_outputs</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>  <span class="c1"># mu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">actor_outputs</span><span class="p">[</span><span class="s1">&#39;logit&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span> <span class="c1"># sigma</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">compute_critic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/model/template/qac.html#QAC.compute_critic"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl>
<dt>Overview:</dt><dd><p>The forward computation graph of compute_critic mode, uses observation and action tensor to produce critic
output, such as <code class="docutils literal notranslate"><span class="pre">q_value</span></code>.</p>
</dd>
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): Dict strcture of input data, including <code class="docutils literal notranslate"><span class="pre">obs</span></code> and <code class="docutils literal notranslate"><span class="pre">action</span></code>                 tensor, also contains <code class="docutils literal notranslate"><span class="pre">logit</span></code> tensor in hybrid action_space.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>outputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.Tensor]</span></code>): Critic output, such as <code class="docutils literal notranslate"><span class="pre">q_value</span></code>.</p></li>
</ul>
</dd>
<dt>ArgumentsKeys:</dt><dd><ul class="simple">
<li><p>obs: (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Observation tensor data, now supports a batch of 1-dim vector data.</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Union[torch.Tensor,</span> <span class="pre">Dict]</span></code>): Continuous action with same size as <code class="docutils literal notranslate"><span class="pre">action_shape</span></code>.</p></li>
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Discrete action logit, only in hybrid action_space.</p></li>
<li><p>action_args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Continuous action arguments, only in hybrid action_space.</p></li>
</ul>
</dd>
<dt>ReturnKeys:</dt><dd><ul class="simple">
<li><p>q_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Q value tensor with same size as batch size.</p></li>
</ul>
</dd>
<dt>Shapes:</dt><dd><ul class="simple">
<li><p>obs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N1)\)</span>, where B is batch size and N1 is <code class="docutils literal notranslate"><span class="pre">obs_shape</span></code>.</p></li>
<li><p>logit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N2)\)</span>, B is batch size and N2 corresponds to                 <code class="docutils literal notranslate"><span class="pre">action_shape.action_type_shape</span></code>.</p></li>
<li><p>action_args (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N3)\)</span>, B is batch size and N3 corresponds to                 <code class="docutils literal notranslate"><span class="pre">action_shape.action_args_shape</span></code>.</p></li>
<li><p>action (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, N4)\)</span>, where B is batch size and N4 is <code class="docutils literal notranslate"><span class="pre">action_shape</span></code>.</p></li>
<li><p>q_value (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): <span class="math notranslate nohighlight">\((B, )\)</span>, where B is batch size.</p></li>
</ul>
</dd>
<dt>Examples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s1">&#39;action&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">QAC</span><span class="p">(</span><span class="n">obs_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">),</span><span class="n">action_shape</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">action_space</span><span class="o">=</span><span class="s1">&#39;regression&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_critic&#39;</span><span class="p">)[</span><span class="s1">&#39;q_value&#39;</span><span class="p">]</span>  <span class="c1"># q value</span>
<span class="gp">... </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0773</span><span class="p">,</span> <span class="mf">0.1639</span><span class="p">,</span> <span class="mf">0.0917</span><span class="p">,</span> <span class="mf">0.0370</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SqueezeBackward1</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/ding/model/template/qac.html#QAC.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><dl class="simple">
<dt>Overview:</dt><dd><p>The unique execution (forward) method of QAC method, and one can indicate different modes to implement             different computation graph, including <code class="docutils literal notranslate"><span class="pre">compute_actor</span></code> and <code class="docutils literal notranslate"><span class="pre">compute_critic</span></code> in QAC.</p>
</dd>
<dt>Mode compute_actor:</dt><dd><dl class="simple">
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.Tensor</span></code>): Observation data, defaults to tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict</span></code>): Output dict data, including differnet key-values among distinct action_space.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Mode compute_critic:</dt><dd><dl class="simple">
<dt>Arguments:</dt><dd><ul class="simple">
<li><p>inputs (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict</span></code>): Input dict data, including obs and action tensor.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>output (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Dict</span></code>): Output dict data, including q_value tensor.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For specific examples, one can refer to API doc of <code class="docutils literal notranslate"><span class="pre">compute_actor</span></code> and <code class="docutils literal notranslate"><span class="pre">compute_critic</span></code> respectively.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="actor-critic">
<h3>训练 actor-critic 模型<a class="headerlink" href="#actor-critic" title="Permalink to this headline">¶</a></h3>
<p>首先，我们在 <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> 中分别初始化 actor 和 critic 优化器。
设置两个独立的优化器可以保证我们在计算 actor 损失时只更新 actor 网络参数而不更新 critic 网络，反之亦然。</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># actor and critic optimizer</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_actor</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">actor</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">learning_rate_actor</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">weight_decay</span>
<span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_critic</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">learning_rate_critic</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">weight_decay</span>
<span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<dl>
<dt>在 <code class="docutils literal notranslate"><span class="pre">_forward_learn</span></code> 中，我们通过计算 critic 损失、更新 critic 网络、计算 actor 损失和更新 actor 网络来更新 actor-critic 策略。</dt><dd><ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">critic</span> <span class="pre">loss</span> <span class="pre">computation</span></code></p>
<blockquote>
<div><ul class="simple">
<li><p>计算当前值和目标值</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># current q value</span>
<span class="n">q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learn_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_critic&#39;</span><span class="p">)[</span><span class="s1">&#39;q_value&#39;</span><span class="p">]</span>
<span class="c1"># target q value. SARSA: first predict next action, then calculate next q value</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">next_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">next_obs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_actor&#39;</span><span class="p">)[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span>
    <span class="n">next_data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="n">next_obs</span><span class="p">,</span> <span class="s1">&#39;action&#39;</span><span class="p">:</span> <span class="n">next_action</span><span class="p">}</span>
    <span class="n">target_q_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">next_data</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_critic&#39;</span><span class="p">)[</span><span class="s1">&#39;q_value&#39;</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>计算损失</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DDPG: single critic network</span>
<span class="n">td_data</span> <span class="o">=</span> <span class="n">v_1step_td_data</span><span class="p">(</span><span class="n">q_value</span><span class="p">,</span> <span class="n">target_q_value</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;done&#39;</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
<span class="n">critic_loss</span><span class="p">,</span> <span class="n">td_error_per_sample</span> <span class="o">=</span> <span class="n">v_1step_td_error</span><span class="p">(</span><span class="n">td_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">)</span>
<span class="n">loss_dict</span><span class="p">[</span><span class="s1">&#39;critic_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">critic_loss</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic</span> <span class="pre">network</span> <span class="pre">update</span></code></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_critic</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss_dict</span><span class="p">[</span><span class="s1">&#39;critic_loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_critic</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">actor</span> <span class="pre">loss</span></code></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">actor_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_learn_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_actor&#39;</span><span class="p">)</span>
<span class="n">actor_data</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;obs&#39;</span><span class="p">]</span>
<span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_learn_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">actor_data</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;compute_critic&#39;</span><span class="p">)[</span><span class="s1">&#39;q_value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">loss_dict</span><span class="p">[</span><span class="s1">&#39;actor_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">actor_loss</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">actor</span> <span class="pre">network</span> <span class="pre">update</span></code></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># actor update</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_actor</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">actor_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_actor</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="id10">
<h3>目标网络<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>我们通过 <code class="docutils literal notranslate"><span class="pre">_init_learn</span></code> 中的目标模型初始化来实现目标网络。
我们配置 <code class="docutils literal notranslate"><span class="pre">learn.target_theta</span></code> 来控制平均中的插值因子。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># main and target models</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_target_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_target_model</span> <span class="o">=</span> <span class="n">model_wrap</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_target_model</span><span class="p">,</span>
    <span class="n">wrapper_name</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">,</span>
    <span class="n">update_type</span><span class="o">=</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span>
    <span class="n">update_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">target_theta</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id11">
<h2>基准<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 19%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>environment</p></th>
<th class="head"><p>best mean reward</p></th>
<th class="head"><p>evaluation results</p></th>
<th class="head"><p>config link</p></th>
<th class="head"><p>comparison</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HalfCheetah</p>
<p>(HalfCheetah-v3)</p>
</td>
<td><p>11334</p></td>
<td><img alt="../_images/halfcheetah_ddpg.png" src="../_images/halfcheetah_ddpg.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/halfcheetah_ddpg_default_config.py">config_link_p</a></p></td>
<td><p>Tianshou(11719)
Spinning-up(11000)</p></td>
</tr>
<tr class="row-odd"><td><p>Hopper</p>
<p>(Hopper-v2)</p>
</td>
<td><p>3516</p></td>
<td><img alt="../_images/hopper_ddpg.png" src="../_images/hopper_ddpg.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/hopper_ddpg_default_config.py">config_link_q</a></p></td>
<td><p>Tianshou(2197)
Spinning-up(1800)</p></td>
</tr>
<tr class="row-even"><td><p>Walker2d</p>
<p>(Walker2d-v2)</p>
</td>
<td><p>3443</p></td>
<td><img alt="../_images/walker2d_ddpg.png" src="../_images/walker2d_ddpg.png" />
</td>
<td><p><a class="reference external" href="https://github.com/opendilab/DI-engine/tree/main/dizoo/mujoco/config/walker2d_ddpg_default_config.py">config_link_s</a></p></td>
<td><p>Tianshou(1401)
Spinning-up(1950)</p></td>
</tr>
</tbody>
</table>
<p>P.S.：</p>
<ol class="arabic simple">
<li><p>上述结果是通过在五个不同的随机种子(0,1,2,3,4)上运行相同的配置获得的。</p></li>
</ol>
</section>
<section id="id12">
<h2>参考<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra: “Continuous control with deep reinforcement learning”, 2015; [<a class="reference external" href="http://arxiv.org/abs/1509.02971">http://arxiv.org/abs/1509.02971</a> arXiv:1509.02971].</p>
<p>David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, et al.. Deterministic Policy Gradient Algorithms. ICML, Jun 2014, Beijing, China. ffhal-00938992f</p>
<p>Hafner, R., Riedmiller, M. Reinforcement learning in feedback control. Mach Learn 84, 137–169 (2011).</p>
<p>Degris, T., White, M., and Sutton, R. S. (2012b). Linear off-policy actor-critic. In 29th International Conference on Machine Learning.</p>
</section>
<section id="id13">
<h2>其他公开的实现<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/ddpg">Baselines</a></p></li>
<li><p><a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ddpg">sb3</a></p></li>
<li><p><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/ddpg.py">rllab</a></p></li>
<li><p><a class="reference external" href="https://github.com/ray-project/ray/tree/master/rllib/agents/ddpg">rllib (Ray)</a></p></li>
<li><p><a class="reference external" href="https://github.com/sfujim/TD3">TD3 release repo</a></p></li>
<li><p><a class="reference external" href="https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ddpg">Spinningup</a></p></li>
<li><p><a class="reference external" href="https://github.com/thu-ml/tianshou/blob/master/tianshou/policy/modelfree/ddpg.py">tianshou</a></p></li>
</ul>
</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="icm_zh.html" class="btn btn-neutral float-right" title="ICM" accesskey="n"
      rel="next">Next <img src="../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="iqn_zh.html" class="btn btn-neutral" title="IQN" accesskey="p"
      rel="prev"><img src="../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2021, OpenDILab Contributors.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">DDPG</a><ul>
<li><a class="reference internal" href="#id1">概述</a></li>
<li><a class="reference internal" href="#id2">核心要点</a></li>
<li><a class="reference internal" href="#id3">关键方程或关键框图</a></li>
<li><a class="reference internal" href="#id5">伪代码</a></li>
<li><a class="reference internal" href="#id6">扩展</a></li>
<li><a class="reference internal" href="#id8">实现</a><ul>
<li><a class="reference internal" href="#id9">模型</a></li>
<li><a class="reference internal" href="#actor-critic">训练 actor-critic 模型</a></li>
<li><a class="reference internal" href="#id10">目标网络</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11">基准</a></li>
<li><a class="reference internal" href="#id12">参考</a></li>
<li><a class="reference internal" href="#id13">其他公开的实现</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/jquery.js"></script>
  <script src="../_static/underscore.js"></script>
  <script src="../_static/doctools.js"></script>
  <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/DI-engine" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>